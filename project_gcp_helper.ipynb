{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00e032c",
   "metadata": {
    "id": "hWgiQS0zkWJ5"
   },
   "source": [
    "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac36d3a",
   "metadata": {
    "id": "c0ccf76b",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Worker_Count",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-fb6c  GCE       3                                             RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security â†’ Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf86c5",
   "metadata": {
    "id": "01ec9fd3"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf199e6a",
   "metadata": {
    "id": "32b3ec57",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Setup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f56ecd",
   "metadata": {
    "id": "5609143b",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a897f2",
   "metadata": {
    "id": "b10cc999",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-jar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Feb 24 17:29 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47900073",
   "metadata": {
    "id": "d3f86f11",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-pyspark-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bed56b",
   "metadata": {
    "id": "5be6dc2a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-spark-version",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-fb6c-m.c.irass3-413914.internal:43779\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3989706b60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "980e62a5",
   "metadata": {
    "id": "7adc1bf5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bucket_name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Put your bucket name below and make sure you can access it without an error\n",
    "bucket_name = 'bucket_for_index' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh':\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e69a501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "body_id_rdd = parquetFile.select(\"text\", \"id\").rdd\n",
    "anchor_id_rdd = parquetFile.select(\"anchor_text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947da8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/25 11:41:42 WARN TaskSetManager: Lost task 120.0 in stage 2.0 (TID 182) (cluster-fb6c-w-2.c.irass3-413914.internal executor 6): org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file gs://bucket_for_index/project_pageRank/pageRank.pkl. Details: \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:730)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:573)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n",
      "Caused by: java.lang.RuntimeException: gs://bucket_for_index/project_pageRank/pageRank.pkl is not a Parquet file. Expected magic number at tail, but found [-85, 10, 117, 46]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$lzycompute$1(ParquetFileFormat.scala:302)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$1(ParquetFileFormat.scala:301)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:306)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:306)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:310)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n",
      "\t... 17 more\n",
      "\n",
      "24/02/25 11:41:43 ERROR TaskSetManager: Task 120 in stage 2.0 failed 4 times; aborting job\n",
      "24/02/25 11:41:43 WARN TaskSetManager: Lost task 79.0 in stage 2.0 (TID 141) (cluster-fb6c-w-0.c.irass3-413914.internal executor 1): TaskKilled (Stage cancelled)\n",
      "24/02/25 11:41:43 WARN TaskSetManager: Lost task 83.0 in stage 2.0 (TID 145) (cluster-fb6c-w-1.c.irass3-413914.internal executor 2): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 120 in stage 2.0 failed 4 times, most recent failure: Lost task 120.3 in stage 2.0 (TID 188) (cluster-fb6c-w-2.c.irass3-413914.internal executor 5): org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file gs://bucket_for_index/project_pageRank/pageRank.pkl. Details: \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:730)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:573)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\nCaused by: java.lang.RuntimeException: gs://bucket_for_index/project_pageRank/pageRank.pkl is not a Parquet file. Expected magic number at tail, but found [-85, 10, 117, 46]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$lzycompute$1(ParquetFileFormat.scala:302)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$1(ParquetFileFormat.scala:301)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:306)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:306)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:310)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file gs://bucket_for_index/project_pageRank/pageRank.pkl. Details: \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:730)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:573)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\nCaused by: java.lang.RuntimeException: gs://bucket_for_index/project_pageRank/pageRank.pkl is not a Parquet file. Expected magic number at tail, but found [-85, 10, 117, 46]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$lzycompute$1(ParquetFileFormat.scala:302)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$1(ParquetFileFormat.scala:301)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:306)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:306)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:310)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3332/2961186741.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_id_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1519\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \"\"\"\n\u001b[0;32m-> 1521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[NumberOrArray]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStatCounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \"\"\"\n\u001b[0;32m-> 1508\u001b[0;31m         return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   1509\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 120 in stage 2.0 failed 4 times, most recent failure: Lost task 120.3 in stage 2.0 (TID 188) (cluster-fb6c-w-2.c.irass3-413914.internal executor 5): org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file gs://bucket_for_index/project_pageRank/pageRank.pkl. Details: \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:730)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:573)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\nCaused by: java.lang.RuntimeException: gs://bucket_for_index/project_pageRank/pageRank.pkl is not a Parquet file. Expected magic number at tail, but found [-85, 10, 117, 46]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$lzycompute$1(ParquetFileFormat.scala:302)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$1(ParquetFileFormat.scala:301)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:306)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:306)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:310)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file gs://bucket_for_index/project_pageRank/pageRank.pkl. Details: \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:730)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:573)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:725)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2067)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\nCaused by: java.lang.RuntimeException: gs://bucket_for_index/project_pageRank/pageRank.pkl is not a Parquet file. Expected magic number at tail, but found [-85, 10, 117, 46]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$lzycompute$1(ParquetFileFormat.scala:302)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.entireParquetFooter$1(ParquetFileFormat.scala:301)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:306)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:306)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:310)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "print(body_id_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(achor_id_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac891c2",
   "metadata": {
    "id": "13ZX4ervQkku"
   },
   "source": [
    "***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c3f5e",
   "metadata": {
    "id": "c0b0f215"
   },
   "source": [
    "# Building an inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f2044",
   "metadata": {
    "id": "02f81c72"
   },
   "source": [
    "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9696d6c",
   "metadata": {},
   "source": [
    "# Building an Anchor Index for the Project #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83add20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "import math\n",
    "\n",
    "def PRINT(text) -> None: print(f'{\"-\"*80}\\n{text}\\n{\"-\"*80}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76616fa4",
   "metadata": {},
   "source": [
    "## Calculate Documents L2 Normalization ##\n",
    "\n",
    "In the next step we calculate L2 Norm for each document text.\n",
    "\n",
    "The ending result will be dictionary which maps -> (key, value) to (doc_id, doc_l2_norm_value)\n",
    "\n",
    "We will need that in order to preform CosinSimilarity in the next steps of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c83f9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                            \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                            \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                            \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd3637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, to_stem):\n",
    "    clean_text = []\n",
    "    text = text.lower()\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text)]\n",
    "    for token in tokens:\n",
    "        if token not in all_stopwords:\n",
    "            if to_stem:\n",
    "                token = stemmer.stem(token)\n",
    "                clean_text.append(token)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1244aaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kind', 'test', 'hello', 'test', 'test', 'kind']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some kind OF test hello test test kind\"\n",
    "\n",
    "text = tokenize(text, True)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e7a69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hell', 'way', 'add', 'add', 'add', 'function']\n",
      "3.4641016151377544\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def l2_norm(text):\n",
    "    # Count the occurrences of each unique word\n",
    "    word_counts = Counter(text)\n",
    "    # Get the counts of each unique word as a list\n",
    "    counts_list = [word_counts[word]**2 for word in set(text)]\n",
    "    l2_sum = 0\n",
    "    for word_num in counts_list:\n",
    "        l2_sum += word_num\n",
    "    l2 = math.sqrt(l2_sum)\n",
    "    #l2_norm = math.sqrt(sum(x**2 for x in counts_list))\n",
    "    return l2\n",
    "\n",
    "text = \"This is some hell of a way just to add add add this function\"\n",
    "text_tok = tokenize(text, True)\n",
    "print(text_tok)\n",
    "print(l2_norm(text_tok))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0935c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "rdd_ = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4177d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_norm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30565a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_doc_l2_norm(row):\n",
    "    doc_id = row['id']\n",
    "    text = row['text']\n",
    "    tok_text = tokenize(text, True)\n",
    "    return (doc_id, l2_norm(tok_text))\n",
    "\n",
    "doc_norm_rdd = rdd_.map(calculate_doc_l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60515d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 17\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_norm = dict(doc_norm_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8a77f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_pkl' \n",
    "doc_l2_norm='doc_l2_norm'\n",
    "bucket_name='bucket_for_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aae1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{doc_l2_norm}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(doc_norm, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1519149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 4045403, Value: 125.50298801223818\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045413, Value: 28.319604517012593\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045419, Value: 38.57460304397182\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045426, Value: 22.891046284519195\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045432, Value: 46.119410230400824\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045456, Value: 79.39773296511683\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045466, Value: 36.15245496505044\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045471, Value: 40.07492981902776\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045479, Value: 108.4250893474384\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045516, Value: 15.394804318340652\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(doc_norm.items())[:10]:\n",
    "    PRINT(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b5d19-0b88-45a0-9628-51a2f13857aa",
   "metadata": {},
   "source": [
    "## Explore the RDD Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1bd28f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'text', 'anchor_text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "doc_text_pairs_test = parquetFile.rdd\n",
    "\n",
    "# Convert RDD to DataFrame to view column names\n",
    "doc_text_pairs_df = doc_text_pairs_test.toDF()\n",
    "\n",
    "# Print column names\n",
    "print(doc_text_pairs_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2aef22c-94ba-491f-84de-c20ee9130e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|     id|               title|                text|         anchor_text|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|4045403|Foster Air Force ...|'''Foster Air For...|[{1176764, Tactic...|\n",
      "|4045413|     Torino Palavela|'''Palavela''', f...|[{77743, 2006 Win...|\n",
      "|4045419|   Mad About the Boy|\"'''Mad About the...|[{34028256, Joyce...|\n",
      "|4045426|       Shayne Breuer|'''Shayne Breuer'...|[{1838386, Woodvi...|\n",
      "|4045432|         Parantaka I|'''Parantaka Chol...|[{1511716, Aditya...|\n",
      "|4045456|Arundel (UK Parli...|'''Arundel''' was...|[{4665376, Arunde...|\n",
      "|4045466|     Andrew Martinez|'''Luis Andrew Ma...|[{4860, Berkeley,...|\n",
      "|4045471|    Vancouver VooDoo|The '''Vancouver ...|[{32706, Vancouve...|\n",
      "|4045479|     Invisible plane|The '''Invisible ...|[{2260539, Ross A...|\n",
      "|4045516|    Shopping channel|'''Shopping chann...|[{592899, special...|\n",
      "|4045519|      Turgay (river)|The '''Turgay''' ...|[{16642, Kazakhst...|\n",
      "|4045523|              Turgay|'''Turgay''' is a...|[{29992, Turkish}...|\n",
      "|4045525|Heinrich Johann N...|'''Heinrich Johan...|[{5178678, Roodt-...|\n",
      "|4045532|                 KUT|thumb|Belo Center...|[{1998, Austin, T...|\n",
      "|4045544|          Dodge Cove|thumb|upright=.9|...|[{232346, Unincor...|\n",
      "|4045546|            Triphone|In linguistics, a...|[{22760983, lingu...|\n",
      "|4045554|Government House ...|'''Government Hou...|[{6898431, Barrin...|\n",
      "|4045575|           Bud Abell|'''Harry Everett'...|[{492475, Linebac...|\n",
      "|4045577|  Susette La Flesche|'''Susette La Fle...|[{4049833, Thomas...|\n",
      "|4045581|Pomme de Terre Ri...|The '''Pomme de T...|[{3434750, United...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_text_pairs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701811af",
   "metadata": {
    "id": "gaaIoFViXyTg"
   },
   "source": [
    "## Import Inverted Index .py File ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121fe102",
   "metadata": {
    "id": "04371c88",
    "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_gcp.py\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c101a8",
   "metadata": {
    "id": "2d3285d8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c259c402",
   "metadata": {
    "id": "2477a5b9"
   },
   "outputs": [],
   "source": [
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2fc6e-f41b-4141-83e0-2af895322881",
   "metadata": {},
   "source": [
    "## Pipeline Code to Generate the Index ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22fbc75-62f2-4e06-9cbe-865ae1178d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# Getting tokens from the text while removing punctuations.\n",
    "def filter_tokens(tokens, tokens2remove=None, use_stemming=False):\n",
    "  ''' \n",
    "  The function takes a list of tokens, filters out `tokens2remove` and\n",
    "      stem the tokens using `stemmer`.\n",
    "  Parameters:\n",
    "  -----------\n",
    "  tokens: list of str.\n",
    "    Input tokens.\n",
    "  tokens2remove: frozenset.\n",
    "    Tokens to remove (before stemming).\n",
    "  use_stemming: bool.\n",
    "    If true, apply stemmer.stem on tokens.\n",
    "  Returns:\n",
    "  --------\n",
    "  list of tokens from the text.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  if tokens2remove != None:\n",
    "    tokens = [token for token in tokens if token not in tokens2remove]\n",
    "  if use_stemming:\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ad8fea",
   "metadata": {
    "id": "a4b6ee29",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-token2bucket",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "def token2bucket_id(token):\n",
    "  return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count(text, id):\n",
    "  ''' \n",
    "  Count the frequency of each word in `text` (tf) that is not included in \n",
    "  `all_stopwords` and return entries that will go into our posting lists. \n",
    "  Parameters:\n",
    "    text: str ,Text of one document\n",
    "    id: int ,Document id\n",
    "  Returns: List of tuples, A list of (token, (doc_id, tf)) pairs \n",
    "  '''\n",
    "\n",
    "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "  \n",
    "  c = Counter([tok for tok in tokens if (tok not in all_stopwords)])\n",
    "  return [(item[0],(id,item[1])) for item in c.items()]\n",
    "\n",
    "def word_count_with_stemming(text, id):\n",
    "  ''' \n",
    "  Count the frequency of each word in `text` (tf) that is not included in \n",
    "  `all_stopwords` and return entries that will go into our posting lists. \n",
    "  Parameters:\n",
    "    text: str ,Text of one document\n",
    "    id: int ,Document id\n",
    "  Returns: List of tuples, A list of (token, (doc_id, tf)) pairs \n",
    "  '''\n",
    "\n",
    "  tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower())]\n",
    "  \n",
    "  c = Counter([tok for tok in tokens if (tok not in all_stopwords)])\n",
    "  return [(item[0],(id,item[1])) for item in c.items()]\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "  ''' \n",
    "  Returns a sorted posting list by wiki_id.\n",
    "  Parameters: unsorted_pl: list of (wiki_id, tf) tuples \n",
    "  Returns: A sorted posting list.\n",
    "  '''\n",
    "  return sorted(unsorted_pl)\n",
    "\n",
    "def calculate_df(postings):\n",
    "  ''' \n",
    "  Takes a posting list RDD and calculate the df for each token.\n",
    "  Parameters: postings: RDD ,An RDD where each element is a (token, posting_list) pair.\n",
    "  Returns:RDD ,An RDD where each element is a (token, df) pair.\n",
    "  '''\n",
    "  return postings.map(lambda x: (x[0],len(x[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir_, bucket_name_):\n",
    "  ''' \n",
    "  A function that partitions the posting lists into buckets, writes out \n",
    "  all posting lists in a bucket to disk, and returns the posting locations for \n",
    "  each bucket. Partitioning done by using `token2bucket`.\n",
    "  Parameters:\n",
    "  \n",
    "  base_dir : string - Name for index directory\n",
    "  bucket_name : string - Name of the bucket we want to store our inverted index directory (base_dir)\n",
    "  -----------\n",
    "    postings: RDD , An RDD where each item is a (w, posting_list) pair.\n",
    "  Returns: RDD\n",
    "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "      posting locations maintain a list for each word of file locations and \n",
    "      offsets its posting list was written to.\n",
    "  '''\n",
    "  return postings.map(lambda x: (token2bucket_id(x[0]),x)).groupByKey().map(lambda x: InvertedIndex.write_a_posting_list(x,\n",
    "                                                                                                                         base_dir= base_dir_,\n",
    "                                                                                                                         bucket_name=bucket_name_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ceaa4-b94f-4c5f-99bf-8b400c9aedbd",
   "metadata": {},
   "source": [
    "## Generate Inverted Index For Title & Id Pairs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58176c45-b468-481d-983f-781b51f90705",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id_pairs = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78c4b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====================================================> (121 + 3) / 124]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of title & id pairs -> 6348910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_pairs = title_id_pairs.count()\n",
    "print(f'Number of title & id pairs -> {count_pairs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddaa48-7619-4e1c-81e1-53aa0c9f5194",
   "metadata": {},
   "source": [
    "### Generate Posting Lists for Title Index & Save them ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c8764e",
   "metadata": {
    "id": "0b5d7296",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-index_construction",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# time the index creation time\n",
    "t_start = time()\n",
    "\n",
    "# word counts map\n",
    "word_counts = title_id_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# filtering postings and calculate df\n",
    "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "# partition posting lists and write out\n",
    "_ = partition_postings_and_write(postings_filtered,\n",
    "                                 base_dir_='title_index_directory',\n",
    "                                 bucket_name_='inverted_indexes_bucket').collect()\n",
    "\n",
    "index_const_time = time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dbc0e14",
   "metadata": {
    "id": "348pECY8cH-T",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-index_const_time",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test index construction time\n",
    "assert index_const_time < 60*120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66660993-d427-452d-83e6-fa56958dcf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_final_indexes/title_index.pkl\n",
      "title_index_directory/0_000.bin\n",
      "title_index_directory/0_posting_locs.pickle\n",
      "title_index_directory/100_000.bin\n",
      "title_index_directory/100_posting_locs.pickle\n",
      "title_index_directory/101_000.bin\n",
      "title_index_directory/101_posting_locs.pickle\n",
      "title_index_directory/102_000.bin\n",
      "title_index_directory/102_posting_locs.pickle\n",
      "title_index_directory/103_000.bin\n",
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "blobs_test = client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                              prefix='title_index_directory')\n",
    "count = 0\n",
    "# Iterate over the blobs\n",
    "for blob in blobs_test:\n",
    "    if count == 10: break\n",
    "    count+=1\n",
    "    print(blob.name)\n",
    "    \n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab3296f4",
   "metadata": {
    "id": "Opl6eRNLM5Xv",
    "nbgrader": {
     "grade": true,
     "grade_id": "collect-posting",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                             prefix='title_index_directory'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "    \n",
    "PRINT('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f66e3a",
   "metadata": {
    "id": "VhAV0A6dNZWY"
   },
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5d2cfb6",
   "metadata": {
    "id": "54vqT_0WNc3w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted = InvertedIndex()\n",
    "\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted.posting_locs = super_posting_locs\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# write the global stats out\n",
    "inverted.write_index(base_dir='project_final_indexes', \n",
    "                     name='title_index',\n",
    "                     bucket_name='inverted_indexes_bucket')\n",
    "\n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def55262-7232-4a30-8f0d-b19de810877f",
   "metadata": {},
   "source": [
    "### Visualize the Index ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53ee777a-3317-45a1-846c-7f254493b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(base_dir, name, bucket_name):\n",
    "    # Initialize the client\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    # Define the path to the index file in the bucket\n",
    "    index_path = f'{base_dir}/{name}.pkl'\n",
    "    \n",
    "    # Get the blob (file) from the bucket\n",
    "    blob = bucket.blob(index_path)\n",
    "    \n",
    "    # Download the blob into memory\n",
    "    index_data = blob.download_as_string()\n",
    "    \n",
    "    # Load the index from the downloaded data\n",
    "    inverted_index = pickle.loads(index_data)\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d9dc349a-4e02-4980-986c-651ebeea2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = load_index(base_dir='project_final_indexes', \n",
    "                            name='title_index',\n",
    "                            bucket_name='inverted_indexes_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b5fe63c-088a-468d-b280-e2db172faefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_locs = inverted_index.posting_locs\n",
    "df_data = inverted_index.df\n",
    "\n",
    "# Convert the posting_locs dictionary to a DataFrame\n",
    "posting_locs_df = pd.DataFrame.from_dict(posting_locs, orient='index', columns=['Posting List'])\n",
    "\n",
    "# Convert the df_data dictionary to a DataFrame\n",
    "df_data_df = pd.DataFrame.from_dict(df_data, orient='index', columns=['Document Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "08359fa8-c762-41d8-bced-4f6ba279ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_df_sorted = df_data_df.sort_values(by='Document Frequency', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b4263b2-452f-4514-8954-4939701f68bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Data Frame shape -> (26981, 1)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'Data Frame shape -> {df_data_df_sorted.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8021387-04a6-4aab-b7a3-e8c857f8ad31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>skyhawks</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ios</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makino</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sewall</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euchromius</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberator</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoder</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wasserman</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pani</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Document Frequency\n",
       "skyhawks                    51\n",
       "ios                         51\n",
       "makino                      51\n",
       "405                         51\n",
       "sewall                      51\n",
       "euchromius                  51\n",
       "liberator                   51\n",
       "yoder                       51\n",
       "wasserman                   51\n",
       "pani                        51"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a91101c3-f253-4b3a-8600-f55ee85a3bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done with Title Index !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT('Done with Title Index !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3ce0b-ebcb-40bd-918e-a147af41ee64",
   "metadata": {},
   "source": [
    "## Generate Inverted Index for Title & ID Pairs using Stemming ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "690b178c-1b8a-4ab0-ad4c-61f7bb75131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# time the index creation time\n",
    "t_start = time()\n",
    "\n",
    "# word counts with stemming map\n",
    "word_counts_with_stemming = title_id_pairs.flatMap(lambda x: word_count_with_stemming(x[0], x[1]))\n",
    "postings = word_counts_with_stemming.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# filtering postings and calculate df\n",
    "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "# partition posting lists and write out\n",
    "_ = partition_postings_and_write(postings_filtered,\n",
    "                                 base_dir_='title_stem_index_directory',\n",
    "                                 bucket_name_='inverted_indexes_bucket').collect()\n",
    "\n",
    "index_const_time = time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05a186e1-8311-49fc-86e9-cdaa4bc8f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_stem_index_directory/0_000.bin\n",
      "title_stem_index_directory/0_posting_locs.pickle\n",
      "title_stem_index_directory/100_000.bin\n",
      "title_stem_index_directory/100_posting_locs.pickle\n",
      "title_stem_index_directory/101_000.bin\n",
      "title_stem_index_directory/101_posting_locs.pickle\n",
      "title_stem_index_directory/102_000.bin\n",
      "title_stem_index_directory/102_posting_locs.pickle\n",
      "title_stem_index_directory/103_000.bin\n",
      "title_stem_index_directory/103_posting_locs.pickle\n",
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "blobs_test = client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                              prefix='title_stem_index_directory')\n",
    "count = 0\n",
    "# Iterate over the blobs\n",
    "for blob in blobs_test:\n",
    "    if count == 10:break\n",
    "    count+=1\n",
    "    print(blob.name)\n",
    "    \n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "492deb11-d4e2-4120-b74a-429624fbd054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                             prefix='title_stem_index_directory'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "    \n",
    "PRINT('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d651b3fa-84bd-4b7b-a25c-bcd80ca41fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted = InvertedIndex()\n",
    "\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted.posting_locs = super_posting_locs\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# write the global stats out\n",
    "inverted.write_index(base_dir='project_final_indexes', \n",
    "                     name='title_stem_index',\n",
    "                     bucket_name='inverted_indexes_bucket')\n",
    "\n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ed6ef-9451-4e1e-a011-365deacf6f76",
   "metadata": {},
   "source": [
    "### Visualize the Index ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ba32d80-4052-44b3-985e-c717c740e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = load_index(base_dir='project_final_indexes', \n",
    "                            name='title_stem_index',\n",
    "                            bucket_name='inverted_indexes_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93e784ce-ab21-4337-8a48-e2839885aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_locs = inverted_index.posting_locs\n",
    "df_data = inverted_index.df\n",
    "\n",
    "# Convert the posting_locs dictionary to a DataFrame\n",
    "posting_locs_df = pd.DataFrame.from_dict(posting_locs, orient='index', columns=['Posting List'])\n",
    "\n",
    "# Convert the df_data dictionary to a DataFrame\n",
    "df_data_df = pd.DataFrame.from_dict(df_data, orient='index', columns=['Document Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc776a83-f726-42ae-9a07-aa91cc4f201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_df_sorted = df_data_df.sort_values(by='Document Frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf351c2f-e774-4b35-986c-9a6664e16e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Data Frame shape -> (25230, 1)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'Data Frame shape -> {df_data_df_sorted.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db416349-3e49-4651-bc49-ba75b640bd99",
   "metadata": {},
   "source": [
    "We can see that by using stemming we reduced the number of words from *26981* to *25230* !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d73abe8-8bc0-4cc7-bbd5-083b7f649d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>list</th>\n",
       "      <td>119863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>footbal</th>\n",
       "      <td>81127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>62624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <td>60826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Document Frequency\n",
       "list                 119863\n",
       "footbal               81127\n",
       "film                  62624\n",
       "station               60826"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_df_sorted.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c0661-462c-44d9-b161-f68450ce1c74",
   "metadata": {},
   "source": [
    "## Generate Inverted Idex For Body & Id Pairs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eeea605f-717f-4661-a20f-f78781426035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===========>                                           (26 + 5) / 124]\r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "body_id_pairs = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f04df369-d1f8-4421-a17f-7d457766fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 101\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# time the index creation time\n",
    "t_start = time()\n",
    "\n",
    "# word counts map\n",
    "word_counts = body_id_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# filtering postings and calculate df\n",
    "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "# partition posting lists and write out\n",
    "_ = partition_postings_and_write(postings_filtered,\n",
    "                                 base_dir_='body_index_directory',\n",
    "                                 bucket_name_='inverted_indexes_bucket').collect()\n",
    "\n",
    "index_const_time = time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63f8f943-82f8-47f2-9de9-354168f8484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Total amount of time: 75.40 minutes\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'Total amount of time: {(index_const_time / 60):.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "465f49e2-5536-41ec-88cb-a98e459d1019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_index_directory/0_000.bin\n",
      "body_index_directory/0_001.bin\n",
      "body_index_directory/0_002.bin\n",
      "body_index_directory/0_003.bin\n",
      "body_index_directory/0_004.bin\n",
      "body_index_directory/0_005.bin\n",
      "body_index_directory/0_006.bin\n",
      "body_index_directory/0_007.bin\n",
      "body_index_directory/0_008.bin\n",
      "body_index_directory/0_009.bin\n",
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "blobs_test = client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                              prefix='body_index_directory')\n",
    "count = 0\n",
    "# Iterate over the blobs\n",
    "for blob in blobs_test:\n",
    "    if count == 10:break\n",
    "    count+=1\n",
    "    print(blob.name)\n",
    "    \n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e509e28b-eea4-4b94-a2f2-3cbc7ed5b724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                             prefix='body_index_directory'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "    \n",
    "PRINT('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c3e5957c-ceef-4db9-b06f-dd9eec01c15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted = InvertedIndex()\n",
    "\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted.posting_locs = super_posting_locs\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# write the global stats out\n",
    "inverted.write_index(base_dir='project_final_indexes', \n",
    "                     name='body_index',\n",
    "                     bucket_name='inverted_indexes_bucket')\n",
    "\n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535470bd-85bd-4f00-99f3-0458c79d2cf4",
   "metadata": {},
   "source": [
    "## Generate Inverted Index for Body & ID Pairs using Stemming ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8e540dad-9afe-4990-9ada-503d1173287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.              (37 + 5) / 124]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/miniconda3/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29767/752836224.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpostings_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mw2df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostings_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mw2df_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# partition posting lists and write out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2224\u001b[0m         \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m         \"\"\"\n\u001b[0;32m-> 2226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[Tuple[K, V]]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"RDD[K]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# time the index creation time\n",
    "t_start = time()\n",
    "\n",
    "# word counts with stemming map\n",
    "word_counts_with_stemming = body_id_pairs.flatMap(lambda x: word_count_with_stemming(x[0], x[1]))\n",
    "postings = word_counts_with_stemming.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# filtering postings and calculate df\n",
    "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "# partition posting lists and write out\n",
    "_ = partition_postings_and_write(postings_filtered,\n",
    "                                 base_dir_='body_stem_index_directory',\n",
    "                                 bucket_name_='inverted_indexes_bucket').collect()\n",
    "\n",
    "index_const_time = time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14206bf1-1e9f-4030-9766-0f665f0e65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT(f'Total amount of time: {(index_const_time / 60):.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5b9e6-146d-4494-b00e-05ece865c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_test = client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                              prefix='body_stem_index_directory')\n",
    "count = 0\n",
    "# Iterate over the blobs\n",
    "for blob in blobs_test:\n",
    "    if count == 10:break\n",
    "    count+=1\n",
    "    print(blob.name)\n",
    "    \n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069d1b7-eeb7-43bd-a903-6182bc24e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                             prefix='body_stem_index_directory'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "    \n",
    "PRINT('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e01f5a-3bf7-46ab-abde-1e6e49e144c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted = InvertedIndex()\n",
    "\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted.posting_locs = super_posting_locs\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# write the global stats out\n",
    "inverted.write_index(base_dir='project_final_indexes', \n",
    "                     name='body_stem_index',\n",
    "                     bucket_name='inverted_indexes_bucket')\n",
    "\n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919aa8f6-5404-44dc-840e-d45bcbe830a0",
   "metadata": {},
   "source": [
    "## Generate Document Lenghs for Body & Title ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bbc474-bb74-4dae-8dca-59c5c0f4005f",
   "metadata": {},
   "source": [
    "### Generate Body Document Length Dictionary ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b275a18-fe81-46ce-8112-2eee642a5020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "body_id_rdd = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3682cb66-0ad8-4ece-84d4-6c9a9429ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_dictionary_lenght = {} # maps (key,value)=(doc_id, body_doc_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59120f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized value: 0.13483997249264842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===>                                                    (7 + 6) / 124]\r"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Example 1D array\n",
    "array_1d = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Reshape the 1D array into a 2D array with one row\n",
    "array_2d = array_1d.reshape(1, -1)\n",
    "\n",
    "# Normalize the 2D array along axis 1 (row-wise) using L2 normalization\n",
    "normalized_array = normalize(array_2d, norm='l2', axis=1)\n",
    "\n",
    "# Get the normalized value (a single number)\n",
    "normalized_value = normalized_array[0, 0]\n",
    "\n",
    "print(\"Normalized value:\", normalized_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93cce016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===>                                                    (7 + 6) / 124]\r"
     ]
    }
   ],
   "source": [
    "def count_unique_words(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Count the occurrences of each word\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Get the counts of unique words\n",
    "    unique_word_counts = list(word_counts.values())\n",
    "    \n",
    "    return unique_word_counts\n",
    "\n",
    "text = 'pizza pizza pasta'\n",
    "result = count_unique_words(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b49f819c-beec-4baa-b356-412a6bcfe672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.][Stage 18:> (0 + 0) / 124]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/miniconda3/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5129/2959437919.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Collect the results as a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbody_dictionary_lenght\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/miniconda3/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def calculate_doc_length(row):\n",
    "    print('hi')\n",
    "    doc_id = row['id']\n",
    "    text = row['text']\n",
    "    text_tok = tokenize(text, True)\n",
    "    print(text_tok)\n",
    "    return (doc_id, len(text_tok))\n",
    "\n",
    "# Apply the function to each row of the RDD and map it to (doc_id, doc_length) pairs\n",
    "doc_lengths_rdd = body_id_rdd.map(calculate_doc_length)\n",
    "\n",
    "# Collect the results as a dictionary\n",
    "body_dictionary_lenght = dict(doc_lengths_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d13e9e-dd44-4f80-91b2-1f6958c4186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_final_indexes' \n",
    "dl_name='body_dictionary_length'\n",
    "bucket_name='inverted_indexes_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "532f5af0-9ba3-4d94-b114-ca6be79127e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket(bucket_name):\n",
    "    return storage.Client('irprojectilayvictor').bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a4a19b8-4a42-43da-a960-8872cd81e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{dl_name}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(body_dictionary_lenght, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b2d1c-bf05-458a-b2d3-93ef28324899",
   "metadata": {},
   "source": [
    "### Verify that the Dictionary Saved Successfully ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187907c6-5ceb-4a28-ae40-d55fe264060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Download the pickled file from the bucket\n",
    "blob = bucket.blob(path)\n",
    "blob.download_to_filename(path)\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394f473a-0dc6-4179-9109-31d056d7df86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 4045403, Value: 14477\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045413, Value: 2069\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045419, Value: 5353\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(loaded_dict.items())[:3]:\n",
    "    PRINT(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33bea4-3bc5-446a-b101-ebcb753d31b3",
   "metadata": {},
   "source": [
    "### Generate Title Document Lenght Dictionary ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47dbf1c-792e-4f4a-8ee5-abfa5746b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "title_id_rdd = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67663ef-1323-4e5a-ac01-d7c795ba3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dictionary = {} # maps (key, value)=(doc_id, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef19c7c4-178b-48fc-8a68-0ab1db8acdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def calculate_title(row):\n",
    "    doc_id = row['id']\n",
    "    title = row['title']\n",
    "    return (doc_id, title)\n",
    "\n",
    "# Apply the function to each row of the RDD and map it to (doc_id, title) pairs\n",
    "title_rdd = title_id_rdd.map(calculate_title)\n",
    "\n",
    "title_dictionary = dict(title_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d70a7e2e-dacb-4200-bda4-864ec3b1d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 4045403, Value: Foster Air Force Base\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045413, Value: Torino Palavela\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045419, Value: Mad About the Boy\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(title_dictionary.items())[:3]:\n",
    "    PRINT(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa352204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2d8ba7a-aa00-46b1-913a-b8c3d816611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_final_indexes' \n",
    "dl_name='doc_norm'\n",
    "bucket_name='inverted_indexes_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad1c9397-392a-4c0e-b08d-8199fcbb8b16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11646/3428522922.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_from_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_norm' is not defined"
     ]
    }
   ],
   "source": [
    "path = str(Path(base_dir) / f'{dl_name}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(doc_norm, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5ebd4-0f9b-459d-8d85-74a56fcd1bac",
   "metadata": {},
   "source": [
    "### Verify that the Dictionary Saved Successfully ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303ff7ad-c3fe-4feb-9094-d7128d4b142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Download the pickled file from the bucket\n",
    "blob = bucket.blob(path)\n",
    "blob.download_to_filename(path)\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77e6996f-d680-4624-8047-57f5ad90f226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 4045403, Value: 21\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045413, Value: 15\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045419, Value: 17\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(loaded_dict.items())[:3]:\n",
    "    PRINT(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dee14",
   "metadata": {
    "id": "fc0667a9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a6d655c112e79c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Execute PageRank & Save ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31a516e2",
   "metadata": {
    "id": "yVjnTvQsegc-"
   },
   "outputs": [],
   "source": [
    "def generate_graph(pages):\n",
    "  ''' \n",
    "  Compute the directed graph generated by wiki links.\n",
    "  Parameters: An RDD where each row consists of one wikipedia articles with 'id' and \n",
    "      'anchor_text'.\n",
    "  Returns: \n",
    "   - edges: RDD\n",
    "      An RDD where each row represents an edge in the directed graph created by\n",
    "      the wikipedia links. The first entry should the source page id and the \n",
    "      second entry is the destination page id. No duplicates should be present. \n",
    "   - vertices: RDD\n",
    "      An RDD where each row represents a vetrix (node) in the directed graph \n",
    "      created by the wikipedia links. No duplicates should be present. \n",
    "  '''\n",
    "  edges_w_dup = pages.flatMap(lambda page: [(page[0],anchor[0]) for anchor in page[1]])\n",
    "  edges = edges_w_dup.distinct()\n",
    "  vertices = edges.flatMap(lambda x: x)\n",
    "  vertices = vertices.distinct()\n",
    "  vertices = vertices.map(lambda x: Row(x)) #converting entries to a format that fits toDF() func up next\n",
    "  return edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bc05ba3",
   "metadata": {
    "id": "db005700",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-PageRank",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/25 08:20:31 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000005 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:20:31.236]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:20:31.236]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:20:31.236]Killed by external signal\n",
      ".\n",
      "24/02/25 08:20:31 ERROR YarnScheduler: Lost executor 4 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000005 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:20:31.236]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:20:31.236]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:20:31.236]Killed by external signal\n",
      ".\n",
      "24/02/25 08:20:31 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1708846121196_0001_01_000005 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:20:31.236]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:20:31.236]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:20:31.236]Killed by external signal\n",
      ".\n",
      "24/02/25 08:20:31 WARN TaskSetManager: Lost task 19.0 in stage 32.0 (TID 902) (cluster-fb6c-w-2.c.irass3-413914.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000005 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:20:31.236]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:20:31.236]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:20:31.236]Killed by external signal\n",
      ".\n",
      "24/02/25 08:21:11 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000016 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:21:11.523]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:21:11.523]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:21:11.524]Killed by external signal\n",
      ".\n",
      "24/02/25 08:21:11 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 15 for reason Container from a bad node: container_1708846121196_0001_01_000016 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:21:11.523]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:21:11.523]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:21:11.524]Killed by external signal\n",
      ".\n",
      "24/02/25 08:21:11 ERROR YarnScheduler: Lost executor 15 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000016 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:21:11.523]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:21:11.523]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:21:11.524]Killed by external signal\n",
      ".\n",
      "24/02/25 08:21:11 WARN TaskSetManager: Lost task 24.0 in stage 32.0 (TID 907) (cluster-fb6c-w-0.c.irass3-413914.internal executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000016 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:21:11.523]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:21:11.523]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:21:11.524]Killed by external signal\n",
      ".\n",
      "24/02/25 08:25:31 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000014 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:25:31.650]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:25:31.650]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:25:31.651]Killed by external signal\n",
      ".\n",
      "24/02/25 08:25:31 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 13 for reason Container from a bad node: container_1708846121196_0001_01_000014 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:25:31.650]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:25:31.650]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:25:31.651]Killed by external signal\n",
      ".\n",
      "24/02/25 08:25:31 ERROR YarnScheduler: Lost executor 13 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000014 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:25:31.650]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:25:31.650]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:25:31.651]Killed by external signal\n",
      ".\n",
      "24/02/25 08:25:31 WARN TaskSetManager: Lost task 36.0 in stage 32.0 (TID 921) (cluster-fb6c-w-1.c.irass3-413914.internal executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000014 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:25:31.650]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:25:31.650]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:25:31.651]Killed by external signal\n",
      ".\n",
      "24/02/25 08:26:34 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000017 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:26:34.286]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:26:34.286]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:26:34.287]Killed by external signal\n",
      ".\n",
      "24/02/25 08:26:34 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 16 for reason Container from a bad node: container_1708846121196_0001_01_000017 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:26:34.286]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:26:34.286]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:26:34.287]Killed by external signal\n",
      ".\n",
      "24/02/25 08:26:34 ERROR YarnScheduler: Lost executor 16 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000017 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:26:34.286]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:26:34.286]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:26:34.287]Killed by external signal\n",
      ".\n",
      "24/02/25 08:26:34 WARN TaskSetManager: Lost task 37.0 in stage 32.0 (TID 922) (cluster-fb6c-w-0.c.irass3-413914.internal executor 16): ExecutorLostFailure (executor 16 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000017 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:26:34.286]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:26:34.286]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:26:34.287]Killed by external signal\n",
      ".\n",
      "24/02/25 08:28:37 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000015 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:28:36.980]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:28:36.980]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:28:36.980]Killed by external signal\n",
      ".\n",
      "24/02/25 08:28:37 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 14 for reason Container from a bad node: container_1708846121196_0001_01_000015 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:28:36.980]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:28:36.980]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:28:36.980]Killed by external signal\n",
      ".\n",
      "24/02/25 08:28:37 ERROR YarnScheduler: Lost executor 14 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000015 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:28:36.980]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:28:36.980]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:28:36.980]Killed by external signal\n",
      ".\n",
      "24/02/25 08:28:37 WARN TaskSetManager: Lost task 44.0 in stage 32.0 (TID 931) (cluster-fb6c-w-1.c.irass3-413914.internal executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000015 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:28:36.980]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:28:36.980]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:28:36.980]Killed by external signal\n",
      ".\n",
      "24/02/25 08:30:36 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000019 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:30:36.927]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:30:36.927]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:30:36.927]Killed by external signal\n",
      ".\n",
      "24/02/25 08:30:36 ERROR YarnScheduler: Lost executor 18 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000019 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:30:36.927]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:30:36.927]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:30:36.927]Killed by external signal\n",
      ".\n",
      "24/02/25 08:30:36 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 18 for reason Container from a bad node: container_1708846121196_0001_01_000019 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:30:36.927]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:30:36.927]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:30:36.927]Killed by external signal\n",
      ".\n",
      "24/02/25 08:30:36 WARN TaskSetManager: Lost task 57.0 in stage 32.0 (TID 945) (cluster-fb6c-w-2.c.irass3-413914.internal executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000019 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:30:36.927]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:30:36.927]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:30:36.927]Killed by external signal\n",
      ".\n",
      "24/02/25 08:31:18 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000020 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:31:17.766]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:31:17.766]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:31:17.771]Killed by external signal\n",
      ".\n",
      "24/02/25 08:31:18 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 19 for reason Container from a bad node: container_1708846121196_0001_01_000020 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:31:17.766]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:31:17.766]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:31:17.771]Killed by external signal\n",
      ".\n",
      "24/02/25 08:31:18 ERROR YarnScheduler: Lost executor 19 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000020 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:31:17.766]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:31:17.766]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:31:17.771]Killed by external signal\n",
      ".\n",
      "24/02/25 08:31:18 WARN TaskSetManager: Lost task 60.0 in stage 32.0 (TID 948) (cluster-fb6c-w-0.c.irass3-413914.internal executor 19): ExecutorLostFailure (executor 19 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000020 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:31:17.766]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:31:17.766]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:31:17.771]Killed by external signal\n",
      ".\n",
      "24/02/25 08:42:50 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000023 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:42:50.404]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:42:50.404]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:42:50.405]Killed by external signal\n",
      ".\n",
      "24/02/25 08:42:50 ERROR YarnScheduler: Lost executor 22 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000023 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:42:50.404]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:42:50.404]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:42:50.405]Killed by external signal\n",
      ".\n",
      "24/02/25 08:42:50 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 22 for reason Container from a bad node: container_1708846121196_0001_01_000023 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:42:50.404]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:42:50.404]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:42:50.405]Killed by external signal\n",
      ".\n",
      "24/02/25 08:42:50 WARN TaskSetManager: Lost task 11.0 in stage 47.0 (TID 1534) (cluster-fb6c-w-1.c.irass3-413914.internal executor 22): ExecutorLostFailure (executor 22 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000023 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:42:50.404]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:42:50.404]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:42:50.405]Killed by external signal\n",
      ".\n",
      "24/02/25 08:43:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_6 !\n",
      "24/02/25 08:43:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_10 !\n",
      "24/02/25 08:43:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_6 !\n",
      "24/02/25 08:43:11 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_10 !\n",
      "24/02/25 08:43:12 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000018 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:43:12.043]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:43:12.043]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:43:12.047]Killed by external signal\n",
      ".\n",
      "24/02/25 08:43:12 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 17 for reason Container from a bad node: container_1708846121196_0001_01_000018 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:43:12.043]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:43:12.043]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:43:12.047]Killed by external signal\n",
      ".\n",
      "24/02/25 08:43:12 ERROR YarnScheduler: Lost executor 17 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000018 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:43:12.043]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:43:12.043]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:43:12.047]Killed by external signal\n",
      ".\n",
      "24/02/25 08:43:12 WARN TaskSetManager: Lost task 22.0 in stage 47.0 (TID 1555) (cluster-fb6c-w-2.c.irass3-413914.internal executor 17): ExecutorLostFailure (executor 17 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000018 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:43:12.043]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:43:12.043]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:43:12.047]Killed by external signal\n",
      ".\n",
      "24/02/25 08:43:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_18 !\n",
      "24/02/25 08:43:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_16 !\n",
      "24/02/25 08:43:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_16 !\n",
      "24/02/25 08:43:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_18 !\n",
      "24/02/25 08:43:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_20 !\n",
      "24/02/25 08:43:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_13 !\n",
      "24/02/25 08:43:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_20 !\n",
      "24/02/25 08:43:21 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000027 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:43:21.650]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:43:21.651]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:43:21.651]Killed by external signal\n",
      ".\n",
      "24/02/25 08:43:21 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 26 for reason Container from a bad node: container_1708846121196_0001_01_000027 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:43:21.650]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:43:21.651]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:43:21.651]Killed by external signal\n",
      ".\n",
      "24/02/25 08:43:21 ERROR YarnScheduler: Lost executor 26 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000027 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:43:21.650]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:43:21.651]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:43:21.651]Killed by external signal\n",
      ".\n",
      "24/02/25 08:43:21 WARN TaskSetManager: Lost task 23.0 in stage 48.0 (TID 1562) (cluster-fb6c-w-0.c.irass3-413914.internal executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000027 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:43:21.650]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:43:21.651]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:43:21.651]Killed by external signal\n",
      ".\n",
      "24/02/25 08:44:35 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000026 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:44:35.175]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:44:35.175]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:44:35.175]Killed by external signal\n",
      ".\n",
      "24/02/25 08:44:35 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 25 for reason Container from a bad node: container_1708846121196_0001_01_000026 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:44:35.175]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:44:35.175]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:44:35.175]Killed by external signal\n",
      ".\n",
      "24/02/25 08:44:35 ERROR YarnScheduler: Lost executor 25 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000026 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:44:35.175]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:44:35.175]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:44:35.175]Killed by external signal\n",
      ".\n",
      "24/02/25 08:44:35 WARN TaskSetManager: Lost task 64.0 in stage 47.0 (TID 1641) (cluster-fb6c-w-1.c.irass3-413914.internal executor 25): ExecutorLostFailure (executor 25 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000026 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:44:35.175]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:44:35.175]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:44:35.175]Killed by external signal\n",
      ".\n",
      "24/02/25 08:44:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_55 !\n",
      "24/02/25 08:44:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_61 !\n",
      "24/02/25 08:44:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_55 !\n",
      "24/02/25 08:44:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_58 !\n",
      "24/02/25 08:44:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_58 !\n",
      "24/02/25 08:44:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_52 !\n",
      "24/02/25 08:44:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_61 !\n",
      "24/02/25 08:44:42 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000032 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:44:41.797]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:44:41.797]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:44:41.797]Killed by external signal\n",
      ".\n",
      "24/02/25 08:44:42 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 31 for reason Container from a bad node: container_1708846121196_0001_01_000032 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:44:41.797]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:44:41.797]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:44:41.797]Killed by external signal\n",
      ".\n",
      "24/02/25 08:44:42 ERROR YarnScheduler: Lost executor 31 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000032 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:44:41.797]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:44:41.797]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:44:41.797]Killed by external signal\n",
      ".\n",
      "24/02/25 08:44:42 WARN TaskSetManager: Lost task 63.0 in stage 48.0 (TID 1644) (cluster-fb6c-w-0.c.irass3-413914.internal executor 31): ExecutorLostFailure (executor 31 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000032 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:44:41.797]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:44:41.797]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:44:41.797]Killed by external signal\n",
      ".\n",
      "24/02/25 08:45:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_87 !\n",
      "24/02/25 08:45:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_91 !\n",
      "24/02/25 08:45:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_97 !\n",
      "24/02/25 08:45:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_93 !\n",
      "24/02/25 08:45:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_97 !\n",
      "24/02/25 08:45:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_93 !\n",
      "24/02/25 08:45:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_91 !\n",
      "24/02/25 08:46:00 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000034 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:45:59.725]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:45:59.725]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:45:59.726]Killed by external signal\n",
      ".\n",
      "24/02/25 08:46:00 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 33 for reason Container from a bad node: container_1708846121196_0001_01_000034 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:45:59.725]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:45:59.725]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:45:59.726]Killed by external signal\n",
      ".\n",
      "24/02/25 08:46:00 ERROR YarnScheduler: Lost executor 33 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000034 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:45:59.725]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:45:59.725]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:45:59.726]Killed by external signal\n",
      ".\n",
      "24/02/25 08:46:00 WARN TaskSetManager: Lost task 100.0 in stage 48.0 (TID 1717) (cluster-fb6c-w-0.c.irass3-413914.internal executor 33): ExecutorLostFailure (executor 33 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000034 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:45:59.725]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:45:59.725]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:45:59.726]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_28 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_56 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_16 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_14 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_22 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_11 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_60 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_4 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_38 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_53 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_3 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_25 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_19 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_3 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_56 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_3 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_8 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_3 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_33 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_60 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_31 !\n",
      "24/02/25 08:47:06 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_2 !\n",
      "24/02/25 08:47:09 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000031 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:47:09.294]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:47:09.295]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:47:09.295]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:09 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 30 for reason Container from a bad node: container_1708846121196_0001_01_000031 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:47:09.294]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:47:09.295]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:47:09.295]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:09 ERROR YarnScheduler: Lost executor 30 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000031 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:47:09.294]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:47:09.295]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:47:09.295]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:09 WARN TaskSetManager: Lost task 40.0 in stage 50.0 (TID 1848) (cluster-fb6c-w-2.c.irass3-413914.internal executor 30): ExecutorLostFailure (executor 30 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000031 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:47:09.294]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:47:09.295]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:47:09.295]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_10 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_118 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_2 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_10 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_6 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_2 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_2 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_10 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_6 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_6 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_2 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_121 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_6 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_3 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_123 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_9 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_3 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_98 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_9 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_109 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_3 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_123 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_9 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_98 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_9 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_101 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_3 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_101 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_123 !\n",
      "24/02/25 08:47:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_101 !\n",
      "24/02/25 08:47:51 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000028 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:47:51.247]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:47:51.247]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:47:51.248]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:51 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 27 for reason Container from a bad node: container_1708846121196_0001_01_000028 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:47:51.247]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:47:51.247]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:47:51.248]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:51 ERROR YarnScheduler: Lost executor 27 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000028 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:47:51.247]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:47:51.247]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:47:51.248]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:51 WARN TaskSetManager: Lost task 10.0 in stage 51.0 (TID 2026) (cluster-fb6c-w-0.c.irass3-413914.internal executor 27): ExecutorLostFailure (executor 27 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000028 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:47:51.247]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:47:51.247]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:47:51.248]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:53 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000030 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:47:52.629]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:47:52.629]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:47:52.630]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:53 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 29 for reason Container from a bad node: container_1708846121196_0001_01_000030 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:47:52.629]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:47:52.629]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:47:52.630]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:53 ERROR YarnScheduler: Lost executor 29 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000030 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:47:52.629]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:47:52.629]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:47:52.630]Killed by external signal\n",
      ".\n",
      "24/02/25 08:47:53 WARN TaskSetManager: Lost task 13.0 in stage 51.0 (TID 2029) (cluster-fb6c-w-1.c.irass3-413914.internal executor 29): ExecutorLostFailure (executor 29 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000030 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:47:52.629]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:47:52.629]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:47:52.630]Killed by external signal\n",
      ".\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_70 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_76 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_70 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_64 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_70 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_76 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_57 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_70 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_64 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_64 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_64 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_63 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_63 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_63 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_69 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_56 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_69 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_74 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_74 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_69 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_63 !\n",
      "24/02/25 08:49:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_69 !\n",
      "24/02/25 08:49:13 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000033 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:49:13.357]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:49:13.357]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:49:13.357]Killed by external signal\n",
      ".\n",
      "24/02/25 08:49:13 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 32 for reason Container from a bad node: container_1708846121196_0001_01_000033 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:49:13.357]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:49:13.357]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:49:13.357]Killed by external signal\n",
      ".\n",
      "24/02/25 08:49:13 ERROR YarnScheduler: Lost executor 32 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000033 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:49:13.357]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:49:13.357]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:49:13.357]Killed by external signal\n",
      ".\n",
      "24/02/25 08:49:13 WARN TaskSetManager: Lost task 76.0 in stage 51.0 (TID 2094) (cluster-fb6c-w-1.c.irass3-413914.internal executor 32): ExecutorLostFailure (executor 32 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000033 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:49:13.357]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:49:13.357]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:49:13.357]Killed by external signal\n",
      ".\n",
      "24/02/25 08:49:14 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000035 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:49:14.113]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:49:14.113]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:49:14.113]Killed by external signal\n",
      ".\n",
      "24/02/25 08:49:14 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 34 for reason Container from a bad node: container_1708846121196_0001_01_000035 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:49:14.113]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:49:14.113]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:49:14.113]Killed by external signal\n",
      ".\n",
      "24/02/25 08:49:14 ERROR YarnScheduler: Lost executor 34 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000035 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:49:14.113]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:49:14.113]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:49:14.113]Killed by external signal\n",
      ".\n",
      "24/02/25 08:49:14 WARN TaskSetManager: Lost task 74.0 in stage 51.0 (TID 2092) (cluster-fb6c-w-0.c.irass3-413914.internal executor 34): ExecutorLostFailure (executor 34 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000035 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:49:14.113]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:49:14.113]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:49:14.113]Killed by external signal\n",
      ".\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_88 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_68 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_56 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_26 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_76 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_15 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_40 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_19 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_44 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_1 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_56 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_19 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_64 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_11 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_11 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_32 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_50 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_44 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_1 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_7 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_72 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_1 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_11 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_26 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_64 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_76 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_56 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_19 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_82 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_122 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_44 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_21 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_40 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_15 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_40 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_36 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_50 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_76 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_56 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_11 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_86 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_92 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_7 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_26 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_19 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_15 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_19 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_92 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_82 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_26 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_44 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_32 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_1 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_64 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_50 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_26 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_40 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_72 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_88 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_88 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_7 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_86 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_53 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_15 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_21 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_53 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_68 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_53 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_64 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_21 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_82 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_68 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_88 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_122 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_50 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_36 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_59 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_64 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_50 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_1 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_36 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_122 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_86 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_59 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_56 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_86 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_88 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_59 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_7 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_21 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_86 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_72 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_59 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_44 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_59 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_36 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_72 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_53 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_53 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_76 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_82 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_32 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_82 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_7 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_40 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_32 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_68 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_21 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_11 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_92 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_68 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_15 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_36 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_92 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_72 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_32 !\n",
      "24/02/25 08:50:55 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_76 !\n",
      "24/02/25 08:50:55 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000036 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:50:55.323]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:50:55.323]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:50:55.323]Killed by external signal\n",
      ".\n",
      "24/02/25 08:50:55 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 35 for reason Container from a bad node: container_1708846121196_0001_01_000036 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:50:55.323]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:50:55.323]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:50:55.323]Killed by external signal\n",
      ".\n",
      "24/02/25 08:50:55 ERROR YarnScheduler: Lost executor 35 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000036 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:50:55.323]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:50:55.323]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:50:55.323]Killed by external signal\n",
      ".\n",
      "24/02/25 08:50:55 WARN TaskSetManager: Lost task 92.0 in stage 52.0 (TID 2235) (cluster-fb6c-w-2.c.irass3-413914.internal executor 35): ExecutorLostFailure (executor 35 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000036 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:50:55.323]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:50:55.323]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:50:55.323]Killed by external signal\n",
      ".\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_51 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_119 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_116 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_113 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_122 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_91 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_119 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_65 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_58 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_2 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_91 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_83 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_83 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_102 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_102 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_122 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_87 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_99 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_105 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_73 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_51 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_87 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_91 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_99 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_116 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_108 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_69 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_77 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_83 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_77 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_105 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_69 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_69 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_95 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_119 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_113 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_119 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_108 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_95 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_69 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_2 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_108 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_2 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_102 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_73 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_119 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_51 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_116 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_65 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_83 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_102 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_51 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_83 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_113 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_77 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_108 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_99 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_105 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_119 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_69 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_77 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_102 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_122 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_95 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_95 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_87 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_2 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_77 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_87 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_105 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_122 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_2 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_87 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_65 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_95 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_91 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_116 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_113 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_119 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_108 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_122 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_99 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_58 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_116 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_73 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_91 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_65 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_58 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_58 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_73 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_105 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_58 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_99 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_73 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_113 !\n",
      "24/02/25 08:51:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_65 !\n",
      "24/02/25 08:51:21 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000037 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:51:21.234]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:51:21.235]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:51:21.235]Killed by external signal\n",
      ".\n",
      "24/02/25 08:51:21 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 36 for reason Container from a bad node: container_1708846121196_0001_01_000037 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:51:21.234]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:51:21.235]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:51:21.235]Killed by external signal\n",
      ".\n",
      "24/02/25 08:51:21 ERROR YarnScheduler: Lost executor 36 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000037 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:51:21.234]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:51:21.235]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:51:21.235]Killed by external signal\n",
      ".\n",
      "24/02/25 08:51:21 WARN TaskSetManager: Lost task 6.0 in stage 53.0 (TID 2283) (cluster-fb6c-w-0.c.irass3-413914.internal executor 36): ExecutorLostFailure (executor 36 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000037 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:51:21.234]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:51:21.235]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:51:21.235]Killed by external signal\n",
      ".\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_0 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_11 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_8 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_0 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_8 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_0 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_11 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_0 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_8 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_8 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_11 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_11 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_8 !\n",
      "24/02/25 08:51:25 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_224_11 !\n",
      "24/02/25 08:51:26 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000038 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:51:25.753]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:51:25.753]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:51:25.753]Killed by external signal\n",
      ".\n",
      "24/02/25 08:51:26 ERROR YarnScheduler: Lost executor 37 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000038 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:51:25.753]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:51:25.753]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:51:25.753]Killed by external signal\n",
      ".\n",
      "24/02/25 08:51:26 WARN TaskSetManager: Lost task 16.0 in stage 53.0 (TID 2294) (cluster-fb6c-w-1.c.irass3-413914.internal executor 37): ExecutorLostFailure (executor 37 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000038 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:51:25.753]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:51:25.753]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:51:25.753]Killed by external signal\n",
      ".\n",
      "24/02/25 08:51:26 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 37 for reason Container from a bad node: container_1708846121196_0001_01_000038 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:51:25.753]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:51:25.753]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:51:25.753]Killed by external signal\n",
      ".\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_100 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_114 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_114 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_114 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_114 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_108 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_108 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_100 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_108 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_100 !\n",
      "24/02/25 08:53:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_108 !\n",
      "24/02/25 08:53:26 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000040 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:53:26.070]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:53:26.070]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:53:26.070]Killed by external signal\n",
      ".\n",
      "24/02/25 08:53:26 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 39 for reason Container from a bad node: container_1708846121196_0001_01_000040 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:53:26.070]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:53:26.070]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:53:26.070]Killed by external signal\n",
      ".\n",
      "24/02/25 08:53:26 ERROR YarnScheduler: Lost executor 39 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000040 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:53:26.070]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:53:26.070]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:53:26.070]Killed by external signal\n",
      ".\n",
      "24/02/25 08:53:26 WARN TaskSetManager: Lost task 1.0 in stage 65.0 (TID 2413) (cluster-fb6c-w-0.c.irass3-413914.internal executor 39): ExecutorLostFailure (executor 39 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000040 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:53:26.070]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:53:26.070]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:53:26.070]Killed by external signal\n",
      ".\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_8 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_4 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_8 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_8 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_123 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_8 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_15 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_4 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_4 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_4 !\n",
      "24/02/25 08:53:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_15 !\n",
      "24/02/25 08:53:51 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000041 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:53:50.690]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:53:50.690]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:53:50.691]Killed by external signal\n",
      ".\n",
      "24/02/25 08:53:51 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 40 for reason Container from a bad node: container_1708846121196_0001_01_000041 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:53:50.690]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:53:50.690]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:53:50.691]Killed by external signal\n",
      ".\n",
      "24/02/25 08:53:51 ERROR YarnScheduler: Lost executor 40 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000041 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:53:50.690]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:53:50.690]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:53:50.691]Killed by external signal\n",
      ".\n",
      "24/02/25 08:53:51 WARN TaskSetManager: Lost task 15.0 in stage 65.0 (TID 2428) (cluster-fb6c-w-2.c.irass3-413914.internal executor 40): ExecutorLostFailure (executor 40 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000041 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:53:50.690]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:53:50.690]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:53:50.691]Killed by external signal\n",
      ".\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_31 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_13 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_31 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_118 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_95 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_118 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_13 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_95 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_19 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_37 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_27 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_90 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_0 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_13 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_27 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_19 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_9 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_19 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_19 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_118 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_37 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_0 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_9 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_27 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_118 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_9 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_0 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_37 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_0 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_37 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_9 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_31 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_13 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_13 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_27 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_90 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_95 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_27 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_31 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_95 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_31 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_9 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_37 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_19 !\n",
      "24/02/25 08:55:59 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_0 !\n",
      "24/02/25 08:56:01 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000044 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:56:00.915]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:56:00.915]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:56:00.915]Killed by external signal\n",
      ".\n",
      "24/02/25 08:56:01 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 43 for reason Container from a bad node: container_1708846121196_0001_01_000044 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:56:00.915]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:56:00.915]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:56:00.915]Killed by external signal\n",
      ".\n",
      "24/02/25 08:56:01 ERROR YarnScheduler: Lost executor 43 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000044 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:56:00.915]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:56:00.915]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:56:00.915]Killed by external signal\n",
      ".\n",
      "24/02/25 08:56:01 WARN TaskSetManager: Lost task 41.0 in stage 66.0 (TID 2561) (cluster-fb6c-w-0.c.irass3-413914.internal executor 43): ExecutorLostFailure (executor 43 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000044 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 08:56:00.915]Container killed on request. Exit code is 137\n",
      "[2024-02-25 08:56:00.915]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 08:56:00.915]Killed by external signal\n",
      ".\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_107 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_117 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_10 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_10 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_103 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_117 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_107 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_117 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_111 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_121 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_5 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_111 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_105 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_97 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_121 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_121 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_10 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_103 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_111 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_10 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_234_103 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_10 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_121 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_5 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_111 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_107 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_117 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_111 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_5 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_5 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_103 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_117 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_121 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_103 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_5 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_107 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_105 !\n",
      "24/02/25 08:56:56 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_107 !\n",
      "24/02/25 08:56:57 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000045 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:56:57.250]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:56:57.250]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:56:57.250]Killed by external signal\n",
      ".\n",
      "24/02/25 08:56:57 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 44 for reason Container from a bad node: container_1708846121196_0001_01_000045 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:56:57.250]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:56:57.250]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:56:57.250]Killed by external signal\n",
      ".\n",
      "24/02/25 08:56:57 ERROR YarnScheduler: Lost executor 44 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000045 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:56:57.250]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:56:57.250]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:56:57.250]Killed by external signal\n",
      ".\n",
      "24/02/25 08:56:57 WARN TaskSetManager: Lost task 16.0 in stage 67.0 (TID 2670) (cluster-fb6c-w-2.c.irass3-413914.internal executor 44): ExecutorLostFailure (executor 44 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000045 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:56:57.250]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:56:57.250]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:56:57.250]Killed by external signal\n",
      ".\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_35 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_35 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_39 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_39 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_35 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_35 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_29 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_35 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_29 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_39 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_39 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_29 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_29 !\n",
      "24/02/25 08:57:28 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_39 !\n",
      "24/02/25 08:57:29 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000042 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:57:29.019]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:57:29.019]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:57:29.020]Killed by external signal\n",
      ".\n",
      "24/02/25 08:57:29 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 41 for reason Container from a bad node: container_1708846121196_0001_01_000042 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:57:29.019]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:57:29.019]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:57:29.020]Killed by external signal\n",
      ".\n",
      "24/02/25 08:57:29 ERROR YarnScheduler: Lost executor 41 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000042 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:57:29.019]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:57:29.019]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:57:29.020]Killed by external signal\n",
      ".\n",
      "24/02/25 08:57:29 WARN TaskSetManager: Lost task 46.0 in stage 67.0 (TID 2701) (cluster-fb6c-w-0.c.irass3-413914.internal executor 41): ExecutorLostFailure (executor 41 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000042 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:57:29.019]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:57:29.019]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:57:29.020]Killed by external signal\n",
      ".\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_62 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_62 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_55 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_52 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_52 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_240_55 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_55 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_62 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_55 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_52 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_62 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_62 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_52 !\n",
      "24/02/25 08:57:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_55 !\n",
      "24/02/25 08:57:49 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000039 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:57:49.422]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:57:49.422]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:57:49.422]Killed by external signal\n",
      ".\n",
      "24/02/25 08:57:49 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 38 for reason Container from a bad node: container_1708846121196_0001_01_000039 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:57:49.422]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:57:49.422]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:57:49.422]Killed by external signal\n",
      ".\n",
      "24/02/25 08:57:49 ERROR YarnScheduler: Lost executor 38 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000039 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:57:49.422]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:57:49.422]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:57:49.422]Killed by external signal\n",
      ".\n",
      "24/02/25 08:57:49 WARN TaskSetManager: Lost task 65.0 in stage 67.0 (TID 2721) (cluster-fb6c-w-1.c.irass3-413914.internal executor 38): ExecutorLostFailure (executor 38 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000039 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:57:49.422]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:57:49.422]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:57:49.422]Killed by external signal\n",
      ".\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_28 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_38 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_28 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_33 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_33 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_23 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_38 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_38 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_33 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_38 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_28 !\n",
      "24/02/25 08:59:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_33 !\n",
      "24/02/25 08:59:49 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000047 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:59:49.136]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:59:49.136]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:59:49.136]Killed by external signal\n",
      ".\n",
      "24/02/25 08:59:49 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 46 for reason Container from a bad node: container_1708846121196_0001_01_000047 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:59:49.136]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:59:49.136]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:59:49.136]Killed by external signal\n",
      ".\n",
      "24/02/25 08:59:49 ERROR YarnScheduler: Lost executor 46 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000047 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:59:49.136]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:59:49.136]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:59:49.136]Killed by external signal\n",
      ".\n",
      "24/02/25 08:59:49 WARN TaskSetManager: Lost task 38.0 in stage 81.0 (TID 2829) (cluster-fb6c-w-2.c.irass3-413914.internal executor 46): ExecutorLostFailure (executor 46 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000047 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 08:59:49.136]Container killed on request. Exit code is 143\n",
      "[2024-02-25 08:59:49.136]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 08:59:49.136]Killed by external signal\n",
      ".\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_105 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_20 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_12 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_29 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_20 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_96 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_5 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_5 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_12 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_105 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_5 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_20 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_29 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_12 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_5 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_20 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_90 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_12 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_5 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_29 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_96 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_90 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_96 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_29 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_96 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_12 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_20 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_105 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_90 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_29 !\n",
      "24/02/25 09:01:38 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_105 !\n",
      "24/02/25 09:01:39 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000049 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:01:38.973]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:01:38.973]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:01:38.973]Killed by external signal\n",
      ".\n",
      "24/02/25 09:01:39 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 48 for reason Container from a bad node: container_1708846121196_0001_01_000049 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:01:38.973]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:01:38.973]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:01:38.973]Killed by external signal\n",
      ".\n",
      "24/02/25 09:01:39 ERROR YarnScheduler: Lost executor 48 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000049 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:01:38.973]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:01:38.973]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:01:38.973]Killed by external signal\n",
      ".\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_9 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_25 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_0 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_119 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_25 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_19 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_14 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_0 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_19 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_98 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_9 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_19 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_19 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_30 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_14 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_0 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_9 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_30 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_9 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_98 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_119 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_0 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_14 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_30 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_95 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_98 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_119 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_9 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_25 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_14 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_30 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_119 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_25 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_95 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_14 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_98 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_30 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_19 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_246_0 !\n",
      "24/02/25 09:01:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_25 !\n",
      "24/02/25 09:01:41 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000050 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:01:41.692]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:01:41.692]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:01:41.693]Killed by external signal\n",
      ".\n",
      "24/02/25 09:01:41 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 49 for reason Container from a bad node: container_1708846121196_0001_01_000050 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:01:41.692]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:01:41.692]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:01:41.693]Killed by external signal\n",
      ".\n",
      "24/02/25 09:01:41 ERROR YarnScheduler: Lost executor 49 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000050 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:01:41.692]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:01:41.692]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:01:41.693]Killed by external signal\n",
      ".\n",
      "24/02/25 09:01:41 WARN TaskSetManager: Lost task 34.0 in stage 82.0 (TID 2932) (cluster-fb6c-w-0.c.irass3-413914.internal executor 49): ExecutorLostFailure (executor 49 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000050 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:01:41.692]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:01:41.692]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:01:41.693]Killed by external signal\n",
      ".\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_25 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_31 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_31 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_25 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_25 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_31 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_31 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_19 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_25 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_31 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_19 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_19 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_19 !\n",
      "24/02/25 09:03:17 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_25 !\n",
      "24/02/25 09:03:24 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000051 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:03:23.446]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:03:23.446]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:03:23.446]Killed by external signal\n",
      ".\n",
      "24/02/25 09:03:24 ERROR YarnScheduler: Lost executor 50 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000051 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:03:23.446]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:03:23.446]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:03:23.446]Killed by external signal\n",
      ".\n",
      "24/02/25 09:03:24 WARN TaskSetManager: Lost task 37.0 in stage 83.0 (TID 3065) (cluster-fb6c-w-2.c.irass3-413914.internal executor 50): ExecutorLostFailure (executor 50 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000051 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:03:23.446]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:03:23.446]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:03:23.446]Killed by external signal\n",
      ".\n",
      "24/02/25 09:03:24 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 50 for reason Container from a bad node: container_1708846121196_0001_01_000051 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:03:23.446]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:03:23.446]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:03:23.446]Killed by external signal\n",
      ".\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_57 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_51 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_51 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_47 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_51 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_57 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_51 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_57 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_47 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_47 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_47 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_51 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_57 !\n",
      "24/02/25 09:03:47 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_57 !\n",
      "24/02/25 09:03:50 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000043 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:03:50.071]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:03:50.071]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:03:50.071]Killed by external signal\n",
      ".\n",
      "24/02/25 09:03:50 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 42 for reason Container from a bad node: container_1708846121196_0001_01_000043 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:03:50.071]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:03:50.071]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:03:50.071]Killed by external signal\n",
      ".\n",
      "24/02/25 09:03:50 ERROR YarnScheduler: Lost executor 42 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000043 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:03:50.071]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:03:50.071]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:03:50.071]Killed by external signal\n",
      ".\n",
      "24/02/25 09:03:50 WARN TaskSetManager: Lost task 63.0 in stage 83.0 (TID 3092) (cluster-fb6c-w-1.c.irass3-413914.internal executor 42): ExecutorLostFailure (executor 42 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000043 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:03:50.071]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:03:50.071]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:03:50.071]Killed by external signal\n",
      ".\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_94 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_94 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_90 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_94 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_90 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_90 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_94 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_94 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_90 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_84 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_84 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_252_84 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_84 !\n",
      "24/02/25 09:04:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_90 !\n",
      "24/02/25 09:04:31 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000048 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:04:31.844]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:04:31.844]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:04:31.844]Killed by external signal\n",
      ".\n",
      "24/02/25 09:04:31 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 47 for reason Container from a bad node: container_1708846121196_0001_01_000048 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:04:31.844]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:04:31.844]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:04:31.844]Killed by external signal\n",
      ".\n",
      "24/02/25 09:04:31 ERROR YarnScheduler: Lost executor 47 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000048 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:04:31.844]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:04:31.844]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:04:31.844]Killed by external signal\n",
      ".\n",
      "24/02/25 09:04:31 WARN TaskSetManager: Lost task 101.0 in stage 83.0 (TID 3130) (cluster-fb6c-w-0.c.irass3-413914.internal executor 47): ExecutorLostFailure (executor 47 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000048 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:04:31.844]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:04:31.844]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:04:31.844]Killed by external signal\n",
      ".\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_5 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_10 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_10 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_5 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_10 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_10 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_16 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_16 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_16 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_116 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_5 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_16 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_19 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_19 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_7 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_13 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_19 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_19 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_2 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_7 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_7 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_13 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_13 !\n",
      "24/02/25 09:05:36 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_13 !\n",
      "24/02/25 09:05:39 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000054 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:05:38.829]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:05:38.829]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:05:38.829]Killed by external signal\n",
      ".\n",
      "24/02/25 09:05:39 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 53 for reason Container from a bad node: container_1708846121196_0001_01_000054 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:05:38.829]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:05:38.829]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:05:38.829]Killed by external signal\n",
      ".\n",
      "24/02/25 09:05:39 ERROR YarnScheduler: Lost executor 53 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000054 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:05:38.829]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:05:38.829]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:05:38.829]Killed by external signal\n",
      ".\n",
      "24/02/25 09:05:39 WARN TaskSetManager: Lost task 22.0 in stage 99.0 (TID 3189) (cluster-fb6c-w-2.c.irass3-413914.internal executor 53): ExecutorLostFailure (executor 53 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000054 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:05:38.829]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:05:38.829]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:05:38.829]Killed by external signal\n",
      ".\n",
      "24/02/25 09:05:40 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000052 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:05:40.361]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:05:40.362]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:05:40.362]Killed by external signal\n",
      ".\n",
      "24/02/25 09:05:40 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 51 for reason Container from a bad node: container_1708846121196_0001_01_000052 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:05:40.361]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:05:40.362]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:05:40.362]Killed by external signal\n",
      ".\n",
      "24/02/25 09:05:40 ERROR YarnScheduler: Lost executor 51 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000052 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:05:40.361]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:05:40.362]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:05:40.362]Killed by external signal\n",
      ".\n",
      "24/02/25 09:05:40 WARN TaskSetManager: Lost task 25.0 in stage 99.0 (TID 3192) (cluster-fb6c-w-1.c.irass3-413914.internal executor 51): ExecutorLostFailure (executor 51 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000052 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:05:40.361]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:05:40.362]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:05:40.362]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_64 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_76 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_76 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_76 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_68 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_80 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_80 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_80 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_68 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_68 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_76 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_80 !\n",
      "24/02/25 09:07:01 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_68 !\n",
      "24/02/25 09:07:03 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000056 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:03.636]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:03.636]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:03.637]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:03 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 55 for reason Container from a bad node: container_1708846121196_0001_01_000056 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:03.636]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:03.636]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:03.637]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:03 ERROR YarnScheduler: Lost executor 55 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000056 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:03.636]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:03.636]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:03.637]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:03 WARN TaskSetManager: Lost task 86.0 in stage 99.0 (TID 3255) (cluster-fb6c-w-0.c.irass3-413914.internal executor 55): ExecutorLostFailure (executor 55 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000056 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:03.636]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:03.636]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:03.637]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_88 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_88 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_88 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_93 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_93 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_93 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_99 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_99 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_99 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_83 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_93 !\n",
      "24/02/25 09:07:24 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_99 !\n",
      "24/02/25 09:07:25 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000055 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:24.745]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:24.746]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:24.746]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:25 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 54 for reason Container from a bad node: container_1708846121196_0001_01_000055 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:24.745]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:24.746]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:24.746]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:25 ERROR YarnScheduler: Lost executor 54 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000055 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:24.745]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:24.746]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:24.746]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:25 WARN TaskSetManager: Lost task 117.0 in stage 99.0 (TID 3274) (cluster-fb6c-w-1.c.irass3-413914.internal executor 54): ExecutorLostFailure (executor 54 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000055 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:24.745]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:24.746]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:24.746]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_6 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_6 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_94 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_102 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_6 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_94 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_120 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_270_13 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_120 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_13 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_102 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_1 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_94 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_1 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_102 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_6 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_13 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_90 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_13 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_1 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_1 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_270_6 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_102 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_270_1 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_120 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_258_13 !\n",
      "24/02/25 09:07:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_120 !\n",
      "24/02/25 09:07:55 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000057 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:55.092]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:55.092]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:55.092]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:55 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 56 for reason Container from a bad node: container_1708846121196_0001_01_000057 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:55.092]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:55.092]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:55.092]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:55 ERROR YarnScheduler: Lost executor 56 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000057 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:55.092]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:55.092]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:55.092]Killed by external signal\n",
      ".\n",
      "24/02/25 09:07:55 WARN TaskSetManager: Lost task 18.0 in stage 100.0 (TID 3296) (cluster-fb6c-w-2.c.irass3-413914.internal executor 56): ExecutorLostFailure (executor 56 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000057 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:07:55.092]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:07:55.092]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:07:55.092]Killed by external signal\n",
      ".\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_18 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_18 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_18 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_18 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_23 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_27 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_23 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_23 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_23 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_23 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_27 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_18 !\n",
      "24/02/25 09:09:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_27 !\n",
      "24/02/25 09:09:59 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000053 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:09:58.665]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:09:58.665]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:09:58.666]Killed by external signal\n",
      ".\n",
      "24/02/25 09:09:59 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 52 for reason Container from a bad node: container_1708846121196_0001_01_000053 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:09:58.665]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:09:58.665]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:09:58.666]Killed by external signal\n",
      ".\n",
      "24/02/25 09:09:59 ERROR YarnScheduler: Lost executor 52 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000053 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:09:58.665]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:09:58.665]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:09:58.666]Killed by external signal\n",
      ".\n",
      "24/02/25 09:09:59 WARN TaskSetManager: Lost task 27.0 in stage 101.0 (TID 3435) (cluster-fb6c-w-0.c.irass3-413914.internal executor 52): ExecutorLostFailure (executor 52 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000053 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:09:58.665]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:09:58.665]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:09:58.666]Killed by external signal\n",
      ".\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_103 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_114 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_108 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_103 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_103 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_114 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_103 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_114 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_114 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_114 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_108 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_108 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_108 !\n",
      "24/02/25 09:11:37 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_264_108 !\n",
      "24/02/25 09:11:38 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000059 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:11:38.339]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:11:38.340]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:11:38.340]Killed by external signal\n",
      ".\n",
      "24/02/25 09:11:38 ERROR YarnScheduler: Lost executor 58 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000059 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:11:38.339]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:11:38.340]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:11:38.340]Killed by external signal\n",
      ".\n",
      "24/02/25 09:11:38 WARN TaskSetManager: Lost task 119.0 in stage 101.0 (TID 3524) (cluster-fb6c-w-0.c.irass3-413914.internal executor 58): ExecutorLostFailure (executor 58 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000059 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:11:38.339]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:11:38.340]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:11:38.340]Killed by external signal\n",
      ".\n",
      "24/02/25 09:11:38 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 58 for reason Container from a bad node: container_1708846121196_0001_01_000059 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:11:38.339]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:11:38.340]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:11:38.340]Killed by external signal\n",
      ".\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_123 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_102 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_102 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_123 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_112 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_102 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_112 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_112 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_112 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_123 !\n",
      "24/02/25 09:11:50 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_123 !\n",
      "24/02/25 09:11:51 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000061 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:11:50.755]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:11:50.755]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:11:50.756]Killed by external signal\n",
      ".\n",
      "24/02/25 09:11:51 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 60 for reason Container from a bad node: container_1708846121196_0001_01_000061 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:11:50.755]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:11:50.755]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:11:50.756]Killed by external signal\n",
      ".\n",
      "24/02/25 09:11:51 ERROR YarnScheduler: Lost executor 60 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000061 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:11:50.755]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:11:50.755]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:11:50.756]Killed by external signal\n",
      ".\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_70 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_59 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_64 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_70 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_64 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_70 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_64 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_59 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_70 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_59 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_52 !\n",
      "24/02/25 09:13:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_64 !\n",
      "24/02/25 09:13:34 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000062 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:13:34.055]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:13:34.055]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:13:34.055]Killed by external signal\n",
      ".\n",
      "24/02/25 09:13:34 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 61 for reason Container from a bad node: container_1708846121196_0001_01_000062 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:13:34.055]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:13:34.055]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:13:34.055]Killed by external signal\n",
      ".\n",
      "24/02/25 09:13:34 ERROR YarnScheduler: Lost executor 61 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000062 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:13:34.055]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:13:34.055]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:13:34.055]Killed by external signal\n",
      ".\n",
      "24/02/25 09:13:34 WARN TaskSetManager: Lost task 76.0 in stage 119.0 (TID 3620) (cluster-fb6c-w-0.c.irass3-413914.internal executor 61): ExecutorLostFailure (executor 61 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000062 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:13:34.055]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:13:34.055]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:13:34.055]Killed by external signal\n",
      ".\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_74 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_81 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_74 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_83 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_83 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_68 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_83 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_81 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_83 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_74 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_81 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_68 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_74 !\n",
      "24/02/25 09:13:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_81 !\n",
      "24/02/25 09:13:57 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000058 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:13:56.456]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:13:56.456]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:13:56.457]Killed by external signal\n",
      ".\n",
      "24/02/25 09:13:57 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 57 for reason Container from a bad node: container_1708846121196_0001_01_000058 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:13:56.456]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:13:56.456]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:13:56.457]Killed by external signal\n",
      ".\n",
      "24/02/25 09:13:57 ERROR YarnScheduler: Lost executor 57 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000058 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:13:56.456]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:13:56.456]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:13:56.457]Killed by external signal\n",
      ".\n",
      "24/02/25 09:13:57 WARN TaskSetManager: Lost task 89.0 in stage 119.0 (TID 3634) (cluster-fb6c-w-1.c.irass3-413914.internal executor 57): ExecutorLostFailure (executor 57 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000058 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:13:56.456]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:13:56.456]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:13:56.457]Killed by external signal\n",
      ".\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_270_17 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_23 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_13 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_1 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_105 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_17 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_23 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_96 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_1 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_117 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_117 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_270_1 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_17 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_105 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_270_13 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_96 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_8 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_1 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_8 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_17 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_105 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_117 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_8 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_1 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_8 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_96 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_105 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_23 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_17 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_117 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_13 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_13 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_270_8 !\n",
      "24/02/25 09:15:16 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_13 !\n",
      "24/02/25 09:15:17 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000029 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 09:15:17.254]Container killed on request. Exit code is 137\n",
      "[2024-02-25 09:15:17.254]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 09:15:17.254]Killed by external signal\n",
      ".\n",
      "24/02/25 09:15:17 ERROR YarnScheduler: Lost executor 28 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000029 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 09:15:17.254]Container killed on request. Exit code is 137\n",
      "[2024-02-25 09:15:17.254]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 09:15:17.254]Killed by external signal\n",
      ".\n",
      "24/02/25 09:15:17 WARN TaskSetManager: Lost task 23.0 in stage 120.0 (TID 3678) (cluster-fb6c-w-2.c.irass3-413914.internal executor 28): ExecutorLostFailure (executor 28 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000029 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 09:15:17.254]Container killed on request. Exit code is 137\n",
      "[2024-02-25 09:15:17.254]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 09:15:17.254]Killed by external signal\n",
      ".\n",
      "24/02/25 09:15:17 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 28 for reason Container from a bad node: container_1708846121196_0001_01_000029 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 09:15:17.254]Container killed on request. Exit code is 137\n",
      "[2024-02-25 09:15:17.254]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 09:15:17.254]Killed by external signal\n",
      ".\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_18 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_18 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_8 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_8 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_8 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_14 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_8 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_14 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_14 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_14 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_18 !\n",
      "24/02/25 09:17:48 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_14 !\n",
      "24/02/25 09:17:49 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000063 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:17:48.910]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:17:48.910]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:17:48.910]Killed by external signal\n",
      ".\n",
      "24/02/25 09:17:49 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 62 for reason Container from a bad node: container_1708846121196_0001_01_000063 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:17:48.910]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:17:48.910]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:17:48.910]Killed by external signal\n",
      ".\n",
      "24/02/25 09:17:49 ERROR YarnScheduler: Lost executor 62 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000063 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:17:48.910]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:17:48.910]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:17:48.910]Killed by external signal\n",
      ".\n",
      "24/02/25 09:17:49 WARN TaskSetManager: Lost task 18.0 in stage 121.0 (TID 3804) (cluster-fb6c-w-0.c.irass3-413914.internal executor 62): ExecutorLostFailure (executor 62 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000063 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:17:48.910]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:17:48.910]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:17:48.910]Killed by external signal\n",
      ".\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_32 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_32 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_38 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_38 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_38 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_32 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_38 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_27 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_27 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_27 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_32 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_27 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_32 !\n",
      "24/02/25 09:18:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_38 !\n",
      "24/02/25 09:18:15 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000060 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:18:14.975]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:18:14.975]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:18:14.975]Killed by external signal\n",
      ".\n",
      "24/02/25 09:18:15 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 59 for reason Container from a bad node: container_1708846121196_0001_01_000060 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:18:14.975]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:18:14.975]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:18:14.975]Killed by external signal\n",
      ".\n",
      "24/02/25 09:18:15 ERROR YarnScheduler: Lost executor 59 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000060 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:18:14.975]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:18:14.975]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:18:14.975]Killed by external signal\n",
      ".\n",
      "24/02/25 09:18:15 WARN TaskSetManager: Lost task 41.0 in stage 121.0 (TID 3828) (cluster-fb6c-w-1.c.irass3-413914.internal executor 59): ExecutorLostFailure (executor 59 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000060 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:18:14.975]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:18:14.975]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:18:14.975]Killed by external signal\n",
      ".\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_71 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_59 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_54 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_59 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_54 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_54 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_59 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_71 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_71 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_54 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_59 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_59 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_71 !\n",
      "24/02/25 09:18:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_71 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_61 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_66 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_74 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_66 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_66 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_74 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_74 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_74 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_66 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_61 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_66 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_74 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_276_61 !\n",
      "24/02/25 09:18:57 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_61 !\n",
      "24/02/25 09:18:58 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000065 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:18:58.403]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:18:58.403]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:18:58.403]Killed by external signal\n",
      ".\n",
      "24/02/25 09:18:58 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 64 for reason Container from a bad node: container_1708846121196_0001_01_000065 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:18:58.403]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:18:58.403]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:18:58.403]Killed by external signal\n",
      ".\n",
      "24/02/25 09:18:58 ERROR YarnScheduler: Lost executor 64 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000065 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:18:58.403]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:18:58.403]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:18:58.403]Killed by external signal\n",
      ".\n",
      "24/02/25 09:18:58 WARN TaskSetManager: Lost task 79.0 in stage 121.0 (TID 3867) (cluster-fb6c-w-0.c.irass3-413914.internal executor 64): ExecutorLostFailure (executor 64 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000065 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:18:58.403]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:18:58.403]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:18:58.403]Killed by external signal\n",
      ".\n",
      "24/02/25 09:19:01 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000064 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:19:00.361]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:19:00.361]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:19:00.361]Killed by external signal\n",
      ".\n",
      "24/02/25 09:19:01 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 63 for reason Container from a bad node: container_1708846121196_0001_01_000064 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:19:00.361]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:19:00.361]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:19:00.361]Killed by external signal\n",
      ".\n",
      "24/02/25 09:19:01 ERROR YarnScheduler: Lost executor 63 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000064 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:19:00.361]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:19:00.361]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:19:00.361]Killed by external signal\n",
      ".\n",
      "24/02/25 09:19:01 WARN TaskSetManager: Lost task 76.0 in stage 121.0 (TID 3864) (cluster-fb6c-w-2.c.irass3-413914.internal executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000064 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:19:00.361]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:19:00.361]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:19:00.361]Killed by external signal\n",
      ".\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_119 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_119 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_119 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_115 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_3 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_106 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_3 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_119 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_106 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_3 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_115 !\n",
      "24/02/25 09:20:02 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_115 !\n",
      "24/02/25 09:20:03 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000066 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:20:02.844]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:20:02.844]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:20:02.845]Killed by external signal\n",
      ".\n",
      "24/02/25 09:20:03 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 65 for reason Container from a bad node: container_1708846121196_0001_01_000066 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:20:02.844]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:20:02.844]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:20:02.845]Killed by external signal\n",
      ".\n",
      "24/02/25 09:20:03 ERROR YarnScheduler: Lost executor 65 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000066 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:20:02.844]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:20:02.844]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:20:02.845]Killed by external signal\n",
      ".\n",
      "24/02/25 09:20:03 WARN TaskSetManager: Lost task 3.0 in stage 141.0 (TID 3929) (cluster-fb6c-w-1.c.irass3-413914.internal executor 65): ExecutorLostFailure (executor 65 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000066 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:20:02.844]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:20:02.844]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:20:02.845]Killed by external signal\n",
      ".\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_14 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_14 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_25 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_19 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_14 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_25 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_19 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_14 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_10 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_19 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_19 !\n",
      "24/02/25 09:20:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_25 !\n",
      "24/02/25 09:20:40 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000068 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:20:40.652]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:20:40.652]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:20:40.652]Killed by external signal\n",
      ".\n",
      "24/02/25 09:20:40 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 67 for reason Container from a bad node: container_1708846121196_0001_01_000068 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:20:40.652]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:20:40.652]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:20:40.652]Killed by external signal\n",
      ".\n",
      "24/02/25 09:20:40 ERROR YarnScheduler: Lost executor 67 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000068 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:20:40.652]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:20:40.652]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:20:40.652]Killed by external signal\n",
      ".\n",
      "24/02/25 09:20:40 WARN TaskSetManager: Lost task 25.0 in stage 141.0 (TID 3952) (cluster-fb6c-w-0.c.irass3-413914.internal executor 67): ExecutorLostFailure (executor 67 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000068 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:20:40.652]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:20:40.652]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:20:40.652]Killed by external signal\n",
      ".\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_69 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_51 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_63 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_63 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_63 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_69 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_57 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_57 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_69 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_57 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_57 !\n",
      "24/02/25 09:21:41 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_63 !\n",
      "24/02/25 09:21:42 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000069 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:21:42.126]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:21:42.126]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:21:42.126]Killed by external signal\n",
      ".\n",
      "24/02/25 09:21:42 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 68 for reason Container from a bad node: container_1708846121196_0001_01_000069 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:21:42.126]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:21:42.126]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:21:42.126]Killed by external signal\n",
      ".\n",
      "24/02/25 09:21:42 ERROR YarnScheduler: Lost executor 68 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000069 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:21:42.126]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:21:42.126]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:21:42.126]Killed by external signal\n",
      ".\n",
      "24/02/25 09:21:42 WARN TaskSetManager: Lost task 69.0 in stage 141.0 (TID 3997) (cluster-fb6c-w-1.c.irass3-413914.internal executor 68): ExecutorLostFailure (executor 68 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000069 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:21:42.126]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:21:42.126]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:21:42.126]Killed by external signal\n",
      ".\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_74 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_81 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_86 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_81 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_81 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_86 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_90 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_90 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_90 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_86 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_86 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_81 !\n",
      "24/02/25 09:22:14 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_90 !\n",
      "24/02/25 09:22:14 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000070 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:22:14.615]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:22:14.615]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:22:14.615]Killed by external signal\n",
      ".\n",
      "24/02/25 09:22:14 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 69 for reason Container from a bad node: container_1708846121196_0001_01_000070 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:22:14.615]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:22:14.615]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:22:14.615]Killed by external signal\n",
      ".\n",
      "24/02/25 09:22:14 ERROR YarnScheduler: Lost executor 69 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000070 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:22:14.615]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:22:14.615]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:22:14.615]Killed by external signal\n",
      ".\n",
      "24/02/25 09:22:14 WARN TaskSetManager: Lost task 90.0 in stage 141.0 (TID 4019) (cluster-fb6c-w-0.c.irass3-413914.internal executor 69): ExecutorLostFailure (executor 69 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000070 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:22:14.615]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:22:14.615]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:22:14.615]Killed by external signal\n",
      ".\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_94 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_26 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_8 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_94 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_18 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_13 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_37 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_13 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_32 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_98 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_1 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_37 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_94 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_26 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_88 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_32 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_45 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_18 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_1 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_8 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_26 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_37 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_1 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_26 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_98 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_8 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_18 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_88 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_18 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_18 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_98 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_45 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_8 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_37 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_1 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_98 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_26 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_8 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_32 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_32 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_13 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_13 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_45 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_32 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_13 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_1 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_282_37 !\n",
      "24/02/25 09:24:35 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_94 !\n",
      "24/02/25 09:24:36 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000067 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 09:24:36.698]Container killed on request. Exit code is 137\n",
      "[2024-02-25 09:24:36.698]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 09:24:36.698]Killed by external signal\n",
      ".\n",
      "24/02/25 09:24:36 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 66 for reason Container from a bad node: container_1708846121196_0001_01_000067 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 09:24:36.698]Container killed on request. Exit code is 137\n",
      "[2024-02-25 09:24:36.698]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 09:24:36.698]Killed by external signal\n",
      ".\n",
      "24/02/25 09:24:36 ERROR YarnScheduler: Lost executor 66 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000067 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 09:24:36.698]Container killed on request. Exit code is 137\n",
      "[2024-02-25 09:24:36.698]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 09:24:36.698]Killed by external signal\n",
      ".\n",
      "24/02/25 09:24:36 WARN TaskSetManager: Lost task 45.0 in stage 142.0 (TID 4081) (cluster-fb6c-w-2.c.irass3-413914.internal executor 66): ExecutorLostFailure (executor 66 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000067 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 137. Diagnostics: [2024-02-25 09:24:36.698]Container killed on request. Exit code is 137\n",
      "[2024-02-25 09:24:36.698]Container exited with a non-zero exit code 137. \n",
      "[2024-02-25 09:24:36.698]Killed by external signal\n",
      ".\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_18 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_18 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_6 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_14 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_6 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_6 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_14 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_18 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_14 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_18 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_14 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_18 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_14 !\n",
      "24/02/25 09:27:40 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_6 !\n",
      "24/02/25 09:27:45 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000072 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:27:44.757]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:27:44.757]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:27:44.757]Killed by external signal\n",
      ".\n",
      "24/02/25 09:27:45 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 71 for reason Container from a bad node: container_1708846121196_0001_01_000072 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:27:44.757]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:27:44.757]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:27:44.757]Killed by external signal\n",
      ".\n",
      "24/02/25 09:27:45 ERROR YarnScheduler: Lost executor 71 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000072 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:27:44.757]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:27:44.757]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:27:44.757]Killed by external signal\n",
      ".\n",
      "24/02/25 09:27:45 WARN TaskSetManager: Lost task 24.0 in stage 143.0 (TID 4191) (cluster-fb6c-w-1.c.irass3-413914.internal executor 71): ExecutorLostFailure (executor 71 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000072 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:27:44.757]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:27:44.757]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:27:44.757]Killed by external signal\n",
      ".\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_30 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_30 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_30 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_22 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_22 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_27 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_22 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_27 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_30 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_22 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_30 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_27 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_27 !\n",
      "24/02/25 09:27:54 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_27 !\n",
      "24/02/25 09:27:55 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000073 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:27:55.323]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:27:55.324]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:27:55.324]Killed by external signal\n",
      ".\n",
      "24/02/25 09:27:55 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 72 for reason Container from a bad node: container_1708846121196_0001_01_000073 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:27:55.323]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:27:55.324]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:27:55.324]Killed by external signal\n",
      ".\n",
      "24/02/25 09:27:55 ERROR YarnScheduler: Lost executor 72 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000073 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:27:55.323]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:27:55.324]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:27:55.324]Killed by external signal\n",
      ".\n",
      "24/02/25 09:27:55 WARN TaskSetManager: Lost task 37.0 in stage 143.0 (TID 4205) (cluster-fb6c-w-0.c.irass3-413914.internal executor 72): ExecutorLostFailure (executor 72 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000073 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:27:55.323]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:27:55.324]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:27:55.324]Killed by external signal\n",
      ".\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_38 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_38 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_38 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_33 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_38 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_288_43 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_33 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_43 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_43 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_43 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_33 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_33 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_43 !\n",
      "24/02/25 09:28:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_38 !\n",
      "24/02/25 09:28:20 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000071 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:28:19.974]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:28:19.974]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:28:19.975]Killed by external signal\n",
      ".\n",
      "24/02/25 09:28:20 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 70 for reason Container from a bad node: container_1708846121196_0001_01_000071 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:28:19.974]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:28:19.974]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:28:19.975]Killed by external signal\n",
      ".\n",
      "24/02/25 09:28:20 ERROR YarnScheduler: Lost executor 70 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000071 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:28:19.974]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:28:19.974]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:28:19.975]Killed by external signal\n",
      ".\n",
      "24/02/25 09:28:20 WARN TaskSetManager: Lost task 50.0 in stage 143.0 (TID 4219) (cluster-fb6c-w-2.c.irass3-413914.internal executor 70): ExecutorLostFailure (executor 70 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000071 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:28:19.974]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:28:19.974]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:28:19.975]Killed by external signal\n",
      ".\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_2 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_2 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_122 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_122 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_106 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_116 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_106 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_106 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_116 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_2 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_122 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_116 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_116 !\n",
      "24/02/25 09:29:52 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_122 !\n",
      "24/02/25 09:29:53 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000075 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:29:53.478]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:29:53.478]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:29:53.478]Killed by external signal\n",
      ".\n",
      "24/02/25 09:29:53 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 74 for reason Container from a bad node: container_1708846121196_0001_01_000075 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:29:53.478]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:29:53.478]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:29:53.478]Killed by external signal\n",
      ".\n",
      "24/02/25 09:29:53 ERROR YarnScheduler: Lost executor 74 on cluster-fb6c-w-0.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000075 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:29:53.478]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:29:53.478]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:29:53.478]Killed by external signal\n",
      ".\n",
      "24/02/25 09:29:53 WARN TaskSetManager: Lost task 2.0 in stage 164.0 (TID 4290) (cluster-fb6c-w-0.c.irass3-413914.internal executor 74): ExecutorLostFailure (executor 74 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000075 on host: cluster-fb6c-w-0.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:29:53.478]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:29:53.478]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:29:53.478]Killed by external signal\n",
      ".\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_103 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_6 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_103 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_103 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_5 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_5 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_107 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_115 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_6 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_107 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_5 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_5 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_107 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_6 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_115 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_107 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_115 !\n",
      "24/02/25 09:30:39 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_115 !\n",
      "24/02/25 09:30:42 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000074 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:30:41.734]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:30:41.734]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:30:41.734]Killed by external signal\n",
      ".\n",
      "24/02/25 09:30:42 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 73 for reason Container from a bad node: container_1708846121196_0001_01_000074 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:30:41.734]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:30:41.734]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:30:41.734]Killed by external signal\n",
      ".\n",
      "24/02/25 09:30:42 ERROR YarnScheduler: Lost executor 73 on cluster-fb6c-w-1.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000074 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:30:41.734]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:30:41.734]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:30:41.734]Killed by external signal\n",
      ".\n",
      "24/02/25 09:30:42 WARN TaskSetManager: Lost task 6.0 in stage 164.0 (TID 4295) (cluster-fb6c-w-1.c.irass3-413914.internal executor 73): ExecutorLostFailure (executor 73 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000074 on host: cluster-fb6c-w-1.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:30:41.734]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:30:41.734]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:30:41.734]Killed by external signal\n",
      ".\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_101 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_15 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_119 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_15 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_1 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_9 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_9 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_9 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_9 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_101 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_119 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_212_1 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_110 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_204_110 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_101 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_101 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_119 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_294_1 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_92 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_214_110 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_300_119 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_202_1 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_197_15 !\n",
      "24/02/25 09:31:03 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_200_110 !\n",
      "24/02/25 09:31:03 WARN YarnAllocator: Container from a bad node: container_1708846121196_0001_01_000076 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:31:03.537]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:31:03.537]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:31:03.538]Killed by external signal\n",
      ".\n",
      "24/02/25 09:31:03 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 75 for reason Container from a bad node: container_1708846121196_0001_01_000076 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:31:03.537]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:31:03.537]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:31:03.538]Killed by external signal\n",
      ".\n",
      "24/02/25 09:31:03 ERROR YarnScheduler: Lost executor 75 on cluster-fb6c-w-2.c.irass3-413914.internal: Container from a bad node: container_1708846121196_0001_01_000076 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:31:03.537]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:31:03.537]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:31:03.538]Killed by external signal\n",
      ".\n",
      "24/02/25 09:31:03 WARN TaskSetManager: Lost task 15.0 in stage 164.0 (TID 4305) (cluster-fb6c-w-2.c.irass3-413914.internal executor 75): ExecutorLostFailure (executor 75 exited caused by one of the running tasks) Reason: Container from a bad node: container_1708846121196_0001_01_000076 on host: cluster-fb6c-w-2.c.irass3-413914.internal. Exit status: 143. Diagnostics: [2024-02-25 09:31:03.537]Container killed on request. Exit code is 143\n",
      "[2024-02-25 09:31:03.537]Container exited with a non-zero exit code 143. \n",
      "[2024-02-25 09:31:03.538]Killed by external signal\n",
      ".\n",
      "/usr/lib/spark/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "\n",
    "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
    "\n",
    "# construct the graph \n",
    "edges, vertices = generate_graph(pages_links)\n",
    "\n",
    "# compute PageRank\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
    "\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "pr = pr.sort(col('pagerank').desc())\n",
    "\n",
    "pr_time = time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3620817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 230:===================================================> (121 + 2) / 124]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|     id|          pagerank|\n",
      "+-------+------------------+\n",
      "|3434750| 9913.728782160779|\n",
      "|  10568| 5385.349263642038|\n",
      "|  32927| 5282.081575765276|\n",
      "|  30680|  5128.23370960412|\n",
      "|5843419| 4957.567686263868|\n",
      "|  68253| 4769.278265355157|\n",
      "|  31717| 4486.350180548311|\n",
      "|  11867|4146.4146509127695|\n",
      "|  14533|3996.4664408855006|\n",
      "| 645042| 3531.627089803743|\n",
      "|  17867|3246.0983906041415|\n",
      "|5042916| 2991.945739166178|\n",
      "|4689264| 2982.324883041747|\n",
      "|  14532| 2934.746829203171|\n",
      "|  25391|   2903.5462235134|\n",
      "|   5405| 2891.416329154635|\n",
      "|4764461| 2834.366987332661|\n",
      "|  15573|2783.8651181588384|\n",
      "|   9316|2782.0396464137702|\n",
      "|8569916|2775.2861918400154|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecefa062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                                     \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "pr_dict = pr.collect()\n",
    "pr_dict = {row['id']: row['pagerank'] for row in pr_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ec59cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_pageRank' \n",
    "page_rank ='pageRank'\n",
    "bucket_name='bucket_for_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1326cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{page_rank}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(pr_dict, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c25458be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 3434750, doc page rank: 9913.728782160779\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 10568, doc page rank: 5385.349263642038\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 32927, doc page rank: 5282.081575765276\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 30680, doc page rank: 5128.23370960412\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 5843419, doc page rank: 4957.567686263868\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 68253, doc page rank: 4769.278265355157\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 31717, doc page rank: 4486.350180548311\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 11867, doc page rank: 4146.4146509127695\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 14533, doc page rank: 3996.4664408855006\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 645042, doc page rank: 3531.627089803743\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(pr_dict.items())[:10]:\n",
    "    PRINT(f\"Key: {key}, doc page rank: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7717604",
   "metadata": {
    "id": "2cc36ca9",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-PageRank_time",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test that PageRank computaion took less than 1 hour\n",
    "assert pr_time < 60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d5523",
   "metadata": {
    "id": "a0ec9661",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-size_ofi_input_data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "54595c29-4ae3-4b78-86d0-d8457ae9c150"
   },
   "outputs": [],
   "source": [
    "# size of input data\n",
    "!gsutil du -sh \"gs://wikidata_preprocessed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25a98a",
   "metadata": {
    "id": "264e0792",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-size_of_index_data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "44d9721a-1cd7-4e59-9f78-5439864cfdad"
   },
   "outputs": [],
   "source": [
    "# size of index data\n",
    "index_dst = f'gs://{bucket_name}/postings_gcp/'\n",
    "!gsutil du -sh \"$index_dst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9538ee",
   "metadata": {
    "id": "LQ7r5rxvVuXb",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-credits",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# How many USD credits did you use in GCP during the course of this assignment?\n",
    "cost = 29 \n",
    "print(f'I used {cost} USD credit during the course of this assignment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e0ed8",
   "metadata": {
    "id": "fdd1bdca"
   },
   "source": [
    "**Bonus (10 points)** if you implement PageRank in pure PySpark, i.e. without using the GraphFrames package, AND manage to complete 10 iterations of your algorithm on the entire English Wikipedia in less than an hour. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8157868",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-PageRank_Bonus",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#If you have decided to do the bonus task - please copy the code here \n",
    "\n",
    "bonus_flag = False # Turn flag on (True) if you have implemented this part\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "# PLACE YOUR CODE HERE\n",
    "\n",
    "pr_time_Bonus = time() - t_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f9c94",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-PageRank_Bonus-time",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note:test that PageRank computaion took less than 1 hour\n",
    "assert pr_time_Bonus < 60*60 and bonus_flag"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment3_gcp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
