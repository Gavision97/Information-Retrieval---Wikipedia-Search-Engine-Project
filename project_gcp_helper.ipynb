{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["# IR Project GCP Helper for Creating Indexes and more #"]},{"cell_type":"code","execution_count":13,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-6ed3  GCE       3                                             RUNNING  us-central1-a\r\n"]}],"source":["!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":null,"id":"bbfd7451","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":15,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","def PRINT(text) -> None: print(f'{\"-\"*80}\\n{text}\\n{\"-\"*80}')\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":16,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar  9 17:13 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":42,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[],"source":["spark = SparkSession.builder.appName(\"YourAppName\").config(\"spark.driver.maxResultSize\", \"4g\").getOrCreate()"]},{"cell_type":"code","execution_count":18,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["bucket_name = 'bucket_for_index_generation' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"76616fa4","metadata":{},"source":["## Calculate Documents L2 Normalization ##\n","\n","In the next step we calculate L2 Norm for each document text.\n","\n","The ending result will be dictionary which maps -> (key, value) to (doc_id, doc_l2_norm_value)\n","\n","We will need that in order to preform CosinSimilarity in the next steps of the project"]},{"cell_type":"code","execution_count":11,"id":"c83f9f56","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n","                            \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n","                            \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                            \"many\", \"however\", \"would\", \"became\"]\n","special_words = ['3d', '4k', 'ip', 'js', 'ai', 'vr', 'ar', 'dl', 'ml', '09', '11', '9']\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){1,24}\"\"\", re.UNICODE)\n","stemmer = PorterStemmer()"]},{"cell_type":"code","execution_count":63,"id":"61510537","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['veri', 'ad', 'fun', 'way', 'q-system', 'genet', '3d', '1997', '22']\n"]}],"source":["text = \"is a very ad fun way Q-system (genetics) 3d the 1997 22\"\n","\n","print(tokenize(text))"]},{"cell_type":"code","execution_count":62,"id":"968dce38","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){1,24}\"\"\", re.UNICODE)\n","stemmer = PorterStemmer()\n","\n","def tokenize(text):\n","    clean_text = []\n","    text = text.lower()\n","    for token in RE_WORD.finditer(text):\n","        stemmed_token = stemmer.stem(token.group())\n","        if stemmed_token not in all_stopwords:\n","            clean_text.append(stemmed_token)\n","    return clean_text"]},{"cell_type":"code","execution_count":11,"id":"8bd3637c","metadata":{},"outputs":[],"source":["def tokenize(text, to_stem):\n","    clean_text = []\n","    text = text.lower()\n","    tokens = [token.group() for token in RE_WORD.finditer(text)]\n","    for token in tokens:\n","        if token not in all_stopwords:\n","            if to_stem:\n","                token = stemmer.stem(token)\n","                clean_text.append(token)\n","    return clean_text"]},{"cell_type":"code","execution_count":39,"id":"0e7a69ae","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['hell', 'add', 'add', 'add', 'function']\n","3.3166247903554\n"]}],"source":["from collections import Counter\n","\n","def l2_norm(text):\n","    # Count the occurrences of each unique word\n","    word_counts = Counter(text)\n","    # Get the counts of each unique word as a list\n","    counts_list = [word_counts[word]**2 for word in set(text)]\n","    l2_sum = 0\n","    for word_num in counts_list:\n","        l2_sum += word_num\n","    l2 = math.sqrt(l2_sum)\n","    #l2_norm = math.sqrt(sum(x**2 for x in counts_list))\n","    return l2\n","\n","text = \"This is some hell of a way just to add add add this function\"\n","text_tok = tokenizer.tokenize(text)\n","print(text_tok)\n","print(l2_norm(text_tok))\n"]},{"cell_type":"code","execution_count":38,"id":"f0935c33","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","rdd_ = parquetFile.select(\"text\", \"id\").rdd"]},{"cell_type":"code","execution_count":40,"id":"d4177d36","metadata":{},"outputs":[],"source":["doc_norm = {}"]},{"cell_type":"code","execution_count":41,"id":"30565a98","metadata":{},"outputs":[],"source":["def calculate_doc_l2_norm(row):\n","    doc_id = row['id']\n","    text = row['text']\n","    tok_text = tokenizer.tokenize(text)\n","    return (doc_id, l2_norm(tok_text))\n","\n","doc_norm_rdd = rdd_.map(calculate_doc_l2_norm)"]},{"cell_type":"code","execution_count":null,"id":"fce6d32f-e0ac-4ee2-a010-9a1bb2ebbe43","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["doc_norm = dict(doc_norm_rdd.collect())"]},{"cell_type":"markdown","id":"4e759fcf-00c9-4fd7-8c2a-4b02834239fb","metadata":{},"source":["### Save to our Bucket ###"]},{"cell_type":"code","execution_count":26,"id":"a8a77f94","metadata":{},"outputs":[],"source":["base_dir='project_final_indexes'  \n","doc_l2_norm='doc_l2_norm_'\n","bucket_name='inverted_indexes_bucket'"]},{"cell_type":"code","execution_count":27,"id":"3aae1a27","metadata":{},"outputs":[],"source":["path = str(Path(base_dir) / f'{doc_l2_norm}.pkl')\n","bucket = None if bucket_name is None else get_bucket(bucket_name)\n","\n","Path(base_dir).mkdir(parents=True, exist_ok=True)\n","\n","blob = bucket.blob(path)\n","pickle.dump(doc_norm, open(path, 'wb'))\n","blob.upload_from_filename(path)"]},{"cell_type":"code","execution_count":29,"id":"a1519149","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Key: 4045403, Value: 125.50298801223818\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045413, Value: 28.319604517012593\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045419, Value: 38.57460304397182\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045426, Value: 22.891046284519195\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045432, Value: 46.119410230400824\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045456, Value: 79.39773296511683\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045466, Value: 36.15245496505044\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045471, Value: 40.07492981902776\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045479, Value: 108.4250893474384\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045516, Value: 15.394804318340652\n","--------------------------------------------------------------------------------\n"]}],"source":["for key, value in list(doc_norm.items())[:10]:\n","    PRINT(f\"Key: {key}, Value: {value}\")"]},{"cell_type":"markdown","id":"617b5d19-0b88-45a0-9628-51a2f13857aa","metadata":{},"source":["### Explore the RDD Dataset ###"]},{"cell_type":"code","execution_count":9,"id":"c1bd28f9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 3:============================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["['id', 'title', 'text', 'anchor_text']\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","rdd = parquetFile.rdd\n","\n","# Convert RDD to DataFrame to view column names\n","rdd = rdd.toDF()\n","\n","# Print column names\n","print(rdd.columns)"]},{"cell_type":"code","execution_count":10,"id":"b2aef22c-94ba-491f-84de-c20ee9130e1b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:=============================>                             (2 + 2) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+--------------------+--------------------+--------------------+\n","|     id|               title|                text|         anchor_text|\n","+-------+--------------------+--------------------+--------------------+\n","|4045403|Foster Air Force ...|'''Foster Air For...|[{1176764, Tactic...|\n","|4045413|     Torino Palavela|'''Palavela''', f...|[{77743, 2006 Win...|\n","|4045419|   Mad About the Boy|\"'''Mad About the...|[{34028256, Joyce...|\n","|4045426|       Shayne Breuer|'''Shayne Breuer'...|[{1838386, Woodvi...|\n","|4045432|         Parantaka I|'''Parantaka Chol...|[{1511716, Aditya...|\n","|4045456|Arundel (UK Parli...|'''Arundel''' was...|[{4665376, Arunde...|\n","|4045466|     Andrew Martinez|'''Luis Andrew Ma...|[{4860, Berkeley,...|\n","|4045471|    Vancouver VooDoo|The '''Vancouver ...|[{32706, Vancouve...|\n","|4045479|     Invisible plane|The '''Invisible ...|[{2260539, Ross A...|\n","|4045516|    Shopping channel|'''Shopping chann...|[{592899, special...|\n","|4045519|      Turgay (river)|The '''Turgay''' ...|[{16642, Kazakhst...|\n","|4045523|              Turgay|'''Turgay''' is a...|[{29992, Turkish}...|\n","|4045525|Heinrich Johann N...|'''Heinrich Johan...|[{5178678, Roodt-...|\n","|4045532|                 KUT|thumb|Belo Center...|[{1998, Austin, T...|\n","|4045544|          Dodge Cove|thumb|upright=.9|...|[{232346, Unincor...|\n","|4045546|            Triphone|In linguistics, a...|[{22760983, lingu...|\n","|4045554|Government House ...|'''Government Hou...|[{6898431, Barrin...|\n","|4045575|           Bud Abell|'''Harry Everett'...|[{492475, Linebac...|\n","|4045577|  Susette La Flesche|'''Susette La Fle...|[{4049833, Thomas...|\n","|4045581|Pomme de Terre Ri...|The '''Pomme de T...|[{3434750, United...|\n","+-------+--------------------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["doc_text_pairs_df.show()"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"gaaIoFViXyTg"},"source":["## Import Inverted Index .py File ##"]},{"cell_type":"code","execution_count":19,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":20,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/09 17:17:56 WARN SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"]}],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":21,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"12332723-b0d7-4385-815b-90993c39018e","metadata":{},"source":["## Building an inverted indexes ##"]},{"cell_type":"code","execution_count":22,"id":"8ad7e8dd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","from nltk.stem.porter import *\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","\n","\n","class Tokenizer:\n","\n","    def __init__(self):\n","        english_stopwords = frozenset(stopwords.words('english'))\n","        corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n","                            \"may\", \"first\", \"see\", \"people\", \"one\", \"two\",\n","                            \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                            \"many\", \"however\", \"would\", \"became\", \"yet\", \"oh\", \"even\",\n","                            \"within\", \"beyond\", \"hey\", \"since\", \"without\", \"ugh\", \"wow\",\n","                            \"ah\", \"already\", \"oops\", \"really\", \"still\", \"hmm\", \"among\"]\n","\n","\n","        self.all_stopwords = english_stopwords.union(corpus_stopwords)\n","        self.stemmer = PorterStemmer()\n","        self.special_words = ['3d', '4k', 'ip', 'js', 'ai', 'vr', 'ar', 'dl', 'ml', '09', '11', '9']\n","        \n","    def get_word_pattern(self):\n","        word_pattern = r\"(?:(?<=^)|(?<=\\s))(\\w+[-']?\\w+([-']\\w+)*)[,.']?(?<![,.!])\"\n","        return word_pattern\n","\n","\n","    def tokenize(self, text):\n","      RE_TOKENIZE = re.compile(rf\"\"\"\n","      (\n","          # Words\n","          (?P<WORD>{self.get_word_pattern()})\n","          # space\n","          |(?P<SPACE>[\\s\\t\\n]+)\n","          # everything else\n","          |(?P<OTHER>\\w+))\"\"\",  re.MULTILINE | re.IGNORECASE | re.VERBOSE | re.UNICODE)\n","\n","      return [self.stemmer.stem(v) for match in RE_TOKENIZE.finditer(text) for k, v in match.groupdict().items() if v is not None and k != 'SPACE' and bool(re.match(r'^[a-zA-Z0-9]+$', v)) and (len(v) > 2 or v.lower() in self.special_words) and v.lower() not in self.all_stopwords and len(v) <= 24] \n"]},{"cell_type":"code","execution_count":16,"id":"7245d7ff","metadata":{},"outputs":[],"source":["stopwords_list_ = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"again\", \"against\", \"ain't\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"an\", \"and\", \"another\", \"any\", \"anybody\", \"anyone\", \"anything\", \"anywhere\", \"are\", \"aren't\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"been\", \"before\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\", \"can\", \"can't\", \"cannot\", \"canst\", \"certain\", \"chiefly\", \"clean\", \"clear\", \"clearly\", \"come\", \"could\", \"couldn't\", \"dare\", \"daren't\", \"de\", \"definitely\", \"did\", \"didn't\", \"different\", \"do\", \"does\", \"doesn't\", \"doing\", \"done\", \"don't\", \"down\", \"downwards\", \"during\", \"each\", \"either\", \"else\", \"elsewhere\", \"enough\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"exactly\", \"except\", \"fairly\", \"far\", \"farther\", \"few\", \"fewer\", \"fifth\", \"first\", \"five\", \"followed\", \"following\", \"follows\", \"for\", \"forth\", \"four\", \"from\", \"further\", \"furthermore\", \"get\", \"gets\", \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"hence\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"least\", \"let\", \"let's\", \"like\", \"likely\", \"little\", \"look\", \"looking\", \"looks\", \"low\", \"lower\", \"made\", \"make\", \"makes\", \"making\", \"many\", \"may\", \"mayn't\", \"me\", \"mean\", \"meantime\", \"meanwhile\", \"might\", \"mightn't\", \"mine\", \"minus\", \"more\", \"most\", \"mostly\", \"much\", \"must\", \"mustn't\", \"my\", \"myself\", \"namely\", \"need\", \"needn't\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"notwithstanding\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"oughtn't\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"possible\", \"present\", \"presumably\", \"probably\", \"provided\", \"provides\", \"que\", \"quite\", \"qv\", \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"recent\", \"recently\", \"regarding\", \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\", \"round\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"someday\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that's\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"there's\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\", \"value\", \"various\", \"very\", \"via\", \"viz\", \"vs\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"welcome\", \"well\", \"went\", \"were\", \"weren't\", \"what\", \"what's\", \"whatever\", \"when\", \"when's\", \"whence\", \"whenever\", \"where\", \"where's\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"why's\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won't\", \"wonder\", \"would\", \"would\", \"wouldn't\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\", \"aboard\", \"about\", \"above\", \"absent\", \"across\", \"after\", \"against\", \"along\", \"alongside\", \"amid\", \"among\", \"amongst\", \"an\", \"and\", \"around\", \"as\", \"aside\", \"astride\", \"at\", \"atop\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"betwixt\", \"beyond\", \"by\", \"despite\", \"down\", \"during\", \"except\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\", \"per\", \"plus\", \"round\", \"save\", \"since\", \"through\", \"throughout\", \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"until\", \"unto\", \"up\", \"upon\", \"with\", \"within\", \"without\", \"worth\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c'mon\", \"c's\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"keep\", \"keeps\", \"kept\", \"kg\", \"know\", \"known\", \"knows\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"que\", \"quickly\", \"quite\", \"qv\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n","corpus_stopwords_ = set(stopwords_list_)"]},{"cell_type":"markdown","id":"9ed2fc6e-f41b-4141-83e0-2af895322881","metadata":{},"source":["### Pipeline Code to Generate the Index ###"]},{"cell_type":"code","execution_count":11,"id":"3521908d","metadata":{},"outputs":[],"source":["tokenizer = Tokenizer()"]},{"cell_type":"code","execution_count":13,"id":"b22fbc75-62f2-4e06-9cbe-865ae1178d71","metadata":{},"outputs":[],"source":["test = 'is a dsfsfsdfsdfdsfdsfdsewlkjkljkljlkjlkjkljkjkr ll pp the Spiderman September 3D 11 11 11 5 0 4 09 attacks attack gpu PDF 1997 \"Who is considered the father of USA?\"  bioinformatics september PDF 3D 4K television mawtānā d sharʿūṭā genetics video games'"]},{"cell_type":"code","execution_count":14,"id":"ce73eb99","metadata":{},"outputs":[{"data":{"text/plain":["['spiderman',\n"," 'septemb',\n"," '3d',\n"," '11',\n"," '11',\n"," '11',\n"," '09',\n"," 'attack',\n"," 'attack',\n"," 'gpu',\n"," 'pdf',\n"," '1997',\n"," 'consid',\n"," 'father',\n"," 'usa',\n"," 'bioinformat',\n"," 'septemb',\n"," 'pdf',\n"," '3d',\n"," '4k',\n"," 'televis',\n"," 'genet',\n"," 'video',\n"," 'game']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["\n","test = tokenizer.tokenize(test)\n","\n","test"]},{"cell_type":"code","execution_count":22,"id":"149fd77b","metadata":{},"outputs":[],"source":["res = word_count(test, 55410)"]},{"cell_type":"code","execution_count":23,"id":"4aa98c82","metadata":{},"outputs":[{"data":{"text/plain":["[('spiderman', (55410, 1)),\n"," ('septemb', (55410, 2)),\n"," ('3d', (55410, 2)),\n"," ('11', (55410, 3)),\n"," ('09', (55410, 1)),\n"," ('attack', (55410, 2)),\n"," ('gpu', (55410, 1)),\n"," ('pdf', (55410, 2)),\n"," ('1997', (55410, 1)),\n"," ('consid', (55410, 1)),\n"," ('father', (55410, 1)),\n"," ('usa', (55410, 1)),\n"," ('bioinformat', (55410, 1)),\n"," ('4k', (55410, 1)),\n"," ('televis', (55410, 1)),\n"," ('genet', (55410, 1)),\n"," ('video', (55410, 1)),\n"," ('game', (55410, 1))]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["res"]},{"cell_type":"code","execution_count":66,"id":"c3f22ef6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Number of stopwords for body index -> 797\n","--------------------------------------------------------------------------------\n"]}],"source":["PRINT(f'Number of stopwords for body index -> {len(tokenizer.all_stopwords)}')"]},{"cell_type":"code","execution_count":31,"id":"1e8f1888","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Number of stopwords for title index -> 216\n","--------------------------------------------------------------------------------\n"]}],"source":["PRINT(f'Number of stopwords for title index -> {len(tokenizer.all_stopwords)}')"]},{"cell_type":"code","execution_count":21,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["tokenizer = Tokenizer()\n","\n","NUM_BUCKETS = 124\n","\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","\n","def word_count(text, id):\n","  ''' \n","  Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","    text: str ,Text of one document\n","    id: int ,Document id\n","  Returns: List of tuples, A list of (token, (doc_id, tf)) pairs \n","  [token.group() for token in RE_WORD.finditer(text.lower())]\n","  '''\n","\n","  tokens = tokenizer.tokenize(text)\n","  c = Counter([tok for tok in tokens])\n","  return [(item[0],(id,item[1])) for item in c.items()]\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' \n","  Returns a sorted posting list by wiki_id.\n","  Parameters: unsorted_pl: list of (wiki_id, tf) tuples \n","  Returns: A sorted posting list.\n","  '''\n","  return sorted(unsorted_pl)\n","\n","def calculate_df(postings):\n","  ''' \n","  Takes a posting list RDD and calculate the df for each token.\n","  Parameters: postings: RDD ,An RDD where each element is a (token, posting_list) pair.\n","  Returns:RDD ,An RDD where each element is a (token, df) pair.\n","  '''\n","  return postings.map(lambda x: (x[0],len(x[1])))\n","\n","\n","def partition_postings_and_write(postings, base_dir_, bucket_name_):\n","  ''' \n","  A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning done by using `token2bucket`.\n","  Parameters:\n","  \n","  base_dir : string - Name for index directory\n","  bucket_name : string - Name of the bucket we want to store our inverted index directory (base_dir)\n","  -----------\n","    postings: RDD , An RDD where each item is a (w, posting_list) pair.\n","  Returns: RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to.\n","  '''\n","  return postings.map(lambda x: (token2bucket_id(x[0]),x)).groupByKey().map(lambda x: InvertedIndex.write_a_posting_list(x,\n","                                                                                                                         base_dir= base_dir_,\n","                                                                                                                         bucket_name=bucket_name_))"]},{"cell_type":"markdown","id":"d45ceaa4-b94f-4c5f-99bf-8b400c9aedbd","metadata":{},"source":["### Generate Inverted Index For Title & Id Pairs ###"]},{"cell_type":"code","execution_count":45,"id":"58176c45-b468-481d-983f-781b51f90705","metadata":{},"outputs":[],"source":["title_id_pairs = parquetFile.select(\"title\", \"id\").rdd"]},{"cell_type":"code","execution_count":46,"id":"78c4b7aa","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:=====================================================> (121 + 3) / 124]\r"]},{"name":"stdout","output_type":"stream","text":["Number of title & id pairs -> 6348910\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["count_pairs = title_id_pairs.count()\n","print(f'Number of title & id pairs -> {count_pairs}')"]},{"cell_type":"markdown","id":"7eddaa48-7619-4e1c-81e1-53aa0c9f5194","metadata":{},"source":["#### Generate Posting Lists for Title Index & Save them ####"]},{"cell_type":"code","execution_count":48,"id":"55c8764e","metadata":{"id":"0b5d7296","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception in thread \"serve RDD 35\" java.net.SocketTimeoutException: Accept timed out\n","\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n","\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n","\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n","\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n","                                                                                \r"]}],"source":["# time the index creation time\n","t_start = time()\n","\n","# word counts map\n","word_counts = title_id_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered,\n","                                 base_dir_='title_index_directory_final',\n","                                 bucket_name_='inverted_indexes_bucket').collect()\n","\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":49,"id":"3dbc0e14","metadata":{"id":"348pECY8cH-T","nbgrader":{"grade":true,"grade_id":"cell-index_const_time","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","The total amount of time required to create the index is 4.68 minutes\n","--------------------------------------------------------------------------------\n"]}],"source":["PRINT(f'The total amount of time required to create the index is {(index_const_time/60):.2f} minutes')"]},{"cell_type":"code","execution_count":50,"id":"66660993-d427-452d-83e6-fa56958dcf5d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["title_index_directory_final/0_000.bin\n","title_index_directory_final/0_posting_locs.pickle\n","title_index_directory_final/100_000.bin\n","title_index_directory_final/100_posting_locs.pickle\n","title_index_directory_final/101_000.bin\n","title_index_directory_final/101_posting_locs.pickle\n","title_index_directory_final/102_000.bin\n","title_index_directory_final/102_posting_locs.pickle\n","title_index_directory_final/103_000.bin\n","title_index_directory_final/103_posting_locs.pickle\n","--------------------------------------------------------------------------------\n","Done\n","--------------------------------------------------------------------------------\n"]}],"source":["blobs_test = client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n","                              prefix='title_index_directory_final')\n","count = 0\n","# Iterate over the blobs\n","for blob in blobs_test:\n","    if count == 10: break\n","    count+=1\n","    print(blob.name)\n","    \n","PRINT('Done')"]},{"cell_type":"code","execution_count":51,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Done.\n","--------------------------------------------------------------------------------\n"]}],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","\n","for blob in client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n","                             prefix='title_index_directory_final'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","    \n","PRINT('Done.')"]},{"cell_type":"markdown","id":"f6f66e3a","metadata":{"id":"VhAV0A6dNZWY"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":52,"id":"a5d2cfb6","metadata":{"id":"54vqT_0WNc3w"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Done\n","--------------------------------------------------------------------------------\n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","\n","# write the global stats out\n","inverted.write_index(base_dir='project_final_indexes', \n","                     name='title_index_final',\n","                     bucket_name='inverted_indexes_bucket')\n","\n","PRINT('Done')"]},{"cell_type":"markdown","id":"def55262-7232-4a30-8f0d-b19de810877f","metadata":{},"source":["#### Visualize the Index ####"]},{"cell_type":"code","execution_count":28,"id":"53ee777a-3317-45a1-846c-7f254493b841","metadata":{},"outputs":[],"source":["def load_index(base_dir, name, bucket_name):\n","    # Initialize the client\n","    client = storage.Client()\n","    \n","    # Get the bucket\n","    bucket = client.get_bucket(bucket_name)\n","    \n","    # Define the path to the index file in the bucket\n","    index_path = f'{base_dir}/{name}.pkl'\n","    \n","    # Get the blob (file) from the bucket\n","    blob = bucket.blob(index_path)\n","    \n","    # Download the blob into memory\n","    index_data = blob.download_as_string()\n","    \n","    # Load the index from the downloaded data\n","    inverted_index = pickle.loads(index_data)\n","    \n","    return inverted_index"]},{"cell_type":"code","execution_count":38,"id":"d9dc349a-4e02-4980-986c-651ebeea2510","metadata":{},"outputs":[],"source":["inverted_index = load_index(base_dir='project_final_indexes', \n","                            name='title_stem_index',\n","                            bucket_name='inverted_indexes_bucket')"]},{"cell_type":"code","execution_count":39,"id":"1b5fe63c-088a-468d-b280-e2db172faefe","metadata":{},"outputs":[],"source":["posting_locs = inverted_index.posting_locs\n","df_data = inverted_index.df\n","\n","# Convert the posting_locs dictionary to a DataFrame\n","posting_locs_df = pd.DataFrame.from_dict(posting_locs, orient='index', columns=['Posting List'])\n","\n","# Convert the df_data dictionary to a DataFrame\n","df_data_df = pd.DataFrame.from_dict(df_data, orient='index', columns=['Document Frequency'])"]},{"cell_type":"code","execution_count":40,"id":"08359fa8-c762-41d8-bced-4f6ba279ed3e","metadata":{},"outputs":[],"source":["df_data_df_sorted = df_data_df.sort_values(by='Document Frequency', ascending=True)"]},{"cell_type":"code","execution_count":41,"id":"b82b543a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Data Frame shape -> (25230, 1)\n","--------------------------------------------------------------------------------\n"]}],"source":["PRINT(f'Data Frame shape -> {df_data_df_sorted.shape}')"]},{"cell_type":"code","execution_count":73,"id":"9b4263b2-452f-4514-8954-4939701f68bd","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Data Frame shape -> (26981, 1)\n","--------------------------------------------------------------------------------\n"]}],"source":["PRINT(f'Data Frame shape -> {df_data_df_sorted.shape}')"]},{"cell_type":"code","execution_count":42,"id":"c0a003e1","metadata":{},"outputs":[],"source":["tesla_frequency = df_data_df_sorted.loc['bioinformat', 'Document Frequency']"]},{"cell_type":"code","execution_count":43,"id":"c6e23b8f","metadata":{},"outputs":[{"data":{"text/plain":["66"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["tesla_frequency"]},{"cell_type":"code","execution_count":54,"id":"a8021387-04a6-4aab-b7a3-e8c857f8ad31","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Document Frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>membership</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>jena</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>deux</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>frye</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>adkin</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>tesla</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>kingsburi</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>staunton</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>liceo</th>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>gaspard</th>\n","      <td>101</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Document Frequency\n","membership                 101\n","jena                       101\n","deux                       101\n","frye                       101\n","adkin                      101\n","tesla                      101\n","kingsburi                  101\n","staunton                   101\n","liceo                      101\n","gaspard                    101"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["df_data_df_sorted.head(10)"]},{"cell_type":"code","execution_count":55,"id":"a91101c3-f253-4b3a-8600-f55ee85a3bc6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Done with Title Index !\n","--------------------------------------------------------------------------------\n"]}],"source":["PRINT('Done with Title Index !')"]},{"cell_type":"markdown","id":"581c0661-462c-44d9-b161-f68450ce1c74","metadata":{},"source":["### Generate Inverted Idex For Body & Id Pairs ###"]},{"cell_type":"code","execution_count":28,"id":"eeea605f-717f-4661-a20f-f78781426035","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","body_id_pairs = parquetFile.select(\"text\", \"id\").rdd"]},{"cell_type":"code","execution_count":29,"id":"f04df369-d1f8-4421-a17f-7d457766fe09","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# time the index creation time\n","t_start = time()\n","\n","# word counts map\n","word_counts = body_id_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered,\n","                                 base_dir_='body_index_directory_final',\n","                                 bucket_name_='inverted_indexes_bucket').collect()\n","\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":30,"id":"63f8f943-82f8-47f2-9de9-354168f8484d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","The total amount of time required to create the index is 3.725 hours\n","--------------------------------------------------------------------------------\n"]}],"source":["PRINT(f'The total amount of time required to create the index is {index_const_time/60/60:.3f} hours')"]},{"cell_type":"code","execution_count":31,"id":"465f49e2-5536-41ec-88cb-a98e459d1019","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["body_index_directory_final/0_000.bin\n","body_index_directory_final/0_001.bin\n","body_index_directory_final/0_002.bin\n","body_index_directory_final/0_003.bin\n","body_index_directory_final/0_004.bin\n","body_index_directory_final/0_005.bin\n","body_index_directory_final/0_006.bin\n","body_index_directory_final/0_007.bin\n","body_index_directory_final/0_008.bin\n","body_index_directory_final/0_009.bin\n","--------------------------------------------------------------------------------\n","Done\n","--------------------------------------------------------------------------------\n"]}],"source":["blobs_test = client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n","                              prefix='body_index_directory_final')\n","count = 0\n","# Iterate over the blobs\n","for blob in blobs_test:\n","    if count == 10:break\n","    count+=1\n","    print(blob.name)\n","    \n","PRINT('Done')"]},{"cell_type":"code","execution_count":32,"id":"e509e28b-eea4-4b94-a2f2-3cbc7ed5b724","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Done.\n","--------------------------------------------------------------------------------\n"]}],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","\n","for blob in client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n","                             prefix='body_index_directory_final'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)\n","    \n","PRINT('Done.')"]},{"cell_type":"code","execution_count":33,"id":"c3e5957c-ceef-4db9-b06f-dd9eec01c15e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Done\n","--------------------------------------------------------------------------------\n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","\n","# write the global stats out\n","inverted.write_index(base_dir='project_final_indexes', \n","                     name='body_index_final',\n","                     bucket_name='inverted_indexes_bucket')\n","\n","PRINT('Done')"]},{"cell_type":"markdown","id":"919aa8f6-5404-44dc-840e-d45bcbe830a0","metadata":{},"source":["## Generate Document Lenghs for Body & Title ##"]},{"cell_type":"markdown","id":"d3bbc474-bb74-4dae-8dca-59c5c0f4005f","metadata":{},"source":["### Generate Body Document Length Dictionary ###"]},{"cell_type":"code","execution_count":11,"id":"3b275a18-fe81-46ce-8112-2eee642a5020","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","body_id_rdd = parquetFile.select(\"text\", \"id\").rdd"]},{"cell_type":"code","execution_count":12,"id":"3682cb66-0ad8-4ece-84d4-6c9a9429ac3f","metadata":{},"outputs":[],"source":["body_dictionary_lenght = {} # maps (key,value)=(doc_id, body_doc_lenght)"]},{"cell_type":"code","execution_count":14,"id":"9e5c614b-7ad0-446c-87a3-763e7575ab4f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def calculate_doc_length(row):\n","    doc_id = row['id']\n","    text = row['text']\n","    text_tok = tokenize(text)\n","    return (doc_id, len(text_tok))\n","\n","# Apply the function to each row of the RDD and map it to (doc_id, doc_length) pairs\n","doc_lengths_rdd = body_id_rdd.map(calculate_doc_length)\n","\n","# Collect the results as a dictionary\n","body_dictionary_lenght = dict(doc_lengths_rdd.collect())"]},{"cell_type":"markdown","id":"41271cf6-be61-4664-8f04-5e1790c8ff08","metadata":{},"source":["#### Save it to our Bucket ####"]},{"cell_type":"code","execution_count":15,"id":"b7d13e9e-dd44-4f80-91b2-1f6958c4186f","metadata":{},"outputs":[],"source":["base_dir='project_final_indexes' \n","dl_name='body_dl_'\n","bucket_name='inverted_indexes_bucket'"]},{"cell_type":"code","execution_count":16,"id":"532f5af0-9ba3-4d94-b114-ca6be79127e7","metadata":{},"outputs":[],"source":["def get_bucket(bucket_name):\n","    return storage.Client('irprojectilayvictor').bucket(bucket_name)"]},{"cell_type":"code","execution_count":17,"id":"9616f3c4","metadata":{},"outputs":[],"source":["path = str(Path(base_dir) / f'{dl_name}.pkl')"]},{"cell_type":"code","execution_count":18,"id":"4a4a19b8-4a42-43da-a960-8872cd81e9e2","metadata":{},"outputs":[],"source":["path = str(Path(base_dir) / f'{dl_name}.pkl')\n","bucket = None if bucket_name is None else get_bucket(bucket_name)\n","\n","Path(base_dir).mkdir(parents=True, exist_ok=True)\n","\n","blob = bucket.blob(path)\n","pickle.dump(body_dictionary_lenght, open(path, 'wb'))\n","blob.upload_from_filename(path)"]},{"cell_type":"markdown","id":"991b2d1c-bf05-458a-b2d3-93ef28324899","metadata":{},"source":["#### Verify that the Dictionary Saved Successfully ####"]},{"cell_type":"code","execution_count":19,"id":"187907c6-5ceb-4a28-ae40-d55fe264060c","metadata":{},"outputs":[],"source":["client = storage.Client()\n","bucket = client.get_bucket(bucket_name)\n","\n","# Download the pickled file from the bucket\n","blob = bucket.blob(path)\n","blob.download_to_filename(path)\n","\n","with open(path, 'rb') as f:\n","    loaded_dict = pickle.load(f)"]},{"cell_type":"code","execution_count":20,"id":"04661a01","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Key: 4045403, Value: 1665\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045413, Value: 241\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045419, Value: 584\n","--------------------------------------------------------------------------------\n"]}],"source":["# final\n","for key, value in list(loaded_dict.items())[:3]:\n","    PRINT(f\"Key: {key}, Value: {value}\")"]},{"cell_type":"markdown","id":"da74dd18-3cf3-441a-a9f7-3992c446e78e","metadata":{},"source":["### Generate Title Document Length Dictionary ###"]},{"cell_type":"code","execution_count":15,"id":"50b90f63","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){1,24}\"\"\", re.UNICODE)\n","stemmer = PorterStemmer()\n","\n","def tokenize(text):\n","    clean_text = []\n","    text = text.lower()\n","    for token in RE_WORD.finditer(text):\n","        stemmed_token = stemmer.stem(token.group())\n","        if stemmed_token not in english_stopwords:\n","            clean_text.append(stemmed_token)\n","    return clean_text"]},{"cell_type":"code","execution_count":14,"id":"f0da85e9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","title_id_rdd = parquetFile.select(\"title\", \"id\").rdd"]},{"cell_type":"code","execution_count":15,"id":"d3126f75","metadata":{},"outputs":[],"source":["title_dictionary_length = {} # maps (key, value)=(doc_id, title_length)"]},{"cell_type":"code","execution_count":19,"id":"25d9f57e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def calculate_title_length(row):\n","    doc_id = row['id']\n","    title = row['title']\n","    title_token = tokenize(title)\n","    return (doc_id, len(title_token))\n","\n","# Apply the function to each row of the RDD and map it to (doc_id, title) pairs\n","title_rdd = title_id_rdd.map(calculate_title_length)\n","\n","title_dictionary_length = dict(title_rdd.collect())"]},{"cell_type":"code","execution_count":20,"id":"1e563906","metadata":{},"outputs":[],"source":["base_dir='project_final_indexes' \n","dl_name='title_DL_'\n","bucket_name='inverted_indexes_bucket'"]},{"cell_type":"code","execution_count":23,"id":"dad7e10f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Done.\n","--------------------------------------------------------------------------------\n"]}],"source":["path = str(Path(base_dir) / f'{dl_name}.pkl')\n","bucket = None if bucket_name is None else get_bucket(bucket_name)\n","\n","Path(base_dir).mkdir(parents=True, exist_ok=True)\n","\n","blob = bucket.blob(path)\n","pickle.dump(title_dictionary_length, open(path, 'wb'))\n","blob.upload_from_filename(path)\n","\n","PRINT('Done.')"]},{"cell_type":"code","execution_count":24,"id":"561745c5","metadata":{},"outputs":[],"source":["client = storage.Client()\n","bucket = client.get_bucket(bucket_name)\n","\n","# Download the pickled file from the bucket\n","blob = bucket.blob(path)\n","blob.download_to_filename(path)\n","\n","with open(path, 'rb') as f:\n","    loaded_dict = pickle.load(f)"]},{"cell_type":"code","execution_count":25,"id":"f00edf6c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Key: 4045403, Value: 4\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045413, Value: 2\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045419, Value: 2\n","--------------------------------------------------------------------------------\n"]}],"source":["for key, value in list(loaded_dict.items())[:3]:\n","    PRINT(f\"Key: {key}, Value: {value}\")"]},{"cell_type":"markdown","id":"af33bea4-3bc5-446a-b101-ebcb753d31b3","metadata":{},"source":["### Generate Title Document Dictionary ###"]},{"cell_type":"code","execution_count":83,"id":"b67663ef-1323-4e5a-ac01-d7c795ba3bcd","metadata":{},"outputs":[],"source":["title_dictionary = {} # maps (key, value)=(doc_id, title_name)"]},{"cell_type":"code","execution_count":26,"id":"a47dbf1c-792e-4f4a-8ee5-abfa5746b186","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","title_id_rdd = parquetFile.select(\"title\", \"id\").rdd"]},{"cell_type":"code","execution_count":12,"id":"ef19c7c4-178b-48fc-8a68-0ab1db8acdd0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def calculate_title(row):\n","    doc_id = row['id']\n","    title = row['title']\n","    return (doc_id, title)\n","\n","# Apply the function to each row of the RDD and map it to (doc_id, title) pairs\n","title_rdd = title_id_rdd.map(calculate_title)\n","\n","title_dictionary = dict(title_rdd.collect())"]},{"cell_type":"code","execution_count":12,"id":"d70a7e2e-dacb-4200-bda4-864ec3b1d2e8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Key: 4045403, Value: Foster Air Force Base\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045413, Value: Torino Palavela\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 4045419, Value: Mad About the Boy\n","--------------------------------------------------------------------------------\n"]}],"source":["for key, value in list(title_dictionary.items())[:3]:\n","    PRINT(f\"Key: {key}, Value: {value}\")"]},{"cell_type":"markdown","id":"a0a9edeb-2d5b-4994-979f-06c7fcfae024","metadata":{},"source":["#### Save it to our Bucket ####"]},{"cell_type":"code","execution_count":19,"id":"b2d8ba7a-aa00-46b1-913a-b8c3d816611e","metadata":{},"outputs":[],"source":["base_dir='project_final_indexes' \n","dl_name='doc_norm'\n","bucket_name='inverted_indexes_bucket'"]},{"cell_type":"code","execution_count":null,"id":"78af3185-9acd-49b3-a8f9-193f36776ce7","metadata":{},"outputs":[],"source":["path = str(Path(base_dir) / f'{dl_name}.pkl')\n","bucket = None if bucket_name is None else get_bucket(bucket_name)\n","\n","Path(base_dir).mkdir(parents=True, exist_ok=True)\n","\n","blob = bucket.blob(path)\n","pickle.dump(doc_norm, open(path, 'wb'))\n","blob.upload_from_filename(path)"]},{"cell_type":"markdown","id":"c52dee14","metadata":{"id":"fc0667a9","nbgrader":{"grade":false,"grade_id":"cell-2a6d655c112e79c5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Execute PageRank & Save ##"]},{"cell_type":"code","execution_count":23,"id":"31a516e2","metadata":{"id":"yVjnTvQsegc-"},"outputs":[],"source":["def generate_graph(pages):\n","  ''' \n","  Compute the directed graph generated by wiki links.\n","  Parameters: An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns: \n","   - edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","   - vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  edges_w_dup = pages.flatMap(lambda page: [(page[0],anchor[0]) for anchor in page[1]])\n","  edges = edges_w_dup.distinct()\n","  vertices = edges.flatMap(lambda x: x)\n","  vertices = vertices.distinct()\n","  vertices = vertices.map(lambda x: Row(x)) #converting entries to a format that fits toDF() func up next\n","  return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"4f9bd4b2-75b1-4f46-a779-9d9e604fdc8e","metadata":{},"outputs":[],"source":["t_start = time()\n","\n","pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n","\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","\n","pr_time = time() - t_start"]},{"cell_type":"code","execution_count":27,"id":"c3620817","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 230:===================================================> (121 + 2) / 124]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|     id|          pagerank|\n","+-------+------------------+\n","|3434750| 9913.728782160779|\n","|  10568| 5385.349263642038|\n","|  32927| 5282.081575765276|\n","|  30680|  5128.23370960412|\n","|5843419| 4957.567686263868|\n","|  68253| 4769.278265355157|\n","|  31717| 4486.350180548311|\n","|  11867|4146.4146509127695|\n","|  14533|3996.4664408855006|\n","| 645042| 3531.627089803743|\n","|  17867|3246.0983906041415|\n","|5042916| 2991.945739166178|\n","|4689264| 2982.324883041747|\n","|  14532| 2934.746829203171|\n","|  25391|   2903.5462235134|\n","|   5405| 2891.416329154635|\n","|4764461| 2834.366987332661|\n","|  15573|2783.8651181588384|\n","|   9316|2782.0396464137702|\n","|8569916|2775.2861918400154|\n","+-------+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["pr.show()"]},{"cell_type":"code","execution_count":null,"id":"83a57122-f2c3-49f9-959c-d65490a552ad","metadata":{},"outputs":[],"source":["pr_dict = pr.collect()\n","pr_dict = {row['id']: row['pagerank'] for row in pr_dict}"]},{"cell_type":"code","execution_count":9,"id":"4ec59cef","metadata":{},"outputs":[],"source":["base_dir='project_pageRank' \n","page_rank ='pageRank'\n","bucket_name='bucket_for_index'"]},{"cell_type":"code","execution_count":35,"id":"f1326cdf","metadata":{},"outputs":[],"source":["path = str(Path(base_dir) / f'{page_rank}.pkl')\n","bucket = None if bucket_name is None else get_bucket(bucket_name)\n","\n","Path(base_dir).mkdir(parents=True, exist_ok=True)\n","\n","blob = bucket.blob(path)\n","pickle.dump(pr_dict, open(path, 'wb'))\n","blob.upload_from_filename(path)"]},{"cell_type":"code","execution_count":37,"id":"c25458be","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------------------------------------\n","Key: 3434750, doc page rank: 9913.728782160779\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 10568, doc page rank: 5385.349263642038\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 32927, doc page rank: 5282.081575765276\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 30680, doc page rank: 5128.23370960412\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 5843419, doc page rank: 4957.567686263868\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 68253, doc page rank: 4769.278265355157\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 31717, doc page rank: 4486.350180548311\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 11867, doc page rank: 4146.4146509127695\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 14533, doc page rank: 3996.4664408855006\n","--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------\n","Key: 645042, doc page rank: 3531.627089803743\n","--------------------------------------------------------------------------------\n"]}],"source":["for key, value in list(pr_dict.items())[:10]:\n","    PRINT(f\"Key: {key}, doc page rank: {value}\")"]},{"cell_type":"markdown","id":"61223055","metadata":{},"source":["# Generate Word Embedding #\n","\n","## Ilay dont forget to save the model in our bucket as '.pkl' so we can load it and use it at runtime"]},{"cell_type":"code","execution_count":43,"id":"98b628fd-f60b-4a77-976b-f46123fa945f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["['id', 'title', 'text', 'anchor_text']\n"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","rdd = parquetFile.rdd\n","\n","# Convert RDD to DataFrame to view column names\n"]},{"cell_type":"code","execution_count":30,"id":"8c5129a8-e37a-411a-a475-258ddd6c38ba","metadata":{},"outputs":[],"source":["tokenizer = Tokenizer() # Run the cell of Tokenizer() class before (the class somewhen in the middle)"]},{"cell_type":"markdown","id":"ffa280bd-7f9d-439a-8bba-f59634138755","metadata":{},"source":["## Train Word2Ven Model ##"]},{"cell_type":"code","execution_count":31,"id":"dadc423f","metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'RDD' object has no attribute 'withColumn'","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_10026/2548520879.py\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mtokenize_udf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mudf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mArrayType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mStringType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"tokens\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenize_udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"text\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# Train Word2Vec model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mAttributeError\u001B[0m: 'RDD' object has no attribute 'withColumn'"]}],"source":["from pyspark.ml.feature import Word2Vec\n","\n","\n","tokenize_udf = udf(lambda text: tokenizer.tokenize(text), ArrayType(StringType()))\n","df = rdd.withColumn(\"tokens\", tokenize_udf(\"text\"))\n","\n","# Train Word2Vec model\n","word2Vec = Word2Vec(vectorSize=100, minCount=10, windowSize=10, inputCol=\"tokens\", outputCol=\"vectors\")\n","model = word2Vec.fit(df)"]},{"cell_type":"markdown","id":"ac9685ea-71ae-4beb-9cc0-1e7b56efd51f","metadata":{},"source":["## Save it first thing after training to avoid kernel issues ##"]},{"cell_type":"code","execution_count":null,"id":"835a02ea","metadata":{},"outputs":[],"source":["base_dir='Work2Vec_dir' \n","page_rank ='word2vec'\n","bucket_name='bucket_for_index'"]},{"cell_type":"code","execution_count":null,"id":"efa4f57c-1fd8-41a9-9502-c0be5a5d92ba","metadata":{},"outputs":[],"source":["def get_bucket(bucket_name):\n","    return storage.Client('irprojectilayvictor').bucket(bucket_name)"]},{"cell_type":"code","execution_count":null,"id":"f3c90ce0-0c0f-4634-99bb-82d7dcc7b165","metadata":{},"outputs":[],"source":["path = str(Path(base_dir) / f'{page_rank}.pkl')\n","bucket = None if bucket_name is None else get_bucket(bucket_name)\n","\n","Path(base_dir).mkdir(parents=True, exist_ok=True)\n","\n","blob = bucket.blob(path)\n","pickle.dump(pr_dict, open(path, 'wb'))\n","blob.upload_from_filename(path)\n","\n","print('Saved !')"]},{"cell_type":"code","execution_count":34,"id":"579270d0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 11:===========================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["[Row(id=1176764, text='Tactical Air Command'), Row(id=91416, text='Victoria County'), Row(id=136747, text='Victoria, Texas'), Row(id=23814944, text='USGS'), Row(id=185235, text='Air Force Base'), Row(id=32090, text='United States Air Force'), Row(id=29810, text='Texas'), Row(id=91416, text='Victoria County'), Row(id=136747, text='Victoria'), Row(id=32927, text='World War II'), Row(id=1176764, text='Tactical Air Command'), Row(id=325329, text='Cold War'), Row(id=136815, text='Georgetown'), Row(id=23869026, text='U.S. Army Air Corps'), Row(id=1875937, text='Brooks Field'), Row(id=53848, text='San Antonio'), Row(id=763002, text='AT-6'), Row(id=1484086, text='Guadalupe River'), Row(id=18717338, text='$'), Row(id=373962, text='War Department'), Row(id=1029642, text='flight officers'), Row(id=6120180, text='warrant officers'), Row(id=201930, text='second lieutenants'), Row(id=60098, text='Pearl Harbor'), Row(id=1468292, text='WACs'), Row(id=763002, text='AT-6'), Row(id=7211, text='P-40'), Row(id=1856667, text='Matagorda Island'), Row(id=15152886, text='Aloe\\xa0AAF'), Row(id=15456875, text='Curtiss JN-6H'), Row(id=1875937, text='Brooks Field'), Row(id=53848, text='San Antonio'), Row(id=16772, text='Korean War'), Row(id=3402989, text='Air Training Command'), Row(id=938411, text='T-28'), Row(id=359127, text='T-33'), Row(id=28118, text='Strategic Air Command'), Row(id=1176764, text='Tactical Air Command'), Row(id=4879797, text='Greenville AFB'), Row(id=11445694, text='Laredo AFB'), Row(id=359331, text='F-86F Sabre'), Row(id=3809275, text='Ninth Air Force'), Row(id=32611, text='Vietnam War'), Row(id=11103727, text='37th Tactical Fighter Wing'), Row(id=50418330, text='Phu Cat Air Base'), Row(id=635378, text='KB-50D Superfortress'), Row(id=229921, text='F-100C/D Super Sabre'), Row(id=464050, text='Nineteenth Air Force'), Row(id=287824, text='1958 Lebanon crisis'), Row(id=7515928, text='Iraq'), Row(id=19833621, text='450th Tactical Fighter Wing'), Row(id=24113, text='President'), Row(id=8182, text='Dwight D. Eisenhower'), Row(id=56080538, text='Seymour Johnson AFB'), Row(id=21650, text='North Carolina'), Row(id=54533, text='Lyndon B. Johnson'), Row(id=381546, text='Ralph Yarborough'), Row(id=6940711, text='Clark Thompson'), Row(id=128852, text='Minot AFB'), Row(id=21651, text='North Dakota'), Row(id=19833621, text='450th Bombardment Wing'), Row(id=18933037, text='B-52H'), Row(id=26470738, text='Gulf Coast Air Corps Training Center'), Row(id=3402989, text='Air Corps Flying Training Command (later Army Air Forces Flying Training Command, Army Air Forces Training Command)'), Row(id=19833621, text='450th Fighter-Bomber Wing (later 450th Fighter-Day Wing, 450th Tactical Fighter Wing)'), Row(id=22445776, text='450th Fighter-Bomber Group (later 450th Fighter-Day Group)'), Row(id=9397737, text='322d Fighter-Day Group'), Row(id=464050, text='Nineteenth Air Force'), Row(id=763002, text='AT-6 Texan'), Row(id=7211, text='P-40 Warhawk'), Row(id=359331, text='F-86 Sabre'), Row(id=229921, text='F-100 Super Sabre'), Row(id=424408, text='General Services Administration'), Row(id=15152886, text='Aloe Army Airfield'), Row(id=460511, text='C-54 Skymaster'), Row(id=2696255, text='Victoria Regional Airport'), Row(id=14298059, text='Civil Air Patrol'), Row(id=57636070, text='Texas World War II Army Airfields'), Row(id=42254836, text='77th Flying Training Wing (World War II)')]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["anchor_text_first_row = parquetFile.select(\"anchor_text\").first()[0]\n","print(anchor_text_first_row)"]},{"cell_type":"code","execution_count":45,"id":"9e354be3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/09 20:02:24 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000108 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:24.715]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:24.715]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:24.715]Killed by external signal\n",".\n","24/03/09 20:02:24 ERROR YarnScheduler: Lost executor 106 on cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000108 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:24.715]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:24.715]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:24.715]Killed by external signal\n",".\n","24/03/09 20:02:24 WARN TaskSetManager: Lost task 9.0 in stage 22.0 (TID 649) (cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal executor 106): ExecutorLostFailure (executor 106 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000108 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:24.715]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:24.715]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:24.715]Killed by external signal\n",".\n","24/03/09 20:02:24 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 106 for reason Container from a bad node: container_1710004378772_0001_01_000108 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:24.715]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:24.715]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:24.715]Killed by external signal\n",".\n","24/03/09 20:02:40 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000112 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:40.091]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:40.092]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:40.092]Killed by external signal\n",".\n","24/03/09 20:02:40 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 110 for reason Container from a bad node: container_1710004378772_0001_01_000112 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:40.091]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:40.092]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:40.092]Killed by external signal\n",".\n","24/03/09 20:02:40 ERROR YarnScheduler: Lost executor 110 on cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000112 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:40.091]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:40.092]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:40.092]Killed by external signal\n",".\n","24/03/09 20:02:40 WARN TaskSetManager: Lost task 9.1 in stage 22.0 (TID 657) (cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal executor 110): ExecutorLostFailure (executor 110 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000112 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:40.091]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:40.092]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:40.092]Killed by external signal\n",".\n","24/03/09 20:02:50 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000111 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:50.734]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:50.734]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:50.734]Killed by external signal\n",".\n","24/03/09 20:02:50 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 109 for reason Container from a bad node: container_1710004378772_0001_01_000111 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:50.734]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:50.734]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:50.734]Killed by external signal\n",".\n","24/03/09 20:02:50 ERROR YarnScheduler: Lost executor 109 on cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000111 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:50.734]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:50.734]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:50.734]Killed by external signal\n",".\n","24/03/09 20:02:50 WARN TaskSetManager: Lost task 15.0 in stage 22.0 (TID 655) (cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal executor 109): ExecutorLostFailure (executor 109 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000111 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:02:50.734]Container killed on request. Exit code is 143\n","[2024-03-09 20:02:50.734]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:02:50.734]Killed by external signal\n",".\n","24/03/09 20:03:15 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000110 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:03:15.381]Container killed on request. Exit code is 143\n","[2024-03-09 20:03:15.381]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:03:15.382]Killed by external signal\n",".\n","24/03/09 20:03:15 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 108 for reason Container from a bad node: container_1710004378772_0001_01_000110 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:03:15.381]Container killed on request. Exit code is 143\n","[2024-03-09 20:03:15.381]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:03:15.382]Killed by external signal\n",".\n","24/03/09 20:03:15 ERROR YarnScheduler: Lost executor 108 on cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000110 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:03:15.381]Container killed on request. Exit code is 143\n","[2024-03-09 20:03:15.381]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:03:15.382]Killed by external signal\n",".\n","24/03/09 20:03:15 WARN TaskSetManager: Lost task 5.0 in stage 22.0 (TID 645) (cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal executor 108): ExecutorLostFailure (executor 108 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000110 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:03:15.381]Container killed on request. Exit code is 143\n","[2024-03-09 20:03:15.381]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:03:15.382]Killed by external signal\n",".\n","24/03/09 20:04:11 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000107 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:11.561]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:11.562]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:11.562]Killed by external signal\n",".\n","24/03/09 20:04:11 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 105 for reason Container from a bad node: container_1710004378772_0001_01_000107 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:11.561]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:11.562]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:11.562]Killed by external signal\n",".\n","24/03/09 20:04:11 ERROR YarnScheduler: Lost executor 105 on cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000107 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:11.561]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:11.562]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:11.562]Killed by external signal\n",".\n","24/03/09 20:04:11 WARN TaskSetManager: Lost task 19.0 in stage 22.0 (TID 663) (cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal executor 105): ExecutorLostFailure (executor 105 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000107 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:11.561]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:11.562]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:11.562]Killed by external signal\n",".\n","24/03/09 20:04:36 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000109 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:35.991]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:35.991]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:35.991]Killed by external signal\n",".\n","24/03/09 20:04:36 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 107 for reason Container from a bad node: container_1710004378772_0001_01_000109 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:35.991]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:35.991]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:35.991]Killed by external signal\n",".\n","24/03/09 20:04:36 ERROR YarnScheduler: Lost executor 107 on cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000109 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:35.991]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:35.991]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:35.991]Killed by external signal\n",".\n","24/03/09 20:04:36 WARN TaskSetManager: Lost task 18.0 in stage 22.0 (TID 662) (cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal executor 107): ExecutorLostFailure (executor 107 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000109 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:35.991]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:35.991]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:35.991]Killed by external signal\n",".\n","24/03/09 20:04:43 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000113 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:43.566]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:43.566]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:43.566]Killed by external signal\n",".\n","24/03/09 20:04:43 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 111 for reason Container from a bad node: container_1710004378772_0001_01_000113 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:43.566]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:43.566]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:43.566]Killed by external signal\n",".\n","24/03/09 20:04:43 ERROR YarnScheduler: Lost executor 111 on cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000113 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:43.566]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:43.566]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:43.566]Killed by external signal\n",".\n","24/03/09 20:04:43 WARN TaskSetManager: Lost task 19.1 in stage 22.0 (TID 664) (cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal executor 111): ExecutorLostFailure (executor 111 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000113 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:04:43.566]Container killed on request. Exit code is 143\n","[2024-03-09 20:04:43.566]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:04:43.566]Killed by external signal\n",".\n","24/03/09 20:06:04 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000116 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:04.655]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:04.655]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:04.656]Killed by external signal\n",".\n","24/03/09 20:06:04 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 114 for reason Container from a bad node: container_1710004378772_0001_01_000116 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:04.655]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:04.655]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:04.656]Killed by external signal\n",".\n","24/03/09 20:06:04 ERROR YarnScheduler: Lost executor 114 on cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000116 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:04.655]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:04.655]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:04.656]Killed by external signal\n",".\n","24/03/09 20:06:04 WARN TaskSetManager: Lost task 5.1 in stage 22.0 (TID 661) (cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal executor 114): ExecutorLostFailure (executor 114 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000116 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:04.655]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:04.655]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:04.656]Killed by external signal\n",".\n","24/03/09 20:06:05 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000115 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:05.017]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:05.017]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:05.018]Killed by external signal\n",".\n","24/03/09 20:06:05 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 113 for reason Container from a bad node: container_1710004378772_0001_01_000115 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:05.017]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:05.017]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:05.018]Killed by external signal\n",".\n","24/03/09 20:06:05 ERROR YarnScheduler: Lost executor 113 on cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000115 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:05.017]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:05.017]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:05.018]Killed by external signal\n",".\n","24/03/09 20:06:05 WARN TaskSetManager: Lost task 24.0 in stage 22.0 (TID 671) (cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal executor 113): ExecutorLostFailure (executor 113 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000115 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:05.017]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:05.017]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:05.018]Killed by external signal\n",".\n","24/03/09 20:06:07 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000114 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:07.600]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:07.601]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:07.601]Killed by external signal\n",".\n","24/03/09 20:06:07 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 112 for reason Container from a bad node: container_1710004378772_0001_01_000114 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:07.600]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:07.601]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:07.601]Killed by external signal\n",".\n","24/03/09 20:06:07 ERROR YarnScheduler: Lost executor 112 on cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000114 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:07.600]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:07.601]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:07.601]Killed by external signal\n",".\n","24/03/09 20:06:07 WARN TaskSetManager: Lost task 22.0 in stage 22.0 (TID 669) (cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal executor 112): ExecutorLostFailure (executor 112 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000114 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:06:07.600]Container killed on request. Exit code is 143\n","[2024-03-09 20:06:07.601]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:06:07.601]Killed by external signal\n",".\n","24/03/09 20:07:48 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000117 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:07:47.710]Container killed on request. Exit code is 143\n","[2024-03-09 20:07:47.710]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:07:47.710]Killed by external signal\n",".\n","24/03/09 20:07:48 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 115 for reason Container from a bad node: container_1710004378772_0001_01_000117 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:07:47.710]Container killed on request. Exit code is 143\n","[2024-03-09 20:07:47.710]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:07:47.710]Killed by external signal\n",".\n","24/03/09 20:07:48 ERROR YarnScheduler: Lost executor 115 on cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000117 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:07:47.710]Container killed on request. Exit code is 143\n","[2024-03-09 20:07:47.710]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:07:47.710]Killed by external signal\n",".\n","24/03/09 20:07:48 WARN TaskSetManager: Lost task 5.2 in stage 22.0 (TID 674) (cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal executor 115): ExecutorLostFailure (executor 115 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000117 on host: cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:07:47.710]Container killed on request. Exit code is 143\n","[2024-03-09 20:07:47.710]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:07:47.710]Killed by external signal\n",".\n","24/03/09 20:08:01 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000118 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:08:01.261]Container killed on request. Exit code is 143\n","[2024-03-09 20:08:01.261]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:08:01.262]Killed by external signal\n",".\n","24/03/09 20:08:01 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 116 for reason Container from a bad node: container_1710004378772_0001_01_000118 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:08:01.261]Container killed on request. Exit code is 143\n","[2024-03-09 20:08:01.261]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:08:01.262]Killed by external signal\n",".\n","24/03/09 20:08:01 ERROR YarnScheduler: Lost executor 116 on cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000118 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:08:01.261]Container killed on request. Exit code is 143\n","[2024-03-09 20:08:01.261]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:08:01.262]Killed by external signal\n",".\n","24/03/09 20:08:01 WARN TaskSetManager: Lost task 28.0 in stage 22.0 (TID 678) (cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal executor 116): ExecutorLostFailure (executor 116 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000118 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:08:01.261]Container killed on request. Exit code is 143\n","[2024-03-09 20:08:01.261]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:08:01.262]Killed by external signal\n",".\n","24/03/09 20:08:45 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000119 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:08:45.654]Container killed on request. Exit code is 143\n","[2024-03-09 20:08:45.654]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:08:45.654]Killed by external signal\n",".\n","24/03/09 20:08:45 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 117 for reason Container from a bad node: container_1710004378772_0001_01_000119 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:08:45.654]Container killed on request. Exit code is 143\n","[2024-03-09 20:08:45.654]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:08:45.654]Killed by external signal\n",".\n","24/03/09 20:08:45 ERROR YarnScheduler: Lost executor 117 on cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000119 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:08:45.654]Container killed on request. Exit code is 143\n","[2024-03-09 20:08:45.654]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:08:45.654]Killed by external signal\n",".\n","24/03/09 20:08:45 WARN TaskSetManager: Lost task 30.0 in stage 22.0 (TID 680) (cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal executor 117): ExecutorLostFailure (executor 117 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000119 on host: cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:08:45.654]Container killed on request. Exit code is 143\n","[2024-03-09 20:08:45.654]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:08:45.654]Killed by external signal\n",".\n","24/03/09 20:09:10 WARN YarnAllocator: Container from a bad node: container_1710004378772_0001_01_000121 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:09:10.565]Container killed on request. Exit code is 143\n","[2024-03-09 20:09:10.565]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:09:10.565]Killed by external signal\n",".\n","24/03/09 20:09:10 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 119 for reason Container from a bad node: container_1710004378772_0001_01_000121 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:09:10.565]Container killed on request. Exit code is 143\n","[2024-03-09 20:09:10.565]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:09:10.565]Killed by external signal\n",".\n","24/03/09 20:09:10 ERROR YarnScheduler: Lost executor 119 on cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal: Container from a bad node: container_1710004378772_0001_01_000121 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:09:10.565]Container killed on request. Exit code is 143\n","[2024-03-09 20:09:10.565]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:09:10.565]Killed by external signal\n",".\n","24/03/09 20:09:10 WARN TaskSetManager: Lost task 5.3 in stage 22.0 (TID 681) (cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal executor 119): ExecutorLostFailure (executor 119 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000121 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:09:10.565]Container killed on request. Exit code is 143\n","[2024-03-09 20:09:10.565]Container exited with a non-zero exit code 143. \n","[2024-03-09 20:09:10.565]Killed by external signal\n",".\n","24/03/09 20:09:10 ERROR TaskSetManager: Task 5 in stage 22.0 failed 4 times; aborting job\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 22.0 failed 4 times, most recent failure: Lost task 5.3 in stage 22.0 (TID 681) (cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal executor 119): ExecutorLostFailure (executor 119 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000121 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:09:10.565]Container killed on request. Exit code is 143\n[2024-03-09 20:09:10.565]Container exited with a non-zero exit code 143. \n[2024-03-09 20:09:10.565]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_10026/1006089541.py\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# Create a dictionary where each title is a key and the corresponding second elements of the tuple are the values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mtitle_anchor_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexploded_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'title'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'anchor_text'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupByKey\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAsMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollectAsMap\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2224\u001B[0m         \u001B[0;36m4\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2225\u001B[0m         \"\"\"\n\u001B[0;32m-> 2226\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2227\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2228\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"RDD[Tuple[K, V]]\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"RDD[K]\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1195\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1197\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1198\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 22.0 failed 4 times, most recent failure: Lost task 5.3 in stage 22.0 (TID 681) (cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal executor 119): ExecutorLostFailure (executor 119 exited caused by one of the running tasks) Reason: Container from a bad node: container_1710004378772_0001_01_000121 on host: cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal. Exit status: 143. Diagnostics: [2024-03-09 20:09:10.565]Container killed on request. Exit code is 143\n[2024-03-09 20:09:10.565]Container exited with a non-zero exit code 143. \n[2024-03-09 20:09:10.565]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}],"source":["from pyspark.sql.functions import explode, col, struct\n","\n","exploded_df = parquetFile.select(col(\"title\"), explode(\"anchor_text\").alias(\"anchor_text\"))\n","\n","# Create a dictionary where each title is a key and the corresponding second elements of the tuple are the values\n","title_anchor_dict = exploded_df.rdd.map(lambda row: (row['title'], row['anchor_text'][1])).groupByKey().mapValues(list).collectAsMap()\n"]},{"cell_type":"code","execution_count":null,"id":"af23f1ae","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/09 20:09:11 WARN TaskSetManager: Lost task 32.0 in stage 22.0 (TID 684) (cluster-6ed3-w-0.us-central1-a.c.irprojectilayvictor.internal executor 122): TaskKilled (Stage cancelled)\n","24/03/09 20:09:12 WARN TaskSetManager: Lost task 30.1 in stage 22.0 (TID 686) (cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal executor 123): TaskKilled (Stage cancelled)\n","24/03/09 20:09:12 WARN TaskSetManager: Lost task 28.1 in stage 22.0 (TID 683) (cluster-6ed3-w-1.us-central1-a.c.irprojectilayvictor.internal executor 118): TaskKilled (Stage cancelled)\n","24/03/09 20:09:12 WARN TaskSetManager: Lost task 33.0 in stage 22.0 (TID 685) (cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal executor 120): TaskKilled (Stage cancelled)\n","24/03/09 20:09:12 WARN TaskSetManager: Lost task 31.0 in stage 22.0 (TID 682) (cluster-6ed3-w-2.us-central1-a.c.irprojectilayvictor.internal executor 121): TaskKilled (Stage cancelled)\n"]}],"source":["base_dir='title_anchor_dict_dir' \n","page_rank ='title_anchor_dict'\n","bucket_name='bucket_for_index'\n","def get_bucket(bucket_name):\n","    return storage.Client('irprojectilayvictor').bucket(bucket_name)\n","path = str(Path(base_dir) / f'{page_rank}.pkl')\n","bucket = None if bucket_name is None else get_bucket(bucket_name)\n","\n","#Path(base_dir).mkdir(parents=True, exist_ok=True)\n","p = Path(base_dir)\n","pv_clean = f'{p.stem}.pkl'\n","\n","blob = bucket.blob(path)\n","with open(pv_clean, 'wb') as f:\n","  pickle.dump(title_anchor_dict, f)\n","#pickle.dump(pr_dict, open(path, 'wb'))\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3eb66687","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}