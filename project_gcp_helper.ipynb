{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00e032c",
   "metadata": {
    "id": "hWgiQS0zkWJ5"
   },
   "source": [
    "# IR Project GCP Helper for Creating Indexes and more #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac36d3a",
   "metadata": {
    "id": "c0ccf76b",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Worker_Count",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-19e5  GCE       3                                             RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf86c5",
   "metadata": {
    "id": "01ec9fd3"
   },
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd7451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf199e6a",
   "metadata": {
    "id": "32b3ec57",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Setup",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f56ecd",
   "metadata": {
    "id": "5609143b",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-Imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "def PRINT(text) -> None: print(f'{\"-\"*80}\\n{text}\\n{\"-\"*80}')\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38a897f2",
   "metadata": {
    "id": "b10cc999",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-jar",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Feb 27 09:23 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bed56b",
   "metadata": {
    "id": "5be6dc2a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-spark-version",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-19e5-m.us-central1-a.c.irprojectilayvictor.internal:33797\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff483739480>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980e62a5",
   "metadata": {
    "id": "7adc1bf5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bucket_name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = 'bucket_for_index_generation' \n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh':\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76616fa4",
   "metadata": {},
   "source": [
    "## Calculate Documents L2 Normalization ##\n",
    "\n",
    "In the next step we calculate L2 Norm for each document text.\n",
    "\n",
    "The ending result will be dictionary which maps -> (key, value) to (doc_id, doc_l2_norm_value)\n",
    "\n",
    "We will need that in order to preform CosinSimilarity in the next steps of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c83f9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                            \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                            \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                            \"many\", \"however\", \"would\", \"became\"]\n",
    "special_words = ['3d', '4k', 'ip', 'js', 'ai', 'vr', 'ar', 'dl', 'ml', '09', '11', '9']\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){1,24}\"\"\", re.UNICODE)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61510537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['veri', 'ad', 'fun', 'way', 'q-system', 'genet', '3d', '1997', '22']\n"
     ]
    }
   ],
   "source": [
    "text = \"is a very ad fun way Q-system (genetics) 3d the 1997 22\"\n",
    "\n",
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "968dce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){1,24}\"\"\", re.UNICODE)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    clean_text = []\n",
    "    text = text.lower()\n",
    "    for token in RE_WORD.finditer(text):\n",
    "        stemmed_token = stemmer.stem(token.group())\n",
    "        if stemmed_token not in all_stopwords:\n",
    "            clean_text.append(stemmed_token)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bd3637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, to_stem):\n",
    "    clean_text = []\n",
    "    text = text.lower()\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text)]\n",
    "    for token in tokens:\n",
    "        if token not in all_stopwords:\n",
    "            if to_stem:\n",
    "                token = stemmer.stem(token)\n",
    "                clean_text.append(token)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e7a69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hell', 'add', 'add', 'add', 'function']\n",
      "3.3166247903554\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def l2_norm(text):\n",
    "    # Count the occurrences of each unique word\n",
    "    word_counts = Counter(text)\n",
    "    # Get the counts of each unique word as a list\n",
    "    counts_list = [word_counts[word]**2 for word in set(text)]\n",
    "    l2_sum = 0\n",
    "    for word_num in counts_list:\n",
    "        l2_sum += word_num\n",
    "    l2 = math.sqrt(l2_sum)\n",
    "    #l2_norm = math.sqrt(sum(x**2 for x in counts_list))\n",
    "    return l2\n",
    "\n",
    "text = \"This is some hell of a way just to add add add this function\"\n",
    "text_tok = tokenizer.tokenize(text)\n",
    "print(text_tok)\n",
    "print(l2_norm(text_tok))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0935c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "rdd_ = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4177d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_norm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30565a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_doc_l2_norm(row):\n",
    "    doc_id = row['id']\n",
    "    text = row['text']\n",
    "    tok_text = tokenizer.tokenize(text)\n",
    "    return (doc_id, l2_norm(tok_text))\n",
    "\n",
    "doc_norm_rdd = rdd_.map(calculate_doc_l2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6d32f-e0ac-4ee2-a010-9a1bb2ebbe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_norm = dict(doc_norm_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e759fcf-00c9-4fd7-8c2a-4b02834239fb",
   "metadata": {},
   "source": [
    "### Save to our Bucket ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8a77f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_final_indexes'  \n",
    "doc_l2_norm='doc_l2_norm_'\n",
    "bucket_name='inverted_indexes_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aae1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{doc_l2_norm}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(doc_norm, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1519149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 4045403, Value: 125.50298801223818\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045413, Value: 28.319604517012593\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045419, Value: 38.57460304397182\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045426, Value: 22.891046284519195\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045432, Value: 46.119410230400824\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045456, Value: 79.39773296511683\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045466, Value: 36.15245496505044\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045471, Value: 40.07492981902776\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045479, Value: 108.4250893474384\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045516, Value: 15.394804318340652\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(doc_norm.items())[:10]:\n",
    "    PRINT(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b5d19-0b88-45a0-9628-51a2f13857aa",
   "metadata": {},
   "source": [
    "### Explore the RDD Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1bd28f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'text', 'anchor_text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "rdd = parquetFile.rdd\n",
    "\n",
    "# Convert RDD to DataFrame to view column names\n",
    "rdd = rdd.toDF()\n",
    "\n",
    "# Print column names\n",
    "print(rdd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2aef22c-94ba-491f-84de-c20ee9130e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|     id|               title|                text|         anchor_text|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|4045403|Foster Air Force ...|'''Foster Air For...|[{1176764, Tactic...|\n",
      "|4045413|     Torino Palavela|'''Palavela''', f...|[{77743, 2006 Win...|\n",
      "|4045419|   Mad About the Boy|\"'''Mad About the...|[{34028256, Joyce...|\n",
      "|4045426|       Shayne Breuer|'''Shayne Breuer'...|[{1838386, Woodvi...|\n",
      "|4045432|         Parantaka I|'''Parantaka Chol...|[{1511716, Aditya...|\n",
      "|4045456|Arundel (UK Parli...|'''Arundel''' was...|[{4665376, Arunde...|\n",
      "|4045466|     Andrew Martinez|'''Luis Andrew Ma...|[{4860, Berkeley,...|\n",
      "|4045471|    Vancouver VooDoo|The '''Vancouver ...|[{32706, Vancouve...|\n",
      "|4045479|     Invisible plane|The '''Invisible ...|[{2260539, Ross A...|\n",
      "|4045516|    Shopping channel|'''Shopping chann...|[{592899, special...|\n",
      "|4045519|      Turgay (river)|The '''Turgay''' ...|[{16642, Kazakhst...|\n",
      "|4045523|              Turgay|'''Turgay''' is a...|[{29992, Turkish}...|\n",
      "|4045525|Heinrich Johann N...|'''Heinrich Johan...|[{5178678, Roodt-...|\n",
      "|4045532|                 KUT|thumb|Belo Center...|[{1998, Austin, T...|\n",
      "|4045544|          Dodge Cove|thumb|upright=.9|...|[{232346, Unincor...|\n",
      "|4045546|            Triphone|In linguistics, a...|[{22760983, lingu...|\n",
      "|4045554|Government House ...|'''Government Hou...|[{6898431, Barrin...|\n",
      "|4045575|           Bud Abell|'''Harry Everett'...|[{492475, Linebac...|\n",
      "|4045577|  Susette La Flesche|'''Susette La Fle...|[{4049833, Thomas...|\n",
      "|4045581|Pomme de Terre Ri...|The '''Pomme de T...|[{3434750, United...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_text_pairs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701811af",
   "metadata": {
    "id": "gaaIoFViXyTg"
   },
   "source": [
    "## Import Inverted Index .py File ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121fe102",
   "metadata": {
    "id": "04371c88",
    "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_gcp.py\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57c101a8",
   "metadata": {
    "id": "2d3285d8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c259c402",
   "metadata": {
    "id": "2477a5b9"
   },
   "outputs": [],
   "source": [
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12332723-b0d7-4385-815b-90993c39018e",
   "metadata": {},
   "source": [
    "## Building an inverted indexes ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ad7e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        english_stopwords = frozenset(stopwords.words('english'))\n",
    "        corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                            \"may\", \"first\", \"see\", \"people\", \"one\", \"two\",\n",
    "                            \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                            \"many\", \"however\", \"would\", \"became\", \"yet\", \"oh\", \"even\",\n",
    "                            \"within\", \"beyond\", \"hey\", \"since\", \"without\", \"ugh\", \"wow\",\n",
    "                            \"ah\", \"already\", \"oops\", \"really\", \"still\", \"hmm\", \"among\"]\n",
    "\n",
    "\n",
    "        self.all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.special_words = ['3d', '4k', 'ip', 'js', 'ai', 'vr', 'ar', 'dl', 'ml', '09', '11', '9']\n",
    "        \n",
    "    def get_word_pattern(self):\n",
    "        word_pattern = r\"(?:(?<=^)|(?<=\\s))(\\w+[-']?\\w+([-']\\w+)*)[,.']?(?<![,.!])\"\n",
    "        return word_pattern\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "      RE_TOKENIZE = re.compile(rf\"\"\"\n",
    "      (\n",
    "          # Words\n",
    "          (?P<WORD>{self.get_word_pattern()})\n",
    "          # space\n",
    "          |(?P<SPACE>[\\s\\t\\n]+)\n",
    "          # everything else\n",
    "          |(?P<OTHER>\\w+))\"\"\",  re.MULTILINE | re.IGNORECASE | re.VERBOSE | re.UNICODE)\n",
    "\n",
    "      return [self.stemmer.stem(v) for match in RE_TOKENIZE.finditer(text) for k, v in match.groupdict().items() if v is not None and k != 'SPACE' and bool(re.match(r'^[a-zA-Z0-9]+$', v)) and (len(v) > 2 or v.lower() in self.special_words) and v.lower() not in self.all_stopwords and len(v) <= 24] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7245d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list_ = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"again\", \"against\", \"ain't\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"an\", \"and\", \"another\", \"any\", \"anybody\", \"anyone\", \"anything\", \"anywhere\", \"are\", \"aren't\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"been\", \"before\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\", \"can\", \"can't\", \"cannot\", \"canst\", \"certain\", \"chiefly\", \"clean\", \"clear\", \"clearly\", \"come\", \"could\", \"couldn't\", \"dare\", \"daren't\", \"de\", \"definitely\", \"did\", \"didn't\", \"different\", \"do\", \"does\", \"doesn't\", \"doing\", \"done\", \"don't\", \"down\", \"downwards\", \"during\", \"each\", \"either\", \"else\", \"elsewhere\", \"enough\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"exactly\", \"except\", \"fairly\", \"far\", \"farther\", \"few\", \"fewer\", \"fifth\", \"first\", \"five\", \"followed\", \"following\", \"follows\", \"for\", \"forth\", \"four\", \"from\", \"further\", \"furthermore\", \"get\", \"gets\", \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"hence\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"least\", \"let\", \"let's\", \"like\", \"likely\", \"little\", \"look\", \"looking\", \"looks\", \"low\", \"lower\", \"made\", \"make\", \"makes\", \"making\", \"many\", \"may\", \"mayn't\", \"me\", \"mean\", \"meantime\", \"meanwhile\", \"might\", \"mightn't\", \"mine\", \"minus\", \"more\", \"most\", \"mostly\", \"much\", \"must\", \"mustn't\", \"my\", \"myself\", \"namely\", \"need\", \"needn't\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"notwithstanding\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"oughtn't\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"possible\", \"present\", \"presumably\", \"probably\", \"provided\", \"provides\", \"que\", \"quite\", \"qv\", \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"recent\", \"recently\", \"regarding\", \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\", \"round\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"someday\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that's\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"there's\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\", \"value\", \"various\", \"very\", \"via\", \"viz\", \"vs\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"welcome\", \"well\", \"went\", \"were\", \"weren't\", \"what\", \"what's\", \"whatever\", \"when\", \"when's\", \"whence\", \"whenever\", \"where\", \"where's\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"why's\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won't\", \"wonder\", \"would\", \"would\", \"wouldn't\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\", \"aboard\", \"about\", \"above\", \"absent\", \"across\", \"after\", \"against\", \"along\", \"alongside\", \"amid\", \"among\", \"amongst\", \"an\", \"and\", \"around\", \"as\", \"aside\", \"astride\", \"at\", \"atop\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"betwixt\", \"beyond\", \"by\", \"despite\", \"down\", \"during\", \"except\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\", \"per\", \"plus\", \"round\", \"save\", \"since\", \"through\", \"throughout\", \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"until\", \"unto\", \"up\", \"upon\", \"with\", \"within\", \"without\", \"worth\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c'mon\", \"c's\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"keep\", \"keeps\", \"kept\", \"kg\", \"know\", \"known\", \"knows\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"que\", \"quickly\", \"quite\", \"qv\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "corpus_stopwords_ = set(stopwords_list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2fc6e-f41b-4141-83e0-2af895322881",
   "metadata": {},
   "source": [
    "### Pipeline Code to Generate the Index ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3521908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22fbc75-62f2-4e06-9cbe-865ae1178d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'is a dsfsfsdfsdfdsfdsfdsewlkjkljkljlkjlkjkljkjkr ll pp the Spiderman September 3D 11 11 11 5 0 4 09 attacks attack gpu PDF 1997 \"Who is considered the father of USA?\"  bioinformatics september PDF 3D 4K television mawtānā d sharʿūṭā genetics video games'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce73eb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spiderman',\n",
       " 'septemb',\n",
       " '3d',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '09',\n",
       " 'attack',\n",
       " 'attack',\n",
       " 'gpu',\n",
       " 'pdf',\n",
       " '1997',\n",
       " 'consid',\n",
       " 'father',\n",
       " 'usa',\n",
       " 'bioinformat',\n",
       " 'septemb',\n",
       " 'pdf',\n",
       " '3d',\n",
       " '4k',\n",
       " 'televis',\n",
       " 'genet',\n",
       " 'video',\n",
       " 'game']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test = tokenizer.tokenize(test)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "149fd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = word_count(test, 55410)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4aa98c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spiderman', (55410, 1)),\n",
       " ('septemb', (55410, 2)),\n",
       " ('3d', (55410, 2)),\n",
       " ('11', (55410, 3)),\n",
       " ('09', (55410, 1)),\n",
       " ('attack', (55410, 2)),\n",
       " ('gpu', (55410, 1)),\n",
       " ('pdf', (55410, 2)),\n",
       " ('1997', (55410, 1)),\n",
       " ('consid', (55410, 1)),\n",
       " ('father', (55410, 1)),\n",
       " ('usa', (55410, 1)),\n",
       " ('bioinformat', (55410, 1)),\n",
       " ('4k', (55410, 1)),\n",
       " ('televis', (55410, 1)),\n",
       " ('genet', (55410, 1)),\n",
       " ('video', (55410, 1)),\n",
       " ('game', (55410, 1))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3f22ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Number of stopwords for body index -> 797\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'Number of stopwords for body index -> {len(tokenizer.all_stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e8f1888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Number of stopwords for title index -> 216\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'Number of stopwords for title index -> {len(tokenizer.all_stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3ad8fea",
   "metadata": {
    "id": "a4b6ee29",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-token2bucket",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "def token2bucket_id(token):\n",
    "  return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count(text, id):\n",
    "  ''' \n",
    "  Count the frequency of each word in `text` (tf) that is not included in \n",
    "  `all_stopwords` and return entries that will go into our posting lists. \n",
    "  Parameters:\n",
    "    text: str ,Text of one document\n",
    "    id: int ,Document id\n",
    "  Returns: List of tuples, A list of (token, (doc_id, tf)) pairs \n",
    "  [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "  '''\n",
    "\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  c = Counter([tok for tok in tokens])\n",
    "  return [(item[0],(id,item[1])) for item in c.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "  ''' \n",
    "  Returns a sorted posting list by wiki_id.\n",
    "  Parameters: unsorted_pl: list of (wiki_id, tf) tuples \n",
    "  Returns: A sorted posting list.\n",
    "  '''\n",
    "  return sorted(unsorted_pl)\n",
    "\n",
    "def calculate_df(postings):\n",
    "  ''' \n",
    "  Takes a posting list RDD and calculate the df for each token.\n",
    "  Parameters: postings: RDD ,An RDD where each element is a (token, posting_list) pair.\n",
    "  Returns:RDD ,An RDD where each element is a (token, df) pair.\n",
    "  '''\n",
    "  return postings.map(lambda x: (x[0],len(x[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir_, bucket_name_):\n",
    "  ''' \n",
    "  A function that partitions the posting lists into buckets, writes out \n",
    "  all posting lists in a bucket to disk, and returns the posting locations for \n",
    "  each bucket. Partitioning done by using `token2bucket`.\n",
    "  Parameters:\n",
    "  \n",
    "  base_dir : string - Name for index directory\n",
    "  bucket_name : string - Name of the bucket we want to store our inverted index directory (base_dir)\n",
    "  -----------\n",
    "    postings: RDD , An RDD where each item is a (w, posting_list) pair.\n",
    "  Returns: RDD\n",
    "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "      posting locations maintain a list for each word of file locations and \n",
    "      offsets its posting list was written to.\n",
    "  '''\n",
    "  return postings.map(lambda x: (token2bucket_id(x[0]),x)).groupByKey().map(lambda x: InvertedIndex.write_a_posting_list(x,\n",
    "                                                                                                                         base_dir= base_dir_,\n",
    "                                                                                                                         bucket_name=bucket_name_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ceaa4-b94f-4c5f-99bf-8b400c9aedbd",
   "metadata": {},
   "source": [
    "### Generate Inverted Index For Title & Id Pairs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58176c45-b468-481d-983f-781b51f90705",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id_pairs = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78c4b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================> (121 + 3) / 124]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of title & id pairs -> 6348910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_pairs = title_id_pairs.count()\n",
    "print(f'Number of title & id pairs -> {count_pairs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddaa48-7619-4e1c-81e1-53aa0c9f5194",
   "metadata": {},
   "source": [
    "#### Generate Posting Lists for Title Index & Save them ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55c8764e",
   "metadata": {
    "id": "0b5d7296",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-index_construction",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 35\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# time the index creation time\n",
    "t_start = time()\n",
    "\n",
    "# word counts map\n",
    "word_counts = title_id_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# filtering postings and calculate df\n",
    "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "# partition posting lists and write out\n",
    "_ = partition_postings_and_write(postings_filtered,\n",
    "                                 base_dir_='title_index_directory_final',\n",
    "                                 bucket_name_='inverted_indexes_bucket').collect()\n",
    "\n",
    "index_const_time = time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3dbc0e14",
   "metadata": {
    "id": "348pECY8cH-T",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-index_const_time",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "The total amount of time required to create the index is 4.68 minutes\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'The total amount of time required to create the index is {(index_const_time/60):.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66660993-d427-452d-83e6-fa56958dcf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_index_directory_final/0_000.bin\n",
      "title_index_directory_final/0_posting_locs.pickle\n",
      "title_index_directory_final/100_000.bin\n",
      "title_index_directory_final/100_posting_locs.pickle\n",
      "title_index_directory_final/101_000.bin\n",
      "title_index_directory_final/101_posting_locs.pickle\n",
      "title_index_directory_final/102_000.bin\n",
      "title_index_directory_final/102_posting_locs.pickle\n",
      "title_index_directory_final/103_000.bin\n",
      "title_index_directory_final/103_posting_locs.pickle\n",
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "blobs_test = client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                              prefix='title_index_directory_final')\n",
    "count = 0\n",
    "# Iterate over the blobs\n",
    "for blob in blobs_test:\n",
    "    if count == 10: break\n",
    "    count+=1\n",
    "    print(blob.name)\n",
    "    \n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab3296f4",
   "metadata": {
    "id": "Opl6eRNLM5Xv",
    "nbgrader": {
     "grade": true,
     "grade_id": "collect-posting",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                             prefix='title_index_directory_final'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "    \n",
    "PRINT('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f66e3a",
   "metadata": {
    "id": "VhAV0A6dNZWY"
   },
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5d2cfb6",
   "metadata": {
    "id": "54vqT_0WNc3w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted = InvertedIndex()\n",
    "\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted.posting_locs = super_posting_locs\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# write the global stats out\n",
    "inverted.write_index(base_dir='project_final_indexes', \n",
    "                     name='title_index_final',\n",
    "                     bucket_name='inverted_indexes_bucket')\n",
    "\n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def55262-7232-4a30-8f0d-b19de810877f",
   "metadata": {},
   "source": [
    "#### Visualize the Index ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53ee777a-3317-45a1-846c-7f254493b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(base_dir, name, bucket_name):\n",
    "    # Initialize the client\n",
    "    client = storage.Client()\n",
    "    \n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    # Define the path to the index file in the bucket\n",
    "    index_path = f'{base_dir}/{name}.pkl'\n",
    "    \n",
    "    # Get the blob (file) from the bucket\n",
    "    blob = bucket.blob(index_path)\n",
    "    \n",
    "    # Download the blob into memory\n",
    "    index_data = blob.download_as_string()\n",
    "    \n",
    "    # Load the index from the downloaded data\n",
    "    inverted_index = pickle.loads(index_data)\n",
    "    \n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9dc349a-4e02-4980-986c-651ebeea2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = load_index(base_dir='project_final_indexes', \n",
    "                            name='title_stem_index',\n",
    "                            bucket_name='inverted_indexes_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b5fe63c-088a-468d-b280-e2db172faefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_locs = inverted_index.posting_locs\n",
    "df_data = inverted_index.df\n",
    "\n",
    "# Convert the posting_locs dictionary to a DataFrame\n",
    "posting_locs_df = pd.DataFrame.from_dict(posting_locs, orient='index', columns=['Posting List'])\n",
    "\n",
    "# Convert the df_data dictionary to a DataFrame\n",
    "df_data_df = pd.DataFrame.from_dict(df_data, orient='index', columns=['Document Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08359fa8-c762-41d8-bced-4f6ba279ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_df_sorted = df_data_df.sort_values(by='Document Frequency', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b82b543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Data Frame shape -> (25230, 1)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'Data Frame shape -> {df_data_df_sorted.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b4263b2-452f-4514-8954-4939701f68bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Data Frame shape -> (26981, 1)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'Data Frame shape -> {df_data_df_sorted.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0a003e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_frequency = df_data_df_sorted.loc['bioinformat', 'Document Frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6e23b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8021387-04a6-4aab-b7a3-e8c857f8ad31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>membership</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jena</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deux</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frye</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adkin</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tesla</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kingsburi</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staunton</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liceo</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaspard</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Document Frequency\n",
       "membership                 101\n",
       "jena                       101\n",
       "deux                       101\n",
       "frye                       101\n",
       "adkin                      101\n",
       "tesla                      101\n",
       "kingsburi                  101\n",
       "staunton                   101\n",
       "liceo                      101\n",
       "gaspard                    101"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_df_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a91101c3-f253-4b3a-8600-f55ee85a3bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done with Title Index !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT('Done with Title Index !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c0661-462c-44d9-b161-f68450ce1c74",
   "metadata": {},
   "source": [
    "### Generate Inverted Idex For Body & Id Pairs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eeea605f-717f-4661-a20f-f78781426035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "body_id_pairs = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f04df369-d1f8-4421-a17f-7d457766fe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# time the index creation time\n",
    "t_start = time()\n",
    "\n",
    "# word counts map\n",
    "word_counts = body_id_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# filtering postings and calculate df\n",
    "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "# partition posting lists and write out\n",
    "_ = partition_postings_and_write(postings_filtered,\n",
    "                                 base_dir_='body_index_directory_final',\n",
    "                                 bucket_name_='inverted_indexes_bucket').collect()\n",
    "\n",
    "index_const_time = time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63f8f943-82f8-47f2-9de9-354168f8484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "The total amount of time required to create the index is 3.725 hours\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "PRINT(f'The total amount of time required to create the index is {index_const_time/60/60:.3f} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "465f49e2-5536-41ec-88cb-a98e459d1019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_index_directory_final/0_000.bin\n",
      "body_index_directory_final/0_001.bin\n",
      "body_index_directory_final/0_002.bin\n",
      "body_index_directory_final/0_003.bin\n",
      "body_index_directory_final/0_004.bin\n",
      "body_index_directory_final/0_005.bin\n",
      "body_index_directory_final/0_006.bin\n",
      "body_index_directory_final/0_007.bin\n",
      "body_index_directory_final/0_008.bin\n",
      "body_index_directory_final/0_009.bin\n",
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "blobs_test = client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                              prefix='body_index_directory_final')\n",
    "count = 0\n",
    "# Iterate over the blobs\n",
    "for blob in blobs_test:\n",
    "    if count == 10:break\n",
    "    count+=1\n",
    "    print(blob.name)\n",
    "    \n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e509e28b-eea4-4b94-a2f2-3cbc7ed5b724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_or_name='inverted_indexes_bucket',\n",
    "                             prefix='body_index_directory_final'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs[k].extend(v)\n",
    "    \n",
    "PRINT('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3e5957c-ceef-4db9-b06f-dd9eec01c15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create inverted index instance\n",
    "inverted = InvertedIndex()\n",
    "\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted.posting_locs = super_posting_locs\n",
    "\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# write the global stats out\n",
    "inverted.write_index(base_dir='project_final_indexes', \n",
    "                     name='body_index_final',\n",
    "                     bucket_name='inverted_indexes_bucket')\n",
    "\n",
    "PRINT('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919aa8f6-5404-44dc-840e-d45bcbe830a0",
   "metadata": {},
   "source": [
    "## Generate Document Lenghs for Body & Title ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bbc474-bb74-4dae-8dca-59c5c0f4005f",
   "metadata": {},
   "source": [
    "### Generate Body Document Length Dictionary ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b275a18-fe81-46ce-8112-2eee642a5020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "body_id_rdd = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3682cb66-0ad8-4ece-84d4-6c9a9429ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_dictionary_lenght = {} # maps (key,value)=(doc_id, body_doc_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e5c614b-7ad0-446c-87a3-763e7575ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def calculate_doc_length(row):\n",
    "    doc_id = row['id']\n",
    "    text = row['text']\n",
    "    text_tok = tokenize(text)\n",
    "    return (doc_id, len(text_tok))\n",
    "\n",
    "# Apply the function to each row of the RDD and map it to (doc_id, doc_length) pairs\n",
    "doc_lengths_rdd = body_id_rdd.map(calculate_doc_length)\n",
    "\n",
    "# Collect the results as a dictionary\n",
    "body_dictionary_lenght = dict(doc_lengths_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41271cf6-be61-4664-8f04-5e1790c8ff08",
   "metadata": {},
   "source": [
    "#### Save it to our Bucket ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d13e9e-dd44-4f80-91b2-1f6958c4186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_final_indexes' \n",
    "dl_name='body_dl_'\n",
    "bucket_name='inverted_indexes_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532f5af0-9ba3-4d94-b114-ca6be79127e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket(bucket_name):\n",
    "    return storage.Client('irprojectilayvictor').bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9616f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{dl_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a4a19b8-4a42-43da-a960-8872cd81e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{dl_name}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(body_dictionary_lenght, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b2d1c-bf05-458a-b2d3-93ef28324899",
   "metadata": {},
   "source": [
    "#### Verify that the Dictionary Saved Successfully ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "187907c6-5ceb-4a28-ae40-d55fe264060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Download the pickled file from the bucket\n",
    "blob = bucket.blob(path)\n",
    "blob.download_to_filename(path)\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04661a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 4045403, Value: 1665\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045413, Value: 241\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045419, Value: 584\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# final\n",
    "for key, value in list(loaded_dict.items())[:3]:\n",
    "    PRINT(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74dd18-3cf3-441a-a9f7-3992c446e78e",
   "metadata": {},
   "source": [
    "### Generate Title Document Length Dictionary ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50b90f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){1,24}\"\"\", re.UNICODE)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    clean_text = []\n",
    "    text = text.lower()\n",
    "    for token in RE_WORD.finditer(text):\n",
    "        stemmed_token = stemmer.stem(token.group())\n",
    "        if stemmed_token not in english_stopwords:\n",
    "            clean_text.append(stemmed_token)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0da85e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "title_id_rdd = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3126f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dictionary_length = {} # maps (key, value)=(doc_id, title_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25d9f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def calculate_title_length(row):\n",
    "    doc_id = row['id']\n",
    "    title = row['title']\n",
    "    title_token = tokenize(title)\n",
    "    return (doc_id, len(title_token))\n",
    "\n",
    "# Apply the function to each row of the RDD and map it to (doc_id, title) pairs\n",
    "title_rdd = title_id_rdd.map(calculate_title_length)\n",
    "\n",
    "title_dictionary_length = dict(title_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e563906",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_final_indexes' \n",
    "dl_name='title_DL_'\n",
    "bucket_name='inverted_indexes_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad7e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "path = str(Path(base_dir) / f'{dl_name}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(title_dictionary_length, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)\n",
    "\n",
    "PRINT('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "561745c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Download the pickled file from the bucket\n",
    "blob = bucket.blob(path)\n",
    "blob.download_to_filename(path)\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f00edf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 4045403, Value: 4\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045413, Value: 2\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045419, Value: 2\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(loaded_dict.items())[:3]:\n",
    "    PRINT(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af33bea4-3bc5-446a-b101-ebcb753d31b3",
   "metadata": {},
   "source": [
    "### Generate Title Document Dictionary ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b67663ef-1323-4e5a-ac01-d7c795ba3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dictionary = {} # maps (key, value)=(doc_id, title_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a47dbf1c-792e-4f4a-8ee5-abfa5746b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "title_id_rdd = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef19c7c4-178b-48fc-8a68-0ab1db8acdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def calculate_title(row):\n",
    "    doc_id = row['id']\n",
    "    title = row['title']\n",
    "    return (doc_id, title)\n",
    "\n",
    "# Apply the function to each row of the RDD and map it to (doc_id, title) pairs\n",
    "title_rdd = title_id_rdd.map(calculate_title)\n",
    "\n",
    "title_dictionary = dict(title_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d70a7e2e-dacb-4200-bda4-864ec3b1d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 4045403, Value: Foster Air Force Base\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045413, Value: Torino Palavela\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 4045419, Value: Mad About the Boy\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(title_dictionary.items())[:3]:\n",
    "    PRINT(f\"Key: {key}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9edeb-2d5b-4994-979f-06c7fcfae024",
   "metadata": {},
   "source": [
    "#### Save it to our Bucket ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2d8ba7a-aa00-46b1-913a-b8c3d816611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_final_indexes' \n",
    "dl_name='doc_norm'\n",
    "bucket_name='inverted_indexes_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af3185-9acd-49b3-a8f9-193f36776ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{dl_name}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(doc_norm, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dee14",
   "metadata": {
    "id": "fc0667a9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a6d655c112e79c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Execute PageRank & Save ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31a516e2",
   "metadata": {
    "id": "yVjnTvQsegc-"
   },
   "outputs": [],
   "source": [
    "def generate_graph(pages):\n",
    "  ''' \n",
    "  Compute the directed graph generated by wiki links.\n",
    "  Parameters: An RDD where each row consists of one wikipedia articles with 'id' and \n",
    "      'anchor_text'.\n",
    "  Returns: \n",
    "   - edges: RDD\n",
    "      An RDD where each row represents an edge in the directed graph created by\n",
    "      the wikipedia links. The first entry should the source page id and the \n",
    "      second entry is the destination page id. No duplicates should be present. \n",
    "   - vertices: RDD\n",
    "      An RDD where each row represents a vetrix (node) in the directed graph \n",
    "      created by the wikipedia links. No duplicates should be present. \n",
    "  '''\n",
    "  edges_w_dup = pages.flatMap(lambda page: [(page[0],anchor[0]) for anchor in page[1]])\n",
    "  edges = edges_w_dup.distinct()\n",
    "  vertices = edges.flatMap(lambda x: x)\n",
    "  vertices = vertices.distinct()\n",
    "  vertices = vertices.map(lambda x: Row(x)) #converting entries to a format that fits toDF() func up next\n",
    "  return edges, vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9bd4b2-75b1-4f46-a779-9d9e604fdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "\n",
    "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
    "\n",
    "# construct the graph \n",
    "edges, vertices = generate_graph(pages_links)\n",
    "\n",
    "# compute PageRank\n",
    "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
    "\n",
    "g = GraphFrame(verticesDF, edgesDF)\n",
    "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "\n",
    "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "pr = pr.sort(col('pagerank').desc())\n",
    "\n",
    "pr_time = time() - t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3620817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 230:===================================================> (121 + 2) / 124]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|     id|          pagerank|\n",
      "+-------+------------------+\n",
      "|3434750| 9913.728782160779|\n",
      "|  10568| 5385.349263642038|\n",
      "|  32927| 5282.081575765276|\n",
      "|  30680|  5128.23370960412|\n",
      "|5843419| 4957.567686263868|\n",
      "|  68253| 4769.278265355157|\n",
      "|  31717| 4486.350180548311|\n",
      "|  11867|4146.4146509127695|\n",
      "|  14533|3996.4664408855006|\n",
      "| 645042| 3531.627089803743|\n",
      "|  17867|3246.0983906041415|\n",
      "|5042916| 2991.945739166178|\n",
      "|4689264| 2982.324883041747|\n",
      "|  14532| 2934.746829203171|\n",
      "|  25391|   2903.5462235134|\n",
      "|   5405| 2891.416329154635|\n",
      "|4764461| 2834.366987332661|\n",
      "|  15573|2783.8651181588384|\n",
      "|   9316|2782.0396464137702|\n",
      "|8569916|2775.2861918400154|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a57122-f2c3-49f9-959c-d65490a552ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_dict = pr.collect()\n",
    "pr_dict = {row['id']: row['pagerank'] for row in pr_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec59cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='project_pageRank' \n",
    "page_rank ='pageRank'\n",
    "bucket_name='bucket_for_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1326cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{page_rank}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(pr_dict, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c25458be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Key: 3434750, doc page rank: 9913.728782160779\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 10568, doc page rank: 5385.349263642038\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 32927, doc page rank: 5282.081575765276\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 30680, doc page rank: 5128.23370960412\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 5843419, doc page rank: 4957.567686263868\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 68253, doc page rank: 4769.278265355157\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 31717, doc page rank: 4486.350180548311\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 11867, doc page rank: 4146.4146509127695\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 14533, doc page rank: 3996.4664408855006\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Key: 645042, doc page rank: 3531.627089803743\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(pr_dict.items())[:10]:\n",
    "    PRINT(f\"Key: {key}, doc page rank: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61223055",
   "metadata": {},
   "source": [
    "# Generate Word Embedding #\n",
    "\n",
    "## Ilay dont forget to save the model in our bucket as '.pkl' so we can load it and use it at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b628fd-f60b-4a77-976b-f46123fa945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'text', 'anchor_text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "rdd = parquetFile.rdd\n",
    "\n",
    "# Convert RDD to DataFrame to view column names\n",
    "rdd = rdd.toDF()\n",
    "\n",
    "# Print column names\n",
    "print(rdd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c5129a8-e37a-411a-a475-258ddd6c38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() # Run the cell of Tokenizer() class before (the class somewhen in the middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa280bd-7f9d-439a-8bba-f59634138755",
   "metadata": {},
   "source": [
    "## Train Word2Ven Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============>                                          (29 + 6) / 124]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "\n",
    "tokenize_udf = udf(lambda text: tokenizer.tokenize(text), ArrayType(StringType()))\n",
    "df = rdd.withColumn(\"tokens\", tokenize_udf(\"text\"))\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=10, windowSize=10, inputCol=\"tokens\", outputCol=\"vectors\")\n",
    "model = word2Vec.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9685ea-71ae-4beb-9cc0-1e7b56efd51f",
   "metadata": {},
   "source": [
    "## Save it first thing after training to avoid kernel issues ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='Work2Vec_dir' \n",
    "page_rank ='word2vec'\n",
    "bucket_name='bucket_for_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4f57c-1fd8-41a9-9502-c0be5a5d92ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket(bucket_name):\n",
    "    return storage.Client('irprojectilayvictor').bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c90ce0-0c0f-4634-99bb-82d7dcc7b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(Path(base_dir) / f'{page_rank}.pkl')\n",
    "bucket = None if bucket_name is None else get_bucket(bucket_name)\n",
    "\n",
    "Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "blob = bucket.blob(path)\n",
    "pickle.dump(pr_dict, open(path, 'wb'))\n",
    "blob.upload_from_filename(path)\n",
    "\n",
    "print('Saved !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579270d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment3_gcp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
